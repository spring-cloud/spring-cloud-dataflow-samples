<?xml version="1.0" encoding="UTF-8" standalone="no"?><!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pls="http://www.w3.org/2005/01/pronunciation-lexicon" xmlns:ssml="http://www.w3.org/2001/10/synthesis" xmlns:svg="http://www.w3.org/2000/svg"><head><title>GemFire Samples</title><link rel="stylesheet" type="text/css" href="docbook-epub.css"/><meta name="generator" content="DocBook XSL Stylesheets V1.78.1"/><link rel="prev" href="ch02s02.xhtml" title="JDBC Samples"/><link rel="next" href="ch02s04.xhtml" title="Custom Stream Application Samples"/></head><body><header/><section class="section" title="GemFire Samples" epub:type="subchapter" id="spring-cloud-data-flow-samples-gemfire-overview"><div class="titlepage"><div><div><h2 class="title" style="clear: both">GemFire Samples</h2></div></div></div><section class="section" title="HTTP to Gemfire Demo" epub:type="division" id="spring-cloud-data-flow-samples-gemfire-http-overview"><div class="titlepage"><div><div><h3 class="title">HTTP to Gemfire Demo</h3></div></div></div><p>In this demonstration, you will learn how to build a data pipeline using <a class="link" href="http://cloud.spring.io/spring-cloud-dataflow/" target="_top">Spring Cloud Data Flow</a> to consume data from an <code class="literal">http</code> endpoint and write to Gemfire using the <code class="literal">gemfire</code> sink.</p><p>We will take you through the steps to configure and run Spring Cloud Data Flow server in either a <a class="link" href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started/" target="_top">local</a> or <a class="link" href="https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current/reference/htmlsingle/#getting-started" target="_top">Cloud Foundry</a> environment.</p><div class="note" title="Note" epub:type="notice"><table style="border: 0; "><tr><td style="text-align: center; vertical-align: top; width: 25; " rowspan="2"><img alt="[Note]" src="images/note.png"/></td><th style="text-align: left; ">Note</th></tr><tr><td style="text-align: left; vertical-align: top; "><p>For legacy reasons the <code class="literal">gemfire</code> Spring Cloud Stream Apps are named after <code class="literal">Pivotal GemFire</code>. The code base for the commercial product has since been open sourced as <code class="literal">Apache Geode</code>. These samples should work with compatible versions of Pivotal GemFire or Apache Geode. Herein we will refer to the installed IMDG simply as <code class="literal">Geode</code>.</p></td></tr></table></div><section class="section" title="Prerequisites" epub:type="division" id="_prerequisites_3"><div class="titlepage"><div><div><h4 class="title">Prerequisites</h4></div></div></div><div class="itemizedlist" epub:type="list"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem" epub:type="list-item">A Running Data Flow Shell</li></ul></div><p>The Spring Cloud Data Flow Shell is available for <a class="link" href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started-deploying-spring-cloud-dataflow" target="_top">download</a> or you can <a class="link" href="https://github.com/spring-cloud/spring-cloud-dataflow" target="_top">build</a> it yourself.</p><div class="note" title="Note" epub:type="notice"><table style="border: 0; "><tr><td style="text-align: center; vertical-align: top; width: 25; " rowspan="2"><img alt="[Note]" src="images/note.png"/></td><th style="text-align: left; ">Note</th></tr><tr><td style="text-align: left; vertical-align: top; "><p>the Spring Cloud Data Flow Shell and Local server implementation are in the same repository and are both built by running <code class="literal">./mvnw install</code> from the project root directory. If you have already run the build, use the jar in <code class="literal">spring-cloud-dataflow-shell/target</code></p></td></tr></table></div><p>To run the Shell open a new terminal session:</p><pre class="screen">$ cd &lt;PATH/TO/SPRING-CLOUD-DATAFLOW-SHELL-JAR&gt;
$ java -jar spring-cloud-dataflow-shell-&lt;VERSION&gt;.jar
  ____                              ____ _                __
 / ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
 \___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
  ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
 |____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
  ____ |_|    _          __|___/                 __________
 |  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
 | | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
 | |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
 |____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/


Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
dataflow:&gt;</pre><div class="note" title="Note" epub:type="notice"><table style="border: 0; "><tr><td style="text-align: center; vertical-align: top; width: 25; " rowspan="2"><img alt="[Note]" src="images/note.png"/></td><th style="text-align: left; ">Note</th></tr><tr><td style="text-align: left; vertical-align: top; "><p>The Spring Cloud Data Flow Shell is a Spring Boot application that connects to the Data Flow Server’s REST API and supports a DSL that simplifies the process of defining a stream or task and managing its lifecycle. Most of these samples
use the shell. If you prefer, you can use the Data Flow UI <a class="link" href="http://localhost:9393/dashboard" target="_top">localhost:9393/dashboard</a>, (or wherever it the server is hosted) to perform the same operations.</p></td></tr></table></div><div class="itemizedlist" epub:type="list"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem" epub:type="list-item">A Geode installation with a locator and cache server running
Unresolved directive in streaming/gemfire/gemfire-http/overview.adoc - include::geode-setup.adoc[]</li></ul></div></section><section class="section" title="Using the Local Server" epub:type="division" id="gemfire-http-local"><div class="titlepage"><div><div><h4 class="title">Using the Local Server</h4></div></div></div><section class="section" title="Additional Prerequisites" epub:type="division" id="_additional_prerequisites_5"><div class="titlepage"><div><div><h5 class="title">Additional Prerequisites</h5></div></div></div><div class="itemizedlist" epub:type="list"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem" epub:type="list-item">A running local Data Flow Server</li></ul></div><p>The Local Data Flow Server is Spring Boot application available for <a class="link" href="http://cloud.spring.io/spring-cloud-dataflow/" target="_top">download</a> or you can <a class="link" href="https://github.com/spring-cloud/spring-cloud-dataflow" target="_top">build</a> it yourself.
If you build it yourself, the executable jar will be in <code class="literal">spring-cloud-dataflow-server-local/target</code></p><p>To run the Local Data Flow server Open a new terminal session:</p><pre class="screen">$cd  &lt;PATH/TO/SPRING-CLOUD-DATAFLOW-LOCAL-JAR&gt;
$java -jar spring-cloud-dataflow-server-local-&lt;VERSION&gt;.jar</pre><div class="itemizedlist" epub:type="list"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem" epub:type="list-item">A running instance of <a class="link" href="https://www.rabbitmq.com" target="_top">Rabbit MQ</a></li></ul></div><p/><div class="orderedlist" epub:type="list"><ol class="orderedlist" type="1"><li class="listitem" epub:type="list-item"><p class="simpara">Use gfsh to start a locator and server</p><pre class="screen">gfsh&gt;start locator --name=locator1
gfsh&gt;start server --name=server1</pre></li><li class="listitem" epub:type="list-item"><p class="simpara">Create a region called <code class="literal">Stocks</code></p><pre class="screen">gfsh&gt;create region --name Stocks --type=REPLICATE</pre></li></ol></div></section><section class="section" title="Use the Shell to create the sample stream" epub:type="division" id="_use_the_shell_to_create_the_sample_stream"><div class="titlepage"><div><div><h5 class="title">Use the Shell to create the sample stream</h5></div></div></div><div class="orderedlist" epub:type="list"><ol class="orderedlist" type="1"><li class="listitem" epub:type="list-item"><p class="simpara"><a class="link" href="https://github.com/spring-cloud/spring-cloud-dataflow/blob/master/spring-cloud-dataflow-docs/src/main/asciidoc/streams.adoc#register-a-stream-app" target="_top">Register</a> the out-of-the-box applications for the Rabbit binder</p><div class="note" title="Note" epub:type="notice"><table style="border: 0; "><tr><td style="text-align: center; vertical-align: top; width: 25; " rowspan="2"><img alt="[Note]" src="images/note.png"/></td><th style="text-align: left; ">Note</th></tr><tr><td style="text-align: left; vertical-align: top; "><p>These samples assume that the Data Flow Server can access a remote Maven repository, <code class="literal"><a class="link" href="https://repo.spring.io/libs-release" target="_top">repo.spring.io/libs-release</a></code> by default. If your Data Flow server is running behind a firewall, or you are using a maven proxy preventing
access to public repositories, you will need to install the sample apps in your internal Maven repository and <a class="link" href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started-maven-configuration" target="_top">configure</a>
the server accordingly.  The sample applications are typically registered using Data Flow’s bulk import facility. For example, the Shell command <code class="literal">dataflow:&gt;app import --uri <a class="link" href="http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven" target="_top">bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</a></code> (The actual URI is release and binder specific so refer to the sample instructions for the actual URL).
The bulk import URI references a plain text file containing entries for all of the publicly available Spring Cloud Stream and Task applications published to <code class="literal"><a class="link" href="https://repo.spring.io" target="_top">repo.spring.io</a></code>. For example,
<code class="literal">source.http=maven://org.springframework.cloud.stream.app:http-source-rabbit:1.2.0.RELEASE</code> registers the <code class="literal">http</code> source app at the corresponding Maven address, relative to the remote repository(ies) configured for the
Data Flow server. The format is <code class="literal">maven://&lt;groupId&gt;:&lt;artifactId&gt;:&lt;version&gt;</code>  You will need to <a class="link" href="https://repo.spring.io/libs-release/org/springframework/cloud/stream/app/spring-cloud-stream-app-descriptor/Bacon.RELEASE/spring-cloud-stream-app-descriptor-Bacon.RELEASE.rabbit-apps-maven-repo-url.properties" target="_top">download</a> the required apps or <a class="link" href="https://github.com/spring-cloud-stream-app-starters" target="_top">build</a> them and then install them in your Maven repository, using whatever group, artifact, and version you choose. If you do
this, register individual apps using <code class="literal">dataflow:&gt;app register…​</code> using the <code class="literal">maven://</code> resource URI format corresponding to your installed app.</p></td></tr></table></div><pre class="screen">dataflow:&gt;app import --uri http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</pre></li><li class="listitem" epub:type="list-item"><p class="simpara">Create the stream</p><p class="simpara">This example creates an http endpoint to which we will post stock prices as a JSON document containing <code class="literal">symbol</code> and <code class="literal">price</code> fields.
The property <code class="literal">--json=true</code> to enable Geode’s JSON support and configures the sink to convert JSON String payloads to <a class="link" href="https://geode.apache.org/releases/latest/javadoc/org/apache/geode/pdx/PdxInstance.html" target="_top">PdxInstance</a>, the recommended way
to store JSON documents in Geode. The <code class="literal">keyExpression</code> property is a SpEL expression used to extract the <code class="literal">symbol</code> value the PdxInstance to use as an entry key.</p><div class="note" title="Note" epub:type="notice"><table style="border: 0; "><tr><td style="text-align: center; vertical-align: top; width: 25; " rowspan="2"><img alt="[Note]" src="images/note.png"/></td><th style="text-align: left; ">Note</th></tr><tr><td style="text-align: left; vertical-align: top; "><p>PDX serialization is very efficient and supports OQL queries without requiring a custom domain class.
Use of custom domain types requires these classes to be in the class path of both the stream apps and the cache server.
For this reason, the use of custom payload types is generally discouraged.</p></td></tr></table></div><pre class="screen">dataflow:&gt;stream create --name stocks --definition "http --port=9090 | gemfire --json=true --regionName=Stocks --keyExpression=payload.getField('symbol')" --deploy
Created and deployed new stream 'stocks'</pre><div class="note" title="Note" epub:type="notice"><table style="border: 0; "><tr><td style="text-align: center; vertical-align: top; width: 25; " rowspan="2"><img alt="[Note]" src="images/note.png"/></td><th style="text-align: left; ">Note</th></tr><tr><td style="text-align: left; vertical-align: top; "><p>If the Geode locator isn’t running on default port on <code class="literal">localhost</code>, add the options <code class="literal">--connect-type=locator --host-addresses=&lt;host&gt;:&lt;port&gt;</code>. If there are multiple
locators, you can provide a comma separated list of locator addresses. This is not necessary for the sample but is typical for production environments to enable fail-over.</p></td></tr></table></div></li><li class="listitem" epub:type="list-item"><p class="simpara">Verify the stream is successfully deployed</p><pre class="screen">dataflow:&gt;stream list</pre></li><li class="listitem" epub:type="list-item"><p class="simpara">Post sample data pointing to the <code class="literal">http</code> endpoint: <code class="literal"><a class="link" href="http://localhost:9090" target="_top">localhost:9090</a></code> (<code class="literal">9090</code> is the <code class="literal">port</code> we specified for the <code class="literal">http</code> source)</p><pre class="screen">dataflow:&gt;http post --target http://localhost:9090 --contentType application/json --data '{"symbol":"VMW","price":117.06}'
&gt; POST (application/json) http://localhost:9090 {"symbol":"VMW","price":117.06}
&gt; 202 ACCEPTED</pre></li><li class="listitem" epub:type="list-item"><p class="simpara">Using <code class="literal">gfsh</code>, connect to the locator if not already connected, and verify the cache entry was created.</p><pre class="screen">gfsh&gt;get --key='VMW' --region=/Stocks
Result      : true
Key Class   : java.lang.String
Key         : VMW
Value Class : org.apache.geode.pdx.internal.PdxInstanceImpl

symbol | price
------ | ------
VMW    | 117.06</pre></li><li class="listitem" epub:type="list-item">You’re done!</li></ol></div></section></section><section class="section" title="Using the Cloud Foundry Server" epub:type="division" id="gemfire-http-cf"><div class="titlepage"><div><div><h4 class="title">Using the Cloud Foundry Server</h4></div></div></div><section class="section" title="Additional Prerequisites" epub:type="division" id="_additional_prerequisites_6"><div class="titlepage"><div><div><h5 class="title">Additional Prerequisites</h5></div></div></div><div class="itemizedlist" epub:type="list"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem" epub:type="list-item">A Cloud Foundry instance</li><li class="listitem" epub:type="list-item">Cloud Data Flow Cloud Foundry Server</li></ul></div><p>The Cloud Foundry Data Flow Server is Spring Boot application available for <a class="link" href="http://cloud.spring.io/spring-cloud-dataflow/" target="_top">download</a> or you can <a class="link" href="https://github.com/spring-cloud/spring-cloud-dataflow-server-cloudfoundry" target="_top">build</a> it yourself.
If you build it yourself, the executable jar will be in <code class="literal">spring-cloud-dataflow-server-cloudfoundry/target</code></p><div class="note" title="Note" epub:type="notice"><table style="border: 0; "><tr><td style="text-align: center; vertical-align: top; width: 25; " rowspan="2"><img alt="[Note]" src="images/note.png"/></td><th style="text-align: left; ">Note</th></tr><tr><td style="text-align: left; vertical-align: top; "><p>Although you can run the Data Flow Cloud Foundry Server locally and configure it to deploy to any Cloud Foundry instance, we will
deploy the server to Cloud Foundry as recommended.</p></td></tr></table></div><div class="orderedlist" epub:type="list"><ol class="orderedlist" type="1"><li class="listitem" epub:type="list-item"><p class="simpara">Verify that CF instance is reachable (Your endpoint urls will be different from what is shown here).</p><pre class="screen">$ cf api
API endpoint: https://api.system.io (API version: ...)

$ cf apps
Getting apps in org [your-org] / space [your-space] as user...
OK

No apps found</pre></li><li class="listitem" epub:type="list-item"><p class="simpara">Follow the instructions to deploy the <a class="link" href="https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current/reference/htmlsingle" target="_top">Spring Cloud Data Flow Cloud Foundry server</a>. Don’t worry about creating a Redis service. We won’t need it. If you are familiar with Cloud Foundry
application manifests, we recommend creating a manifest for the the Data Flow server as shown <a class="link" href="https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current/reference/htmlsingle/#sample-manifest-template" target="_top">here</a>.</p><div class="warning" title="Warning" epub:type="warning"><table style="border: 0; "><tr><td style="text-align: center; vertical-align: top; width: 25; " rowspan="2"><img alt="[Warning]" src="images/warning.png"/></td><th style="text-align: left; ">Warning</th></tr><tr><td style="text-align: left; vertical-align: top; "><p>As of this writing, there is a typo on the <code class="literal">SPRING_APPLICATION_JSON</code> entry in the sample manifest. <code class="literal">SPRING_APPLICATION_JSON</code> must be followed by <code class="literal">:</code> and The JSON string must be
wrapped in single quotes. Alternatively, you can replace that line with <code class="literal">MAVEN_REMOTE_REPOSITORIES_REPO1_URL: <a class="link" href="https://repo.spring.io/libs-snapshot" target="_top">repo.spring.io/libs-snapshot</a></code>.  If your Cloud Foundry installation is behind a firewall, you may need to install the stream apps used in this sample in your internal Maven repository and <a class="link" href="https://docs.spring.io/spring-cloud-dataflow/docs/1.3.0.M2/reference/htmlsingle/#getting-started-maven-configuration" target="_top">configure</a> the server to access that repository.</p></td></tr></table></div></li><li class="listitem" epub:type="list-item"><p class="simpara">Once you have successfully executed <code class="literal">cf push</code>, verify the dataflow server is running</p><pre class="screen">$ cf apps
Getting apps in org [your-org] / space [your-space] as user...
OK

name                 requested state   instances   memory   disk   urls
dataflow-server      started           1/1         1G       1G     dataflow-server.app.io</pre></li><li class="listitem" epub:type="list-item">Notice that the <code class="literal">dataflow-server</code> application is started and ready for interaction via the url endpoint</li><li class="listitem" epub:type="list-item"><p class="simpara">Connect the <code class="literal">shell</code> with <code class="literal">server</code> running on Cloud Foundry, e.g., <code class="literal"><a class="link" href="http://dataflow-server.app.io" target="_top">dataflow-server.app.io</a></code></p><pre class="screen">$ cd &lt;PATH/TO/SPRING-CLOUD-DATAFLOW-SHELL-JAR&gt;
$ java -jar spring-cloud-dataflow-shell-&lt;VERSION&gt;.jar

  ____                              ____ _                __
 / ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
 \___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
  ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
 |____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
  ____ |_|    _          __|___/                 __________
 |  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
 | | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
 | |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
 |____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/


Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
server-unknown:&gt;</pre><pre class="screen">server-unknown:&gt;dataflow config server http://dataflow-server.app.io
Successfully targeted http://dataflow-server.app.io
dataflow:&gt;</pre><div class="itemizedlist" epub:type="list"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem" epub:type="list-item">Running instance of a <code class="literal">rabbit</code> service in Cloud Foundry</li><li class="listitem" epub:type="list-item">Running instance of the <a class="link" href="https://docs.pivotal.io/p-cloud-cache/1-0/developer.html" target="_top">Pivotal Cloud Cache for PCF</a> (PCC) service <code class="literal">cloudcache</code> in Cloud Foundry.</li></ul></div></li><li class="listitem" epub:type="list-item"><p class="simpara"><a class="link" href="https://github.com/spring-cloud/spring-cloud-dataflow/blob/master/spring-cloud-dataflow-docs/src/main/asciidoc/streams.adoc#register-a-stream-app" target="_top">Register</a> the out-of-the-box applications for the Rabbit binder</p><div class="note" title="Note" epub:type="notice"><table style="border: 0; "><tr><td style="text-align: center; vertical-align: top; width: 25; " rowspan="2"><img alt="[Note]" src="images/note.png"/></td><th style="text-align: left; ">Note</th></tr><tr><td style="text-align: left; vertical-align: top; "><p>These samples assume that the Data Flow Server can access a remote Maven repository, <code class="literal"><a class="link" href="https://repo.spring.io/libs-release" target="_top">repo.spring.io/libs-release</a></code> by default. If your Data Flow server is running behind a firewall, or you are using a maven proxy preventing
access to public repositories, you will need to install the sample apps in your internal Maven repository and <a class="link" href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started-maven-configuration" target="_top">configure</a>
the server accordingly.  The sample applications are typically registered using Data Flow’s bulk import facility. For example, the Shell command <code class="literal">dataflow:&gt;app import --uri <a class="link" href="http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven" target="_top">bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</a></code> (The actual URI is release and binder specific so refer to the sample instructions for the actual URL).
The bulk import URI references a plain text file containing entries for all of the publicly available Spring Cloud Stream and Task applications published to <code class="literal"><a class="link" href="https://repo.spring.io" target="_top">repo.spring.io</a></code>. For example,
<code class="literal">source.http=maven://org.springframework.cloud.stream.app:http-source-rabbit:1.2.0.RELEASE</code> registers the <code class="literal">http</code> source app at the corresponding Maven address, relative to the remote repository(ies) configured for the
Data Flow server. The format is <code class="literal">maven://&lt;groupId&gt;:&lt;artifactId&gt;:&lt;version&gt;</code>  You will need to <a class="link" href="https://repo.spring.io/libs-release/org/springframework/cloud/stream/app/spring-cloud-stream-app-descriptor/Bacon.RELEASE/spring-cloud-stream-app-descriptor-Bacon.RELEASE.rabbit-apps-maven-repo-url.properties" target="_top">download</a> the required apps or <a class="link" href="https://github.com/spring-cloud-stream-app-starters" target="_top">build</a> them and then install them in your Maven repository, using whatever group, artifact, and version you choose. If you do
this, register individual apps using <code class="literal">dataflow:&gt;app register…​</code> using the <code class="literal">maven://</code> resource URI format corresponding to your installed app.</p></td></tr></table></div><pre class="screen">dataflow:&gt;app import --uri http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</pre></li><li class="listitem" epub:type="list-item"><p class="simpara">Get the PCC connection information</p><pre class="screen">$ cf service-key cloudcache my-service-key
Getting key my-service-key for service instance cloudcache as &lt;user&gt;...

{
 "locators": [
  "10.0.16.9[55221]",
  "10.0.16.11[55221]",
  "10.0.16.10[55221]"
 ],
 "urls": {
  "gfsh": "http://...",
  "pulse": "http://.../pulse"
 },
 "users": [
  {
   "password": &lt;password&gt;,
   "username": "cluster_operator"
  },
  {
   "password": &lt;password&gt;,
   "username": "developer"
  }
 ]
}</pre></li><li class="listitem" epub:type="list-item"><p class="simpara">Using <code class="literal">gfsh</code>, connect to the PCC instance as <code class="literal">cluster_operator</code> using the service key values and create the Stocks region.</p><pre class="screen">gfsh&gt;connect --use-http --url=&lt;gfsh-url&gt; --user=cluster_operator --password=&lt;cluster_operator_password&gt;
gfsh&gt;create region --name Stocks --type=REPLICATE</pre></li><li class="listitem" epub:type="list-item"><p class="simpara">Create the stream, connecting to the PCC instance as developer</p><p class="simpara">This example creates an http endpoint to which we will post stock prices as a JSON document containing <code class="literal">symbol</code> and <code class="literal">price</code> fields.
The property <code class="literal">--json=true</code> to enable Geode’s JSON support and configures the sink to convert JSON String payloads to <a class="link" href="https://geode.apache.org/releases/latest/javadoc/org/apache/geode/pdx/PdxInstance.html" target="_top">PdxInstance</a>, the recommended way
to store JSON documents in Geode. The <code class="literal">keyExpression</code> property is a SpEL expression used to extract the <code class="literal">symbol</code> value the PdxInstance to use as an entry key.</p><div class="note" title="Note" epub:type="notice"><table style="border: 0; "><tr><td style="text-align: center; vertical-align: top; width: 25; " rowspan="2"><img alt="[Note]" src="images/note.png"/></td><th style="text-align: left; ">Note</th></tr><tr><td style="text-align: left; vertical-align: top; "><p>PDX serialization is very efficient and supports OQL queries without requiring a custom domain class.
Use of custom domain types requires these classes to be in the class path of both the stream apps and the cache server.
For this reason, the use of custom payload types is generally discouraged.</p></td></tr></table></div><pre class="screen">dataflow:&gt;stream create --name stocks --definition "http --security.basic.enabled=false | gemfire --username=developer --password=&lt;developer-password&gt; --connect-type=locator --host-addresses=10.0.16.9:55221 --regionName=Stocks --keyExpression=payload.getField('symbol')" --deploy</pre></li><li class="listitem" epub:type="list-item"><p class="simpara">Verify the stream is successfully deployed</p><pre class="screen">dataflow:&gt;stream list</pre></li><li class="listitem" epub:type="list-item"><p class="simpara">Post sample data pointing to the <code class="literal">http</code> endpoint</p><p class="simpara">Get the url of the http source using <code class="literal">cf apps</code></p><pre class="screen">dataflow:&gt;http post --target http://&lt;http source url&gt; --contentType application/json --data '{"symbol":"VMW","price":117.06}'
&gt; POST (application/json) http://... {"symbol":"VMW","price":117.06}
&gt; 202 ACCEPTED</pre></li><li class="listitem" epub:type="list-item"><p class="simpara">Using <code class="literal">gfsh</code>, connect to the PCC instance as <code class="literal">cluster_operator</code> using the service key values.</p><pre class="screen">gfsh&gt;connect --use-http --url=&lt;gfsh-url&gt; --user=cluster_operator --password=&lt;cluster_operator_password&gt;
gfsh&gt;get --key='VMW' --region=/Stocks
Result      : true
Key Class   : java.lang.String
Key         : VMW
Value Class : org.apache.geode.pdx.internal.PdxInstanceImpl

symbol | price
------ | ------
VMW    | 117.06</pre></li><li class="listitem" epub:type="list-item">You’re done!</li></ol></div></section></section><section class="section" title="Summary" epub:type="division" id="_summary_3"><div class="titlepage"><div><div><h4 class="title">Summary</h4></div></div></div><p>In this sample, you have learned:</p><div class="itemizedlist" epub:type="list"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem" epub:type="list-item">How to use Spring Cloud Data Flow’s <code class="literal">Local</code> and <code class="literal">Cloud Foundry</code> servers</li><li class="listitem" epub:type="list-item">How to use Spring Cloud Data Flow’s <code class="literal">shell</code></li><li class="listitem" epub:type="list-item">How to create streaming data pipeline to connect and write to <code class="literal">gemfire</code></li></ul></div></section></section><section class="section" title="Gemfire to Log Demo" epub:type="division" id="spring-cloud-data-flow-samples-gemfire-log-overview"><div class="titlepage"><div><div><h3 class="title">Gemfire to Log Demo</h3></div></div></div><p>In this demonstration, you will learn how to build a data pipeline using <a class="link" href="http://cloud.spring.io/spring-cloud-dataflow/" target="_top">Spring Cloud Data Flow</a> to consume data from a <code class="literal">gemfire</code> endpoint and write to a log using the <code class="literal">log</code> sink.
The <code class="literal">gemfire</code> source creates a <a class="link" href="https://geode.apache.org/releases/latest/javadoc/org/apache/geode/cache/CacheListener.html" target="_top">CacheListener</a> to monitor events for a region and publish a message whenever an entry is changed.</p><p>We will take you through the steps to configure and run Spring Cloud Data Flow server in either a <a class="link" href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started/" target="_top">local</a> or <a class="link" href="https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current/reference/htmlsingle/#getting-started" target="_top">Cloud Foundry</a> environment.</p><div class="note" title="Note" epub:type="notice"><table style="border: 0; "><tr><td style="text-align: center; vertical-align: top; width: 25; " rowspan="2"><img alt="[Note]" src="images/note.png"/></td><th style="text-align: left; ">Note</th></tr><tr><td style="text-align: left; vertical-align: top; "><p>For legacy reasons the <code class="literal">gemfire</code> Spring Cloud Stream Apps are named after <code class="literal">Pivotal GemFire</code>. The code base for the commercial product has since been open sourced as <code class="literal">Apache Geode</code>. These samples should work with compatible versions of Pivotal GemFire or Apache Geode. Herein we will refer to the installed IMDG simply as <code class="literal">Geode</code>.</p></td></tr></table></div><section class="section" title="Prerequisites" epub:type="division" id="_prerequisites_4"><div class="titlepage"><div><div><h4 class="title">Prerequisites</h4></div></div></div><div class="itemizedlist" epub:type="list"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem" epub:type="list-item">A Running Data Flow Shell</li></ul></div><p>The Spring Cloud Data Flow Shell is available for <a class="link" href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started-deploying-spring-cloud-dataflow" target="_top">download</a> or you can <a class="link" href="https://github.com/spring-cloud/spring-cloud-dataflow" target="_top">build</a> it yourself.</p><div class="note" title="Note" epub:type="notice"><table style="border: 0; "><tr><td style="text-align: center; vertical-align: top; width: 25; " rowspan="2"><img alt="[Note]" src="images/note.png"/></td><th style="text-align: left; ">Note</th></tr><tr><td style="text-align: left; vertical-align: top; "><p>the Spring Cloud Data Flow Shell and Local server implementation are in the same repository and are both built by running <code class="literal">./mvnw install</code> from the project root directory. If you have already run the build, use the jar in <code class="literal">spring-cloud-dataflow-shell/target</code></p></td></tr></table></div><p>To run the Shell open a new terminal session:</p><pre class="screen">$ cd &lt;PATH/TO/SPRING-CLOUD-DATAFLOW-SHELL-JAR&gt;
$ java -jar spring-cloud-dataflow-shell-&lt;VERSION&gt;.jar
  ____                              ____ _                __
 / ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
 \___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
  ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
 |____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
  ____ |_|    _          __|___/                 __________
 |  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
 | | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
 | |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
 |____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/


Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
dataflow:&gt;</pre><div class="note" title="Note" epub:type="notice"><table style="border: 0; "><tr><td style="text-align: center; vertical-align: top; width: 25; " rowspan="2"><img alt="[Note]" src="images/note.png"/></td><th style="text-align: left; ">Note</th></tr><tr><td style="text-align: left; vertical-align: top; "><p>The Spring Cloud Data Flow Shell is a Spring Boot application that connects to the Data Flow Server’s REST API and supports a DSL that simplifies the process of defining a stream or task and managing its lifecycle. Most of these samples
use the shell. If you prefer, you can use the Data Flow UI <a class="link" href="http://localhost:9393/dashboard" target="_top">localhost:9393/dashboard</a>, (or wherever it the server is hosted) to perform the same operations.</p></td></tr></table></div><div class="itemizedlist" epub:type="list"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem" epub:type="list-item">A Geode installation with a locator and cache server running
Unresolved directive in streaming/gemfire/gemfire-log/overview.adoc - include::geode-setup.adoc[]</li></ul></div></section><section class="section" title="Using the Local Server" epub:type="division" id="gemfire-log-local"><div class="titlepage"><div><div><h4 class="title">Using the Local Server</h4></div></div></div><section class="section" title="Additional Prerequisites" epub:type="division" id="_additional_prerequisites_7"><div class="titlepage"><div><div><h5 class="title">Additional Prerequisites</h5></div></div></div><div class="itemizedlist" epub:type="list"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem" epub:type="list-item">A Running Data Flow Server</li></ul></div><p>The Local Data Flow Server is Spring Boot application available for <a class="link" href="http://cloud.spring.io/spring-cloud-dataflow/" target="_top">download</a> or you can <a class="link" href="https://github.com/spring-cloud/spring-cloud-dataflow" target="_top">build</a> it yourself.
If you build it yourself, the executable jar will be in <code class="literal">spring-cloud-dataflow-server-local/target</code></p><p>To run the Local Data Flow server Open a new terminal session:</p><pre class="screen">$cd  &lt;PATH/TO/SPRING-CLOUD-DATAFLOW-LOCAL-JAR&gt;
$java -jar spring-cloud-dataflow-server-local-&lt;VERSION&gt;.jar</pre><div class="itemizedlist" epub:type="list"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem" epub:type="list-item">A running instance of <a class="link" href="https://www.rabbitmq.com" target="_top">Rabbit MQ</a></li></ul></div><p/><div class="orderedlist" epub:type="list"><ol class="orderedlist" type="1"><li class="listitem" epub:type="list-item"><p class="simpara">Use gfsh to start a locator and server</p><pre class="screen">gfsh&gt;start locator --name=locator1
gfsh&gt;start server --name=server1</pre></li><li class="listitem" epub:type="list-item"><p class="simpara">Create a region called <code class="literal">Test</code></p><pre class="screen">gfsh&gt;create region --name Test --type=REPLICATE</pre></li></ol></div></section><section class="section" title="Use the Shell to create the sample stream." epub:type="division" id="_use_the_shell_to_create_the_sample_stream_2"><div class="titlepage"><div><div><h5 class="title">Use the Shell to create the sample stream.</h5></div></div></div><div class="orderedlist" epub:type="list"><ol class="orderedlist" type="1"><li class="listitem" epub:type="list-item"><p class="simpara"><a class="link" href="https://github.com/spring-cloud/spring-cloud-dataflow/blob/master/spring-cloud-dataflow-docs/src/main/asciidoc/streams.adoc#register-a-stream-app" target="_top">Register</a> the out-of-the-box applications for the Rabbit binder</p><div class="note" title="Note" epub:type="notice"><table style="border: 0; "><tr><td style="text-align: center; vertical-align: top; width: 25; " rowspan="2"><img alt="[Note]" src="images/note.png"/></td><th style="text-align: left; ">Note</th></tr><tr><td style="text-align: left; vertical-align: top; "><p>These samples assume that the Data Flow Server can access a remote Maven repository, <code class="literal"><a class="link" href="https://repo.spring.io/libs-release" target="_top">repo.spring.io/libs-release</a></code> by default. If your Data Flow server is running behind a firewall, or you are using a maven proxy preventing
access to public repositories, you will need to install the sample apps in your internal Maven repository and <a class="link" href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started-maven-configuration" target="_top">configure</a>
the server accordingly.  The sample applications are typically registered using Data Flow’s bulk import facility. For example, the Shell command <code class="literal">dataflow:&gt;app import --uri <a class="link" href="http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven" target="_top">bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</a></code> (The actual URI is release and binder specific so refer to the sample instructions for the actual URL).
The bulk import URI references a plain text file containing entries for all of the publicly available Spring Cloud Stream and Task applications published to <code class="literal"><a class="link" href="https://repo.spring.io" target="_top">repo.spring.io</a></code>. For example,
<code class="literal">source.http=maven://org.springframework.cloud.stream.app:http-source-rabbit:1.2.0.RELEASE</code> registers the <code class="literal">http</code> source app at the corresponding Maven address, relative to the remote repository(ies) configured for the
Data Flow server. The format is <code class="literal">maven://&lt;groupId&gt;:&lt;artifactId&gt;:&lt;version&gt;</code>  You will need to <a class="link" href="https://repo.spring.io/libs-release/org/springframework/cloud/stream/app/spring-cloud-stream-app-descriptor/Bacon.RELEASE/spring-cloud-stream-app-descriptor-Bacon.RELEASE.rabbit-apps-maven-repo-url.properties" target="_top">download</a> the required apps or <a class="link" href="https://github.com/spring-cloud-stream-app-starters" target="_top">build</a> them and then install them in your Maven repository, using whatever group, artifact, and version you choose. If you do
this, register individual apps using <code class="literal">dataflow:&gt;app register…​</code> using the <code class="literal">maven://</code> resource URI format corresponding to your installed app.</p></td></tr></table></div><pre class="screen">dataflow:&gt;app import --uri http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</pre></li><li class="listitem" epub:type="list-item"><p class="simpara">Create the stream</p><p class="simpara">This example creates an gemfire source to which will publish events on a region</p><pre class="screen">dataflow:&gt;stream create --name events --definition " gemfire --regionName=Test | log" --deploy
Created and deployed new stream 'events'</pre><div class="note" title="Note" epub:type="notice"><table style="border: 0; "><tr><td style="text-align: center; vertical-align: top; width: 25; " rowspan="2"><img alt="[Note]" src="images/note.png"/></td><th style="text-align: left; ">Note</th></tr><tr><td style="text-align: left; vertical-align: top; "><p>If the Geode locator isn’t running on default port on <code class="literal">localhost</code>, add the options <code class="literal">--connect-type=locator --host-addresses=&lt;host&gt;:&lt;port&gt;</code>. If there are multiple
locators, you can provide a comma separated list of locator addresses. This is not necessary for the sample but is typical for production environments to enable fail-over.</p></td></tr></table></div></li><li class="listitem" epub:type="list-item"><p class="simpara">Verify the stream is successfully deployed</p><pre class="screen">dataflow:&gt;stream list</pre></li><li class="listitem" epub:type="list-item"><p class="simpara">Monitor stdout for the log sink. When you deploy the stream, you will see log messages in the Data Flow server console like this</p><pre class="screen">2017-10-28 17:28:23.275  INFO 15603 --- [nio-9393-exec-2] o.s.c.d.spi.local.LocalAppDeployer       : Deploying app with deploymentId events.log instance 0.
   Logs will be in /var/folders/hd/5yqz2v2d3sxd3n879f4sg4gr0000gn/T/spring-cloud-dataflow-4093992067314402881/events-1509226103269/events.log
2017-10-28 17:28:23.277  INFO 15603 --- [nio-9393-exec-2] o.s.c.d.s.c.StreamDeploymentController   : Downloading resource URI [maven://org.springframework.cloud.stream.app:gemfire-source-rabbit:1.2.0.RELEASE]
2017-10-28 17:28:23.311  INFO 15603 --- [nio-9393-exec-2] o.s.c.d.s.c.StreamDeploymentController   : Deploying application named [gemfire] as part of stream named [events] with resource URI [maven://org.springframework.cloud.stream.app:gemfire-source-rabbit:1.2.0.RELEASE]
2017-10-28 17:28:23.318  INFO 15603 --- [nio-9393-exec-2] o.s.c.d.spi.local.LocalAppDeployer       : Deploying app with deploymentId events.gemfire instance 0.
   Logs will be in /var/folders/hd/5yqz2v2d3sxd3n879f4sg4gr0000gn/T/spring-cloud-dataflow-4093992067314402881/events-1509226103311/events.gemfire</pre><p class="simpara">Copy the location of the <code class="literal">log</code> sink logs. This is a directory that ends in <code class="literal">events.log</code>. The log files will be in <code class="literal">stdout_0.log</code> under this directory. You can monitor the output of the log sink using <code class="literal">tail</code>, or something similar:</p><pre class="screen">$tail -f /var/folders/hd/5yqz2v2d3sxd3n879f4sg4gr0000gn/T/spring-cloud-dataflow-4093992067314402881/events-1509226103269/events.log/stdout_0.log</pre></li><li class="listitem" epub:type="list-item"><p class="simpara">Using <code class="literal">gfsh</code>, create and update some cache entries</p><pre class="screen">gfsh&gt;put --region /Test --key 1  --value "value 1"
gfsh&gt;put --region /Test --key 2  --value "value 2"
gfsh&gt;put --region /Test --key 3  --value "value 3"
gfsh&gt;put --region /Test --key 1  --value "new value 1"</pre></li><li class="listitem" epub:type="list-item"><p class="simpara">Observe the log output
You should see messages like:</p><pre class="screen">2017-10-28 17:28:52.893  INFO 18986 --- [emfire.events-1] log sink                               : value 1"
2017-10-28 17:28:52.893  INFO 18986 --- [emfire.events-1] log sink                               : value 2"
2017-10-28 17:28:52.893  INFO 18986 --- [emfire.events-1] log sink                               : value 3"
2017-10-28 17:28:52.893  INFO 18986 --- [emfire.events-1] log sink                               : new value 1"</pre><p class="simpara">By default, the message payload contains the updated value. Depending on your application, you may need additional information. The data comes from <a class="link" href="https://geode.apache.org/releases/latest/javadoc/org/apache/geode/cache/EntryEvent.html" target="_top">EntryEvent</a>. You
can access any fields using the source’s <code class="literal">cache-event-expression</code> property. This takes a SpEL expression bound to the EntryEvent. Try something like <code class="literal">--cache-event-expression='{key:'+key+',new_value:'+newValue+'}'</code> (HINT: You will need to destroy the stream and recreate it to
add this property, an exercise left to the reader). Now you should see log messages like:</p><pre class="screen">2017-10-28 17:28:52.893  INFO 18986 --- [emfire.events-1] log-sink                                 : {key:1,new_value:value 1}
2017-10-28 17:41:24.466  INFO 18986 --- [emfire.events-1] log-sink                                 : {key:2,new_value:value 2}</pre></li><li class="listitem" epub:type="list-item">You’re done!</li></ol></div></section></section><section class="section" title="Using the Cloud Foundry Server" epub:type="division" id="gemfire-log-cf"><div class="titlepage"><div><div><h4 class="title">Using the Cloud Foundry Server</h4></div></div></div><section class="section" title="Additional Prerequisites" epub:type="division" id="_additional_prerequisites_8"><div class="titlepage"><div><div><h5 class="title">Additional Prerequisites</h5></div></div></div><div class="itemizedlist" epub:type="list"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem" epub:type="list-item">A Cloud Foundry instance</li><li class="listitem" epub:type="list-item">The Spring Cloud Data Flow Cloud Foundry Server</li></ul></div><p>The Cloud Foundry Data Flow Server is Spring Boot application available for <a class="link" href="http://cloud.spring.io/spring-cloud-dataflow/" target="_top">download</a> or you can <a class="link" href="https://github.com/spring-cloud/spring-cloud-dataflow-server-cloudfoundry" target="_top">build</a> it yourself.
If you build it yourself, the executable jar will be in <code class="literal">spring-cloud-dataflow-server-cloudfoundry/target</code></p><div class="note" title="Note" epub:type="notice"><table style="border: 0; "><tr><td style="text-align: center; vertical-align: top; width: 25; " rowspan="2"><img alt="[Note]" src="images/note.png"/></td><th style="text-align: left; ">Note</th></tr><tr><td style="text-align: left; vertical-align: top; "><p>Although you can run the Data Flow Cloud Foundry Server locally and configure it to deploy to any Cloud Foundry instance, we will
deploy the server to Cloud Foundry as recommended.</p></td></tr></table></div><div class="orderedlist" epub:type="list"><ol class="orderedlist" type="1"><li class="listitem" epub:type="list-item"><p class="simpara">Verify that CF instance is reachable (Your endpoint urls will be different from what is shown here).</p><pre class="screen">$ cf api
API endpoint: https://api.system.io (API version: ...)

$ cf apps
Getting apps in org [your-org] / space [your-space] as user...
OK

No apps found</pre></li><li class="listitem" epub:type="list-item"><p class="simpara">Follow the instructions to deploy the <a class="link" href="https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current/reference/htmlsingle" target="_top">Spring Cloud Data Flow Cloud Foundry server</a>. Don’t worry about creating a Redis service. We won’t need it. If you are familiar with Cloud Foundry
application manifests, we recommend creating a manifest for the the Data Flow server as shown <a class="link" href="https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current/reference/htmlsingle/#sample-manifest-template" target="_top">here</a>.</p><div class="warning" title="Warning" epub:type="warning"><table style="border: 0; "><tr><td style="text-align: center; vertical-align: top; width: 25; " rowspan="2"><img alt="[Warning]" src="images/warning.png"/></td><th style="text-align: left; ">Warning</th></tr><tr><td style="text-align: left; vertical-align: top; "><p>As of this writing, there is a typo on the <code class="literal">SPRING_APPLICATION_JSON</code> entry in the sample manifest. <code class="literal">SPRING_APPLICATION_JSON</code> must be followed by <code class="literal">:</code> and The JSON string must be
wrapped in single quotes. Alternatively, you can replace that line with <code class="literal">MAVEN_REMOTE_REPOSITORIES_REPO1_URL: <a class="link" href="https://repo.spring.io/libs-snapshot" target="_top">repo.spring.io/libs-snapshot</a></code>.  If your Cloud Foundry installation is behind a firewall, you may need to install the stream apps used in this sample in your internal Maven repository and <a class="link" href="https://docs.spring.io/spring-cloud-dataflow/docs/1.3.0.M2/reference/htmlsingle/#getting-started-maven-configuration" target="_top">configure</a> the server to access that repository.</p></td></tr></table></div></li><li class="listitem" epub:type="list-item"><p class="simpara">Once you have successfully executed <code class="literal">cf push</code>, verify the dataflow server is running</p><pre class="screen">$ cf apps
Getting apps in org [your-org] / space [your-space] as user...
OK

name                 requested state   instances   memory   disk   urls
dataflow-server      started           1/1         1G       1G     dataflow-server.app.io</pre></li><li class="listitem" epub:type="list-item">Notice that the <code class="literal">dataflow-server</code> application is started and ready for interaction via the url endpoint</li><li class="listitem" epub:type="list-item"><p class="simpara">Connect the <code class="literal">shell</code> with <code class="literal">server</code> running on Cloud Foundry, e.g., <code class="literal"><a class="link" href="http://dataflow-server.app.io" target="_top">dataflow-server.app.io</a></code></p><pre class="screen">$ cd &lt;PATH/TO/SPRING-CLOUD-DATAFLOW-SHELL-JAR&gt;
$ java -jar spring-cloud-dataflow-shell-&lt;VERSION&gt;.jar

  ____                              ____ _                __
 / ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
 \___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
  ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
 |____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
  ____ |_|    _          __|___/                 __________
 |  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
 | | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
 | |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
 |____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/


Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
server-unknown:&gt;</pre><pre class="screen">server-unknown:&gt;dataflow config server http://dataflow-server.app.io
Successfully targeted http://dataflow-server.app.io
dataflow:&gt;</pre><div class="itemizedlist" epub:type="list"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem" epub:type="list-item">Running instance of a <code class="literal">rabbit</code> service in Cloud Foundry</li><li class="listitem" epub:type="list-item">Running instance of the <a class="link" href="https://docs.pivotal.io/p-cloud-cache/1-0/developer.html" target="_top">Pivotal Cloud Cache for PCF</a> (PCC) service <code class="literal">cloudcache</code> in Cloud Foundry.</li></ul></div></li><li class="listitem" epub:type="list-item"><p class="simpara"><a class="link" href="https://github.com/spring-cloud/spring-cloud-dataflow/blob/master/spring-cloud-dataflow-docs/src/main/asciidoc/streams.adoc#register-a-stream-app" target="_top">Register</a> the out-of-the-box applications for the Rabbit binder</p><div class="note" title="Note" epub:type="notice"><table style="border: 0; "><tr><td style="text-align: center; vertical-align: top; width: 25; " rowspan="2"><img alt="[Note]" src="images/note.png"/></td><th style="text-align: left; ">Note</th></tr><tr><td style="text-align: left; vertical-align: top; "><p>These samples assume that the Data Flow Server can access a remote Maven repository, <code class="literal"><a class="link" href="https://repo.spring.io/libs-release" target="_top">repo.spring.io/libs-release</a></code> by default. If your Data Flow server is running behind a firewall, or you are using a maven proxy preventing
access to public repositories, you will need to install the sample apps in your internal Maven repository and <a class="link" href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started-maven-configuration" target="_top">configure</a>
the server accordingly.  The sample applications are typically registered using Data Flow’s bulk import facility. For example, the Shell command <code class="literal">dataflow:&gt;app import --uri <a class="link" href="http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven" target="_top">bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</a></code> (The actual URI is release and binder specific so refer to the sample instructions for the actual URL).
The bulk import URI references a plain text file containing entries for all of the publicly available Spring Cloud Stream and Task applications published to <code class="literal"><a class="link" href="https://repo.spring.io" target="_top">repo.spring.io</a></code>. For example,
<code class="literal">source.http=maven://org.springframework.cloud.stream.app:http-source-rabbit:1.2.0.RELEASE</code> registers the <code class="literal">http</code> source app at the corresponding Maven address, relative to the remote repository(ies) configured for the
Data Flow server. The format is <code class="literal">maven://&lt;groupId&gt;:&lt;artifactId&gt;:&lt;version&gt;</code>  You will need to <a class="link" href="https://repo.spring.io/libs-release/org/springframework/cloud/stream/app/spring-cloud-stream-app-descriptor/Bacon.RELEASE/spring-cloud-stream-app-descriptor-Bacon.RELEASE.rabbit-apps-maven-repo-url.properties" target="_top">download</a> the required apps or <a class="link" href="https://github.com/spring-cloud-stream-app-starters" target="_top">build</a> them and then install them in your Maven repository, using whatever group, artifact, and version you choose. If you do
this, register individual apps using <code class="literal">dataflow:&gt;app register…​</code> using the <code class="literal">maven://</code> resource URI format corresponding to your installed app.</p></td></tr></table></div><pre class="screen">dataflow:&gt;app import --uri http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</pre></li><li class="listitem" epub:type="list-item"><p class="simpara">Get the PCC connection information</p><pre class="screen">$ cf service-key cloudcache my-service-key
Getting key my-service-key for service instance cloudcache as &lt;user&gt;...

{
 "locators": [
  "10.0.16.9[55221]",
  "10.0.16.11[55221]",
  "10.0.16.10[55221]"
 ],
 "urls": {
  "gfsh": "http://...",
  "pulse": "http://.../pulse"
 },
 "users": [
  {
   "password": &lt;password&gt;,
   "username": "cluster_operator"
  },
  {
   "password": &lt;password&gt;,
   "username": "developer"
  }
 ]
}</pre></li><li class="listitem" epub:type="list-item"><p class="simpara">Using <code class="literal">gfsh</code>, connect to the PCC instance as <code class="literal">cluster_operator</code> using the service key values and create the Test region.</p><pre class="screen">gfsh&gt;connect --use-http --url=&lt;gfsh-url&gt; --user=cluster_operator --password=&lt;cluster_operator_password&gt;
gfsh&gt;create region --name Test --type=REPLICATE</pre></li><li class="listitem" epub:type="list-item"><p class="simpara">Create the stream, connecting to the PCC instance as developer. This example creates an gemfire source to which will publish events on a region</p><pre class="screen">dataflow stream create --name events --definition " gemfire --username=developer --password=&lt;developer-password&gt; --connect-type=locator --host-addresses=10.0.16.9:55221 --regionName=Test | log" --deploy</pre></li><li class="listitem" epub:type="list-item"><p class="simpara">Verify the stream is successfully deployed</p><pre class="screen">dataflow:&gt;stream list</pre></li><li class="listitem" epub:type="list-item"><p class="simpara">Monitor stdout for the log sink</p><pre class="screen">cf logs &lt;log-sink-app-name&gt;</pre></li><li class="listitem" epub:type="list-item"><p class="simpara">Using <code class="literal">gfsh</code>, create and update some cache entries</p><pre class="screen">gfsh&gt;connect --use-http --url=&lt;gfsh-url&gt; --user=cluster_operator --password=&lt;cluster_operator_password&gt;
gfsh&gt;put --region /Test --key 1  --value "value 1"
gfsh&gt;put --region /Test --key 2  --value "value 2"
gfsh&gt;put --region /Test --key 3  --value "value 3"
gfsh&gt;put --region /Test --key 1  --value "new value 1"</pre></li><li class="listitem" epub:type="list-item"><p class="simpara">Observe the log output</p><p class="simpara">You should see messages like:</p><pre class="screen">2017-10-28 17:28:52.893  INFO 18986 --- [emfire.events-1] log sink                               : value 1"
2017-10-28 17:28:52.893  INFO 18986 --- [emfire.events-1] log sink                               : value 2"
2017-10-28 17:28:52.893  INFO 18986 --- [emfire.events-1] log sink                               : value 3"
2017-10-28 17:28:52.893  INFO 18986 --- [emfire.events-1] log sink                               : new value 1"</pre><p class="simpara">By default, the message payload contains the updated value. Depending on your application, you may need additional information. The data comes from <a class="link" href="https://geode.apache.org/releases/latest/javadoc/org/apache/geode/cache/EntryEvent.html" target="_top">EntryEvent</a>. You
can access any fields using the source’s <code class="literal">cache-event-expression</code> property. This takes a SpEL expression bound to the EntryEvent. Try something like <code class="literal">--cache-event-expression='{key:'+key+',new_value:'+newValue+'}'</code> (HINT: You will need to destroy the stream and recreate it to
add this property, an exercise left to the reader). Now you should see log messages like:</p><pre class="screen">2017-10-28 17:28:52.893  INFO 18986 --- [emfire.events-1] log-sink                                 : {key:1,new_value:value 1}
2017-10-28 17:41:24.466  INFO 18986 --- [emfire.events-1] log-sink                                 : {key:2,new_value:value 2}</pre></li><li class="listitem" epub:type="list-item">You’re done!</li></ol></div></section></section><section class="section" title="Summary" epub:type="division" id="_summary_4"><div class="titlepage"><div><div><h4 class="title">Summary</h4></div></div></div><p>In this sample, you have learned:</p><div class="itemizedlist" epub:type="list"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem" epub:type="list-item">How to use Spring Cloud Data Flow’s <code class="literal">Local</code> and <code class="literal">Cloud Foundry</code> servers</li><li class="listitem" epub:type="list-item">How to use Spring Cloud Data Flow’s <code class="literal">shell</code></li><li class="listitem" epub:type="list-item">How to create streaming data pipeline to connect and publish events from <code class="literal">gemfire</code></li></ul></div></section></section><section class="section" title="Gemfire CQ to Log Demo" epub:type="division" id="spring-cloud-data-flow-samples-gemfire-cq-log-overview"><div class="titlepage"><div><div><h3 class="title">Gemfire CQ to Log Demo</h3></div></div></div><p>In this demonstration, you will learn how to build a data pipeline using <a class="link" href="http://cloud.spring.io/spring-cloud-dataflow/" target="_top">Spring Cloud Data Flow</a> to consume data from a <code class="literal">gemfire-cq</code> (Continuous Query) endpoint and write to a log using the <code class="literal">log</code> sink.
The <code class="literal">gemfire-cq</code> source creates a Continuous Query to monitor events for a region that match the query’s result set and publish a message whenever such an event is emitted. In this example, we simulate monitoring orders to trigger a process whenever
the quantity ordered is above a defined limit.</p><p>We will take you through the steps to configure and run Spring Cloud Data Flow server in either a <a class="link" href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started/" target="_top">local</a> or <a class="link" href="https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current/reference/htmlsingle/#getting-started" target="_top">Cloud Foundry</a> environment.</p><div class="note" title="Note" epub:type="notice"><table style="border: 0; "><tr><td style="text-align: center; vertical-align: top; width: 25; " rowspan="2"><img alt="[Note]" src="images/note.png"/></td><th style="text-align: left; ">Note</th></tr><tr><td style="text-align: left; vertical-align: top; "><p>For legacy reasons the <code class="literal">gemfire</code> Spring Cloud Stream Apps are named after <code class="literal">Pivotal GemFire</code>. The code base for the commercial product has since been open sourced as <code class="literal">Apache Geode</code>. These samples should work with compatible versions of Pivotal GemFire or Apache Geode. Herein we will refer to the installed IMDG simply as <code class="literal">Geode</code>.</p></td></tr></table></div><section class="section" title="Prerequisites" epub:type="division" id="_prerequisites_5"><div class="titlepage"><div><div><h4 class="title">Prerequisites</h4></div></div></div><div class="itemizedlist" epub:type="list"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem" epub:type="list-item">A Running Data Flow Shell</li></ul></div><p>The Spring Cloud Data Flow Shell is available for <a class="link" href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started-deploying-spring-cloud-dataflow" target="_top">download</a> or you can <a class="link" href="https://github.com/spring-cloud/spring-cloud-dataflow" target="_top">build</a> it yourself.</p><div class="note" title="Note" epub:type="notice"><table style="border: 0; "><tr><td style="text-align: center; vertical-align: top; width: 25; " rowspan="2"><img alt="[Note]" src="images/note.png"/></td><th style="text-align: left; ">Note</th></tr><tr><td style="text-align: left; vertical-align: top; "><p>the Spring Cloud Data Flow Shell and Local server implementation are in the same repository and are both built by running <code class="literal">./mvnw install</code> from the project root directory. If you have already run the build, use the jar in <code class="literal">spring-cloud-dataflow-shell/target</code></p></td></tr></table></div><p>To run the Shell open a new terminal session:</p><pre class="screen">$ cd &lt;PATH/TO/SPRING-CLOUD-DATAFLOW-SHELL-JAR&gt;
$ java -jar spring-cloud-dataflow-shell-&lt;VERSION&gt;.jar
  ____                              ____ _                __
 / ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
 \___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
  ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
 |____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
  ____ |_|    _          __|___/                 __________
 |  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
 | | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
 | |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
 |____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/


Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
dataflow:&gt;</pre><div class="note" title="Note" epub:type="notice"><table style="border: 0; "><tr><td style="text-align: center; vertical-align: top; width: 25; " rowspan="2"><img alt="[Note]" src="images/note.png"/></td><th style="text-align: left; ">Note</th></tr><tr><td style="text-align: left; vertical-align: top; "><p>The Spring Cloud Data Flow Shell is a Spring Boot application that connects to the Data Flow Server’s REST API and supports a DSL that simplifies the process of defining a stream or task and managing its lifecycle. Most of these samples
use the shell. If you prefer, you can use the Data Flow UI <a class="link" href="http://localhost:9393/dashboard" target="_top">localhost:9393/dashboard</a>, (or wherever it the server is hosted) to perform the same operations.</p></td></tr></table></div><div class="itemizedlist" epub:type="list"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem" epub:type="list-item">A Geode installation with a locator and cache server running
Unresolved directive in streaming/gemfire/gemfire-cq-log/overview.adoc - include::geode-setup.adoc[]</li></ul></div></section><section class="section" title="Using the Local Server" epub:type="division" id="gemfire-cq-log-local"><div class="titlepage"><div><div><h4 class="title">Using the Local Server</h4></div></div></div><section class="section" title="Additional Prerequisites" epub:type="division" id="_additional_prerequisites_9"><div class="titlepage"><div><div><h5 class="title">Additional Prerequisites</h5></div></div></div><div class="itemizedlist" epub:type="list"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem" epub:type="list-item">A Running Data Flow Server</li></ul></div><p>The Local Data Flow Server is Spring Boot application available for <a class="link" href="http://cloud.spring.io/spring-cloud-dataflow/" target="_top">download</a> or you can <a class="link" href="https://github.com/spring-cloud/spring-cloud-dataflow" target="_top">build</a> it yourself.
If you build it yourself, the executable jar will be in <code class="literal">spring-cloud-dataflow-server-local/target</code></p><p>To run the Local Data Flow server Open a new terminal session:</p><pre class="screen">$cd  &lt;PATH/TO/SPRING-CLOUD-DATAFLOW-LOCAL-JAR&gt;
$java -jar spring-cloud-dataflow-server-local-&lt;VERSION&gt;.jar</pre><div class="itemizedlist" epub:type="list"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem" epub:type="list-item">A running instance of <a class="link" href="https://www.rabbitmq.com" target="_top">Rabbit MQ</a></li></ul></div><p/><div class="orderedlist" epub:type="list"><ol class="orderedlist" type="1"><li class="listitem" epub:type="list-item"><p class="simpara">Use gfsh to start a locator and server</p><pre class="screen">gfsh&gt;start locator --name=locator1
gfsh&gt;start server --name=server1</pre></li><li class="listitem" epub:type="list-item"><p class="simpara">Create a region called <code class="literal">Orders</code></p><pre class="screen">gfsh&gt;create region --name Orders --type=REPLICATE</pre><p class="simpara">===== Use the Shell to create the sample stream.</p></li><li class="listitem" epub:type="list-item"><p class="simpara"><a class="link" href="https://github.com/spring-cloud/spring-cloud-dataflow/blob/master/spring-cloud-dataflow-docs/src/main/asciidoc/streams.adoc#register-a-stream-app" target="_top">Register</a> the out-of-the-box applications for the Rabbit binder</p><div class="note" title="Note" epub:type="notice"><table style="border: 0; "><tr><td style="text-align: center; vertical-align: top; width: 25; " rowspan="2"><img alt="[Note]" src="images/note.png"/></td><th style="text-align: left; ">Note</th></tr><tr><td style="text-align: left; vertical-align: top; "><p>These samples assume that the Data Flow Server can access a remote Maven repository, <code class="literal"><a class="link" href="https://repo.spring.io/libs-release" target="_top">repo.spring.io/libs-release</a></code> by default. If your Data Flow server is running behind a firewall, or you are using a maven proxy preventing
access to public repositories, you will need to install the sample apps in your internal Maven repository and <a class="link" href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started-maven-configuration" target="_top">configure</a>
the server accordingly.  The sample applications are typically registered using Data Flow’s bulk import facility. For example, the Shell command <code class="literal">dataflow:&gt;app import --uri <a class="link" href="http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven" target="_top">bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</a></code> (The actual URI is release and binder specific so refer to the sample instructions for the actual URL).
The bulk import URI references a plain text file containing entries for all of the publicly available Spring Cloud Stream and Task applications published to <code class="literal"><a class="link" href="https://repo.spring.io" target="_top">repo.spring.io</a></code>. For example,
<code class="literal">source.http=maven://org.springframework.cloud.stream.app:http-source-rabbit:1.2.0.RELEASE</code> registers the <code class="literal">http</code> source app at the corresponding Maven address, relative to the remote repository(ies) configured for the
Data Flow server. The format is <code class="literal">maven://&lt;groupId&gt;:&lt;artifactId&gt;:&lt;version&gt;</code>  You will need to <a class="link" href="https://repo.spring.io/libs-release/org/springframework/cloud/stream/app/spring-cloud-stream-app-descriptor/Bacon.RELEASE/spring-cloud-stream-app-descriptor-Bacon.RELEASE.rabbit-apps-maven-repo-url.properties" target="_top">download</a> the required apps or <a class="link" href="https://github.com/spring-cloud-stream-app-starters" target="_top">build</a> them and then install them in your Maven repository, using whatever group, artifact, and version you choose. If you do
this, register individual apps using <code class="literal">dataflow:&gt;app register…​</code> using the <code class="literal">maven://</code> resource URI format corresponding to your installed app.</p></td></tr></table></div><pre class="screen">dataflow:&gt;app import --uri http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</pre></li><li class="listitem" epub:type="list-item"><p class="simpara">Create the stream</p><p class="simpara">This example creates an gemfire-cq source to which will publish events matching a query criteria on a region. In this case we will monitor the <code class="literal">Orders</code> region. For simplicity, we will avoid creating a data structure for the order.
Each cache entry contains an integer value representing the quantity of the ordered item. This stream will fire a message whenever the value&gt;999. By default, the source emits only the value. Here we will override that using the
<code class="literal">cq-event-expression</code> property.  This accepts a SpEL expression bound to a <a class="link" href="https://geode.apache.org/releases/latest/javadoc/org/apache/geode/cache/query/CqEvent.html" target="_top">CQEvent</a>. To reference the entire CQEvent instace, we use <code class="literal">#this</code>.
In order to display the contents in the log, we will invoke <code class="literal">toString()</code> on the instance.</p><pre class="screen">dataflow:&gt;stream create --name orders --definition " gemfire-cq --query='SELECT * from /Orders o where o &gt; 999' --cq-event-expression=#this.toString() | log" --deploy
Created and deployed new stream 'events'</pre><div class="note" title="Note" epub:type="notice"><table style="border: 0; "><tr><td style="text-align: center; vertical-align: top; width: 25; " rowspan="2"><img alt="[Note]" src="images/note.png"/></td><th style="text-align: left; ">Note</th></tr><tr><td style="text-align: left; vertical-align: top; "><p>If the Geode locator isn’t running on default port on <code class="literal">localhost</code>, add the options <code class="literal">--connect-type=locator --host-addresses=&lt;host&gt;:&lt;port&gt;</code>. If there are multiple
locators, you can provide a comma separated list of locator addresses. This is not necessary for the sample but is typical for production environments to enable fail-over.</p></td></tr></table></div></li><li class="listitem" epub:type="list-item"><p class="simpara">Verify the stream is successfully deployed</p><pre class="screen">dataflow:&gt;stream list</pre></li><li class="listitem" epub:type="list-item"><p class="simpara">Monitor stdout for the log sink. When you deploy the stream, you will see log messages in the Data Flow server console like this</p><pre class="screen">2017-10-30 09:39:36.283  INFO 8167 --- [nio-9393-exec-5] o.s.c.d.spi.local.LocalAppDeployer       : Deploying app with deploymentId orders.log instance 0.
   Logs will be in /var/folders/hd/5yqz2v2d3sxd3n879f4sg4gr0000gn/T/spring-cloud-dataflow-5375107584795488581/orders-1509370775940/orders.log</pre><p class="simpara">Copy the location of the <code class="literal">log</code> sink logs. This is a directory that ends in <code class="literal">orders.log</code>. The log files will be in <code class="literal">stdout_0.log</code> under this directory. You can monitor the output of the log sink using <code class="literal">tail</code>, or something similar:</p><pre class="screen">$tail -f /var/folders/hd/5yqz2v2d3sxd3n879f4sg4gr0000gn/T/spring-cloud-dataflow-5375107584795488581/orders-1509370775940/orders.log/stdout_0.log</pre></li><li class="listitem" epub:type="list-item"><p class="simpara">Using <code class="literal">gfsh</code>, create and update some cache entries</p><pre class="screen">gfsh&gt;put --region Orders  --value-class java.lang.Integer --key 01234 --value 1000
gfsh&gt;put --region Orders  --value-class java.lang.Integer --key 11234 --value 1005
gfsh&gt;put --region Orders  --value-class java.lang.Integer --key 21234 --value 100
gfsh&gt;put --region Orders  --value-class java.lang.Integer --key 31234 --value 999
gfsh&gt;put --region Orders  --value-class java.lang.Integer --key 21234 --value 1000</pre></li><li class="listitem" epub:type="list-item"><p class="simpara">Observe the log output
You should see messages like:</p><pre class="screen">2017-10-30 09:53:02.231  INFO 8563 --- [ire-cq.orders-1] log-sink                                 : CqEvent [CqName=GfCq1; base operation=CREATE; cq operation=CREATE; key=01234; value=1000]
2017-10-30 09:53:19.732  INFO 8563 --- [ire-cq.orders-1] log-sink                                 : CqEvent [CqName=GfCq1; base operation=CREATE; cq operation=CREATE; key=11234; value=1005]
2017-10-30 09:53:53.242  INFO 8563 --- [ire-cq.orders-1] log-sink                                 : CqEvent [CqName=GfCq1; base operation=UPDATE; cq operation=CREATE; key=21234; value=1000]</pre></li><li class="listitem" epub:type="list-item">Another interesting demonstration combines <code class="literal">gemfire-cq</code> with the <a class="link" href=":../http-gemfire/README.adoc" target="_top">http-gemfire</a> example.</li></ol></div><pre class="screen">dataflow:&gt; stream create --name stocks --definition "http --port=9090 | gemfire-json-server --regionName=Stocks --keyExpression=payload.getField('symbol')" --deploy
dataflow:&gt; stream create --name stock_watch --definition "gemfire-cq --query='Select * from /Stocks where symbol=''VMW''' | log" --deploy</pre><div class="orderedlist" epub:type="list"><ol class="orderedlist" type="1"><li class="listitem" epub:type="list-item">You’re done!</li></ol></div></section></section><section class="section" title="Using the Cloud Foundry Server" epub:type="division" id="gemfire-cq-log-cf"><div class="titlepage"><div><div><h4 class="title">Using the Cloud Foundry Server</h4></div></div></div><section class="section" title="Additional Prerequisites" epub:type="division" id="_additional_prerequisites_10"><div class="titlepage"><div><div><h5 class="title">Additional Prerequisites</h5></div></div></div><div class="itemizedlist" epub:type="list"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem" epub:type="list-item">A Cloud Foundry instance</li><li class="listitem" epub:type="list-item">The Spring Cloud Data Flow Cloud Foundry Server</li></ul></div><p>The Cloud Foundry Data Flow Server is Spring Boot application available for <a class="link" href="http://cloud.spring.io/spring-cloud-dataflow/" target="_top">download</a> or you can <a class="link" href="https://github.com/spring-cloud/spring-cloud-dataflow-server-cloudfoundry" target="_top">build</a> it yourself.
If you build it yourself, the executable jar will be in <code class="literal">spring-cloud-dataflow-server-cloudfoundry/target</code></p><div class="note" title="Note" epub:type="notice"><table style="border: 0; "><tr><td style="text-align: center; vertical-align: top; width: 25; " rowspan="2"><img alt="[Note]" src="images/note.png"/></td><th style="text-align: left; ">Note</th></tr><tr><td style="text-align: left; vertical-align: top; "><p>Although you can run the Data Flow Cloud Foundry Server locally and configure it to deploy to any Cloud Foundry instance, we will
deploy the server to Cloud Foundry as recommended.</p></td></tr></table></div><div class="orderedlist" epub:type="list"><ol class="orderedlist" type="1"><li class="listitem" epub:type="list-item"><p class="simpara">Verify that CF instance is reachable (Your endpoint urls will be different from what is shown here).</p><pre class="screen">$ cf api
API endpoint: https://api.system.io (API version: ...)

$ cf apps
Getting apps in org [your-org] / space [your-space] as user...
OK

No apps found</pre></li><li class="listitem" epub:type="list-item"><p class="simpara">Follow the instructions to deploy the <a class="link" href="https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current/reference/htmlsingle" target="_top">Spring Cloud Data Flow Cloud Foundry server</a>. Don’t worry about creating a Redis service. We won’t need it. If you are familiar with Cloud Foundry
application manifests, we recommend creating a manifest for the the Data Flow server as shown <a class="link" href="https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current/reference/htmlsingle/#sample-manifest-template" target="_top">here</a>.</p><div class="warning" title="Warning" epub:type="warning"><table style="border: 0; "><tr><td style="text-align: center; vertical-align: top; width: 25; " rowspan="2"><img alt="[Warning]" src="images/warning.png"/></td><th style="text-align: left; ">Warning</th></tr><tr><td style="text-align: left; vertical-align: top; "><p>As of this writing, there is a typo on the <code class="literal">SPRING_APPLICATION_JSON</code> entry in the sample manifest. <code class="literal">SPRING_APPLICATION_JSON</code> must be followed by <code class="literal">:</code> and The JSON string must be
wrapped in single quotes. Alternatively, you can replace that line with <code class="literal">MAVEN_REMOTE_REPOSITORIES_REPO1_URL: <a class="link" href="https://repo.spring.io/libs-snapshot" target="_top">repo.spring.io/libs-snapshot</a></code>.  If your Cloud Foundry installation is behind a firewall, you may need to install the stream apps used in this sample in your internal Maven repository and <a class="link" href="https://docs.spring.io/spring-cloud-dataflow/docs/1.3.0.M2/reference/htmlsingle/#getting-started-maven-configuration" target="_top">configure</a> the server to access that repository.</p></td></tr></table></div></li><li class="listitem" epub:type="list-item"><p class="simpara">Once you have successfully executed <code class="literal">cf push</code>, verify the dataflow server is running</p><pre class="screen">$ cf apps
Getting apps in org [your-org] / space [your-space] as user...
OK

name                 requested state   instances   memory   disk   urls
dataflow-server      started           1/1         1G       1G     dataflow-server.app.io</pre></li><li class="listitem" epub:type="list-item">Notice that the <code class="literal">dataflow-server</code> application is started and ready for interaction via the url endpoint</li><li class="listitem" epub:type="list-item"><p class="simpara">Connect the <code class="literal">shell</code> with <code class="literal">server</code> running on Cloud Foundry, e.g., <code class="literal"><a class="link" href="http://dataflow-server.app.io" target="_top">dataflow-server.app.io</a></code></p><pre class="screen">$ cd &lt;PATH/TO/SPRING-CLOUD-DATAFLOW-SHELL-JAR&gt;
$ java -jar spring-cloud-dataflow-shell-&lt;VERSION&gt;.jar

  ____                              ____ _                __
 / ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
 \___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
  ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
 |____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
  ____ |_|    _          __|___/                 __________
 |  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
 | | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
 | |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
 |____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/


Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
server-unknown:&gt;</pre><pre class="screen">server-unknown:&gt;dataflow config server http://dataflow-server.app.io
Successfully targeted http://dataflow-server.app.io
dataflow:&gt;</pre><div class="itemizedlist" epub:type="list"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem" epub:type="list-item">Running instance of a <code class="literal">rabbit</code> service in Cloud Foundry</li><li class="listitem" epub:type="list-item">Running instance of the <a class="link" href="https://docs.pivotal.io/p-cloud-cache/1-0/developer.html" target="_top">Pivotal Cloud Cache for PCF</a> (PCC) service <code class="literal">cloudcache</code> in Cloud Foundry.</li></ul></div></li><li class="listitem" epub:type="list-item"><p class="simpara"><a class="link" href="https://github.com/spring-cloud/spring-cloud-dataflow/blob/master/spring-cloud-dataflow-docs/src/main/asciidoc/streams.adoc#register-a-stream-app" target="_top">Register</a> the out-of-the-box applications for the Rabbit binder</p><div class="note" title="Note" epub:type="notice"><table style="border: 0; "><tr><td style="text-align: center; vertical-align: top; width: 25; " rowspan="2"><img alt="[Note]" src="images/note.png"/></td><th style="text-align: left; ">Note</th></tr><tr><td style="text-align: left; vertical-align: top; "><p>These samples assume that the Data Flow Server can access a remote Maven repository, <code class="literal"><a class="link" href="https://repo.spring.io/libs-release" target="_top">repo.spring.io/libs-release</a></code> by default. If your Data Flow server is running behind a firewall, or you are using a maven proxy preventing
access to public repositories, you will need to install the sample apps in your internal Maven repository and <a class="link" href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started-maven-configuration" target="_top">configure</a>
the server accordingly.  The sample applications are typically registered using Data Flow’s bulk import facility. For example, the Shell command <code class="literal">dataflow:&gt;app import --uri <a class="link" href="http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven" target="_top">bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</a></code> (The actual URI is release and binder specific so refer to the sample instructions for the actual URL).
The bulk import URI references a plain text file containing entries for all of the publicly available Spring Cloud Stream and Task applications published to <code class="literal"><a class="link" href="https://repo.spring.io" target="_top">repo.spring.io</a></code>. For example,
<code class="literal">source.http=maven://org.springframework.cloud.stream.app:http-source-rabbit:1.2.0.RELEASE</code> registers the <code class="literal">http</code> source app at the corresponding Maven address, relative to the remote repository(ies) configured for the
Data Flow server. The format is <code class="literal">maven://&lt;groupId&gt;:&lt;artifactId&gt;:&lt;version&gt;</code>  You will need to <a class="link" href="https://repo.spring.io/libs-release/org/springframework/cloud/stream/app/spring-cloud-stream-app-descriptor/Bacon.RELEASE/spring-cloud-stream-app-descriptor-Bacon.RELEASE.rabbit-apps-maven-repo-url.properties" target="_top">download</a> the required apps or <a class="link" href="https://github.com/spring-cloud-stream-app-starters" target="_top">build</a> them and then install them in your Maven repository, using whatever group, artifact, and version you choose. If you do
this, register individual apps using <code class="literal">dataflow:&gt;app register…​</code> using the <code class="literal">maven://</code> resource URI format corresponding to your installed app.</p></td></tr></table></div><pre class="screen">dataflow:&gt;app import --uri http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</pre></li><li class="listitem" epub:type="list-item"><p class="simpara">Get the PCC connection information</p><pre class="screen">$ cf service-key cloudcache my-service-key
Getting key my-service-key for service instance cloudcache as &lt;user&gt;...

{
 "locators": [
  "10.0.16.9[55221]",
  "10.0.16.11[55221]",
  "10.0.16.10[55221]"
 ],
 "urls": {
  "gfsh": "http://...",
  "pulse": "http://.../pulse"
 },
 "users": [
  {
   "password": &lt;password&gt;,
   "username": "cluster_operator"
  },
  {
   "password": &lt;password&gt;,
   "username": "developer"
  }
 ]
}</pre></li><li class="listitem" epub:type="list-item"><p class="simpara">Using <code class="literal">gfsh</code>, connect to the PCC instance as <code class="literal">cluster_operator</code> using the service key values and create the Test region.</p><pre class="screen">gfsh&gt;connect --use-http --url=&lt;gfsh-url&gt; --user=cluster_operator --password=&lt;cluster_operator_password&gt;
gfsh&gt;create region --name Orders --type=REPLICATE</pre></li><li class="listitem" epub:type="list-item"><p class="simpara">Create the stream using the Data Flow Shell</p><p class="simpara">This example creates an gemfire-cq source to which will publish events matching a query criteria on a region. In this case we will monitor the <code class="literal">Orders</code> region. For simplicity, we will avoid creating a data structure for the order.
Each cache entry contains an integer value representing the quantity of the ordered item. This stream will fire a message whenever the value&gt;999. By default, the source emits only the value. Here we will override that using the
<code class="literal">cq-event-expression</code> property.  This accepts a SpEL expression bound to a <a class="link" href="https://geode.apache.org/releases/latest/javadoc/org/apache/geode/cache/query/CqEvent.html" target="_top">CQEvent</a>. To reference the entire CQEvent instance, we use <code class="literal">#this</code>.
In order to display the contents in the log, we will invoke <code class="literal">toString()</code> on the instance.</p><pre class="screen">dataflow:&gt;stream create --name orders --definition " gemfire-cq  --username=developer --password=&lt;developer-password&gt; --connect-type=locator --host-addresses=10.0.16.9:55221 --query='SELECT * from /Orders o where o &gt; 999' --cq-event-expression=#this.toString()  | log" --deploy
Created and deployed new stream 'events'</pre></li><li class="listitem" epub:type="list-item"><p class="simpara">Verify the stream is successfully deployed</p><pre class="screen">dataflow:&gt;stream list</pre></li><li class="listitem" epub:type="list-item"><p class="simpara">Monitor stdout for the log sink</p><pre class="screen">cf logs &lt;log-sink-app-name&gt;</pre></li><li class="listitem" epub:type="list-item"><p class="simpara">Using <code class="literal">gfsh</code>, create and update some cache entries</p><pre class="screen">gfsh&gt;connect --use-http --url=&lt;gfsh-url&gt; --user=cluster_operator --password=&lt;cluster_operator_password&gt;
gfsh&gt;put --region Orders  --value-class java.lang.Integer --key 01234 --value 1000
gfsh&gt;put --region Orders  --value-class java.lang.Integer --key 11234 --value 1005
gfsh&gt;put --region Orders  --value-class java.lang.Integer --key 21234 --value 100
gfsh&gt;put --region Orders  --value-class java.lang.Integer --key 31234 --value 999
gfsh&gt;put --region Orders  --value-class java.lang.Integer --key 21234 --value 1000</pre></li><li class="listitem" epub:type="list-item"><p class="simpara">Observe the log output
You should see messages like:</p><pre class="screen">2017-10-30 09:53:02.231  INFO 8563 --- [ire-cq.orders-1] log-sink                                 : CqEvent [CqName=GfCq1; base operation=CREATE; cq operation=CREATE; key=01234; value=1000]
2017-10-30 09:53:19.732  INFO 8563 --- [ire-cq.orders-1] log-sink                                 : CqEvent [CqName=GfCq1; base operation=CREATE; cq operation=CREATE; key=11234; value=1005]
2017-10-30 09:53:53.242  INFO 8563 --- [ire-cq.orders-1] log-sink                                 : CqEvent [CqName=GfCq1; base operation=UPDATE; cq operation=CREATE; key=21234; value=1000]</pre></li><li class="listitem" epub:type="list-item">Another interesting demonstration combines <code class="literal">gemfire-cq</code> with the <a class="link" href="../http-gemfire/README.adoc" target="_top">http-gemfire</a> example.</li></ol></div><pre class="screen">dataflow:&gt; stream create --name stocks --definition "http --port=9090 | gemfire-json-server --regionName=Stocks --keyExpression=payload.getField('symbol')" --deploy
dataflow:&gt; stream create --name stock_watch --definition "gemfire-cq --query='Select * from /Stocks where symbol=''VMW''' | log" --deploy</pre><div class="orderedlist" epub:type="list"><ol class="orderedlist" type="1"><li class="listitem" epub:type="list-item">You’re done!</li></ol></div></section></section><section class="section" title="Summary" epub:type="division" id="_summary_5"><div class="titlepage"><div><div><h4 class="title">Summary</h4></div></div></div><p>In this sample, you have learned:</p><div class="itemizedlist" epub:type="list"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem" epub:type="list-item">How to use Spring Cloud Data Flow’s <code class="literal">Local</code> and <code class="literal">Cloud Foundry</code> servers</li><li class="listitem" epub:type="list-item">How to use Spring Cloud Data Flow’s <code class="literal">shell</code></li><li class="listitem" epub:type="list-item">How to create streaming data pipeline to connect and publish CQ events from <code class="literal">gemfire</code></li></ul></div></section></section></section><footer/></body></html>