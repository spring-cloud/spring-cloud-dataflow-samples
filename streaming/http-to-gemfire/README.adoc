:sectnums:
= HTTP to Gemfire Demo

In this demonstration, you will learn how to orchestrate a data pipeline using http://cloud.spring.io/spring-cloud-dataflow/[Spring Cloud Data Flow] to consume data from an `http` endpoint and write to Gemfire using the `gemfire` sink.

We will take you through the steps to configure and run Spring Cloud Data Flow server in either a https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started/[local] or https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current/reference/htmlsingle/#getting-started[Cloud Foundry] environment.

NOTE: For legacy reasons the `gemfire` Spring Cloud Stream Apps are named after `Pivotal GemFire`. The code base for the commercial product has since been open sourced as `Apache Geode`. These samples should work with compatible versions of Pivotal GemFire or Apache Geode. Herein we will refer to the installed IMDG simply as `Geode`.

== Using Local Server

=== Prerequisites

In order to get started, make sure that you have the following components:


* The Spring Cloud Data Flow Local Server. This is a Spring Boot app. Instructions to download the current executable jar are https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started/[here] or you can
https://github.com/spring-cloud/spring-cloud-dataflow[build it yourself].
* The Spring Cloud Data Flow DSL Shell. This is a Spring Boot app. Instructions to download the current executable jar are https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started/[here] or you can
https://github.com/spring-cloud/spring-cloud-dataflow[build it yourself].
* Running instance of https://www.rabbitmq.com[Rabbit MQ]
* A Geode installation with a locator and cache server running.


==== Setup Geode

If you do not have access an existing Geode installation, install http://geode.apache.org[Apache Geode] or
http://geode.apache.org/[Pivotal Gemfire] and start a locator and server using the `gfsh` CLI.
```
    _________________________     __
   / _____/ ______/ ______/ /____/ /
  / /  __/ /___  /_____  / _____  /
 / /__/ / ____/  _____/ / /    / /
/______/_/      /______/_/    /_/    1.2.1

Monitor and Manage Apache Geode
gfsh>start locator --name=locator1
...
gfsh>start server --name=server1

```

* Create a region called `Stocks`

```
gfsh>create region --name Stocks --type=REPLICATE
```

=== Running the Sample Locally

. Launch the local `server`
+
```
$ cd <PATH/TO/SPRING-CLOUD-DATAFLOW-LOCAL-JAR>
$ java -jar spring-cloud-dataflow-server-local-<VERSION>.jar
```

. Start Spring Cloud Data Flow's `shell` in a separate terminal
+
```
$ cd <PATH/TO/SPRING-CLOUD-DATAFLOW-SHELL-JAR>
$ java -jar spring-cloud-dataflow-shell-<VERSION>.jar

  ____                              ____ _                __
 / ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
 \___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
  ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
 |____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
  ____ |_|    _          __|___/                 __________
 |  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
 | | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
 | |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
 |____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/


Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
```

. https://github.com/spring-cloud/spring-cloud-dataflow/blob/master/spring-cloud-dataflow-docs/src/main/asciidoc/streams.adoc#register-a-stream-app[Register] the out-of-the-box applications for the Rabbit binder
+
```
dataflow:>app import --uri http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven
```
. Create the stream
+
```
dataflow:>stream create --name stocks --definition "http --port=9090 | gemfire --json=true --regionName=Stocks --keyExpression=payload.getField('symbol')" --deploy
Created and deployed new stream 'stocks'
```
NOTE: If the Geode locator isn't running on default port on `localhost`, add the options --pool.connect-type=locator --pool.host-addresses=<host>:<port>. If there are multiple
locators, you can provide a comma separated list of locator addresses. This is not necessary for the sample but is typical for production environments to enable fail-over.

. Verify the stream is successfully deployed
+
```
dataflow:>stream list
```

. Post sample data pointing to the `http` endpoint: `http://localhost:9090` [`9090` is the `server.port` we specified for the `http` source in this case]
+
```
dataflow:>http post --target http://localhost:9090 --contentType application/json --data '{"symbol":"VMW","price":117.06}'
> POST (application/json) http://localhost:9090 {"symbol":"VMW","price":117.06}
> 202 ACCEPTED
```
+
. Using `gfsh`, connect to the locator if not already connected, and verify the cache entry was created.
+
```
gfsh>get --key='VMW' --region=/Stocks
Result      : true
Key Class   : java.lang.String
Key         : VMW
Value Class : org.apache.geode.pdx.internal.PdxInstanceImpl

symbol | price
------ | ------
VMW    | 117.06
```
+
. You're done!

NOTE: This example sets the property `--json=true` to enable Geode's JSON support. This converts JSON String payloads to https://geode.apache.org/releases/latest/javadoc/org/apache/geode/pdx/PdxInstance.html[PdxInstance], enabling efficient serialization and field level mapping without
requiring a custom domain class. Use of custom domain types requires these classes to be in the class path of both the stream apps and the cache server. For this reason, the use of custom payload types is generally discouraged.


== Using the Cloud Foundry Server

=== Prerequisites

In order to get started, make sure that you have the following components:

* The Spring Cloud Data Flow DSL Shell. This is a Spring Boot app. Instructions to download the current executable jar are https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started/[here] or you can
https://github.com/spring-cloud/spring-cloud-dataflow[build it yourself].

NOTE: If you have already have the shell running from the previous example, you can use that here.

* The Spring Cloud Data Flow Cloud Foundry Server. This is a Spring Boot app. Instructions to download the current executable jar are https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current/reference/htmlsingle/#getting-started[here] or you can https://github.com/spring-cloud/spring-cloud-dataflow-server-cloudfoundry[build it yourself]
* Running instance of a `rabbit` service in Cloud Foundry
* Running instance of the https://docs.pivotal.io/p-cloud-cache/1-0/index.html[Pivotal Cloud Cache for PCF] (PCC) service `cloudcache` in Cloud Foundry.
* A local Geode installation with `gfsh`.

NOTE: We will generally follow the normal https://docs.pivotal.io/p-cloud-cache/1-0/developer.html#create[instructions] for creating a PCC service instance and connecting to PCC from `gfsh`.


==== Setup Geode

If you do not have access an existing Geode installation, install http://geode.apache.org[Apache Geode] or
http://geode.apache.org/[Pivotal Gemfire] and start the `gfsh` CLI.
```
    _________________________     __
   / _____/ ______/ ______/ /____/ /
  / /  __/ /___  /_____  / _____  /
 / /__/ / ____/  _____/ / /    / /
/______/_/      /______/_/    /_/    1.2.1

Monitor and Manage Apache Geode
```

=== Running the Sample in Cloud Foundry

. Verify that CF instance is reachable
+

```
$ cf api
API endpoint: https://api.system.io (API version: 2.43.0)

$ cf apps
Getting apps in org user-dataflow / space development as user...
OK

No apps found
```
. Follow the instructions to deploy the https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current-SNAPSHOT/reference/htmlsingle[Spring Cloud Data Flow Cloud Foundry server]. Don't worry about creating a Redis service. We won't need it. If you are familiar with Cloud Foundry
application manifests, we recommend creating a manifest for the the Data Flow server as shown https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current-SNAPSHOT/reference/htmlsingle/#sample-manifest-template[here].
+
WARNING: As of this writing, there is a typo on the `SPRING_APPLICATION_JSON` entry in the sample manifest. `SPRING_APPLICATION_JSON` must be followed by `:` and The JSON string must be
wrapped in single quotes.

. Once you have executed successfully `cf push`, verify the dataflow server is running
+

```
$ cf apps
Getting apps in org user-dataflow / space development as user...
OK

name                 requested state   instances   memory   disk   urls
dataflow-server      started           1/1         1G       1G     dataflow-server.app.io
```

+
. Notice that `dataflow-server` application is started and ready for interaction via `http://dataflow-server.app.io` endpoint

. Connect to the server from the Spring Cloud Data Flow Shell
+

```
$ cd <PATH/TO/SPRING-CLOUD-DATAFLOW-SHELL-JAR>
$ java -jar spring-cloud-dataflow-shell-<VERSION>.jar

  ____                              ____ _                __
 / ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
 \___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
  ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
 |____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
  ____ |_|    _          __|___/                 __________
 |  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
 | | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
 | |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
 |____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/


Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
server-unknown:>
```
+
. Connect the `shell` with `server` running on Cloud Foundry, e.g., `http://dataflow-server.app.io`
+

```
server-unknown:>dataflow config server http://dataflow-server.app.io
Successfully targeted http://dataflow-server.app.io
dataflow:>
```

. https://github.com/spring-cloud/spring-cloud-dataflow/blob/master/spring-cloud-dataflow-docs/src/main/asciidoc/streams.adoc#register-a-stream-app[Register] the out-of-the-box applications for the Rabbit binder
+
```
dataflow:>app import --uri http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven
```
. Get the PCC connection information
+
```
$ cf service-key cloudcache my-service-key
Getting key my-service-key for service instance cloudcache as <user>...

{
 "locators": [
  "10.0.16.9[55221]",
  "10.0.16.11[55221]",
  "10.0.16.10[55221]"
 ],
 "urls": {
  "gfsh": "http://...",
  "pulse": "http://.../pulse"
 },
 "users": [
  {
   "password": <password>,
   "username": "cluster_operator"
  },
  {
   "password": <password>,
   "username": "developer"
  }
 ]
}
```
Currently we need the username, password, and one of the locator addresses
. Create the stream, connecting to the PCC instance as developer
+
```
dataflow:>stream create --name stocks --definition "http --security.basic.enabled=false | gemfire --username=developer --password=<developer-password> --connect-type=locator --host-addresses=10.0.16.9:55221 --regionName=Stocks --keyExpression=payload.getField('symbol')" --deploy
```
. Verify the stream is successfully deployed
+
```
dataflow:>stream list
```

. Post sample data pointing to the `http` endpoint: `http://localhost:9090` [`9090` is the `server.port` we specified for the `http` source in this case]
+

Get the url of the http source using `cf apps`

```
dataflow:>http post --target http://<http source url> --contentType application/json --data '{"symbol":"VMW","price":117.06}'
> POST (application/json) http://... {"symbol":"VMW","price":117.06}
> 202 ACCEPTED
```
+
. Using `gfsh`, connect to the PCC instance as `cluster_operator` using the service key values.
```
gfsh>connect --use-http --url=<gfsh-url> --user=cluster_operator --password=<cluster_operator_password>
gfsh>get --key='VMW' --region=/Stocks
Result      : true
Key Class   : java.lang.String
Key         : VMW
Value Class : org.apache.geode.pdx.internal.PdxInstanceImpl

symbol | price
------ | ------
VMW    | 117.06
```
+
. You're done!

NOTE: This example sets the property `--json=true` to enable Geode's JSON support. This converts JSON String payloads to https://geode.apache.org/releases/latest/javadoc/org/apache/geode/pdx/PdxInstance.html[PdxInstance], enabling efficient serialization and field level mapping without
requiring a custom domain class. Use of custom domain types requires these classes to be in the class path of both the stream apps and the cache server. For this reason, the use of custom payload types is generally discouraged.

:!sectnums:
== Summary

In this sample, you have learned:

* How to use Spring Cloud Data Flow's `Local` and `Cloud Foundry` servers
* How to use Spring Cloud Data Flow's `shell`
* How to create streaming data pipeline to connect and write to `gemfire`
