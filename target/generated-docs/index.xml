<?xml version="1.0" encoding="UTF-8"?>
<?asciidoc-toc?>
<?asciidoc-numbered?>
<book xmlns="http://docbook.org/ns/docbook" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:lang="en">
<info>
<title>Spring Cloud Data Flow Samples</title>
<date>2017-10-30</date>
<productname>Spring Cloud Data Flow Samples</productname>
<releaseinfo>1.2.0.BUILD-SNAPSHOT</releaseinfo>
<copyright>
	<year>2013-2017</year>
	<holder>Pivotal Software, Inc.</holder>
</copyright>
<legalnotice>
	<para>
		Copies of this document may be made for your own use and for distribution to
		others, provided that you do not charge any fee for such copies and further
		provided that each copy contains this Copyright Notice, whether distributed in
		print or electronically.
	</para>
</legalnotice>
<cover>
	<mediaobject>
		<imageobject>
			<imagedata fileref="images/cover.png">
				<info>
					<othercredit>
						<orgname>Pivotal Software, Inc.</orgname>
					</othercredit>
				</info>
			</imagedata>
		</imageobject>
	</mediaobject>
</cover>
</info>
<preface>
<title></title>
<simpara xml:id="overview">This project contains samples and demonstrations for Spring Cloud Data Flow.</simpara>
<simpara xml:id="spring-cloud-data-flow-samples-overview">This repository provides sample starter applications and code for use with the Spring Cloud Data Flow project. The following samples are available:</simpara>
</preface>
<chapter xml:id="_streaming">
<title>Streaming</title>
<section xml:id="spring-cloud-data-flow-samples-cassandra-overview">
<title>Cassandra Samples</title>
<section xml:id="spring-cloud-data-flow-samples-http-cassandra-overview">
<title>HTTP to Cassandra Demo</title>
<simpara>In this demonstration, you will learn how to build a data pipeline using <link xlink:href="http://cloud.spring.io/spring-cloud-dataflow/">Spring Cloud Data Flow</link> to consume data from an <emphasis>HTTP</emphasis> endpoint and write the payload to a <emphasis>Cassandra</emphasis> database.</simpara>
<simpara>We will take you through the steps to configure and Spring Cloud Data Flow server in either a <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started/">local</link> or <link xlink:href="https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current/reference/htmlsingle/#getting-started">Cloud Foundry</link> environment.</simpara>
<section xml:id="_prerequisites">
<title>Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>A Running Data Flow Shell</simpara>
</listitem>
</itemizedlist>
<simpara>The Spring Cloud Data Flow Shell is available for <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started-deploying-spring-cloud-dataflow">download</link> or you can <link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow">build</link> it yourself.</simpara>
<note>
<simpara>The Spring Cloud Data Flow Shell is a Spring Boot application that connects to the Data Flow Server’s REST API and supports a DSL that simplifies the process of defining a stream or task and managing its lifecycle. Most of these samples
use the shell. If you prefer, you can use the Data Flow UI <link xlink:href="http://localhost:9393/dashboard">localhost:9393/dashboard</link>, (or wherever it the server is hosted) to perform the same operations.</simpara>
</note>
<note>
<simpara>the Spring Cloud Data Flow Shell and Local server implementation are in the same repository and are both built by running <literal>./mvnw install</literal> from the project root directory. If you have already run the build, use the jar in <literal>spring-cloud-dataflow-shell/target</literal></simpara>
</note>
<simpara>To run the Shell open a new terminal session:</simpara>
<screen>$ cd &lt;PATH/TO/SPRING-CLOUD-DATAFLOW-SHELL-JAR&gt;
$ java -jar spring-cloud-dataflow-shell-&lt;VERSION&gt;.jar
  ____                              ____ _                __
 / ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
 \___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
  ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
 |____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
  ____ |_|    _          __|___/                 __________
 |  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
 | | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
 | |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
 |____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/


Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
dataflow:&gt;</screen>
<note>
<simpara>The shell will try to connect to a local server by default. If the Local Dataflow Server is not running you will see:</simpara>
</note>
<screen> ____                              ____ _                __
/ ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
\___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
 ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
|____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
 ____ |_|    _          __|___/                 __________
|  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
| | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
| |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
|____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/


Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
server unknown:&gt;</screen>
<simpara>Connect the <literal>shell</literal> to the <literal>server</literal> running on , e.g., <literal><link xlink:href="http://dataflow-server.app.io">dataflow-server.app.io</link></literal></simpara>
<screen>server unknown:&gt;dataflow config server http://dataflow-server.app.io
Successfully targeted http://dataflow-server.app.io
dataflow:&gt;</screen>
</section>
<section xml:id="http-cassandra-local">
<title>Using the Local Server</title>
<section xml:id="_additional_prerequisites">
<title>Additional Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>A running local Data Flow Server</simpara>
</listitem>
</itemizedlist>
<simpara>The Local Data Flow Server is Spring Boot application available for <link xlink:href="http://cloud.spring.io/spring-cloud-dataflow/">download</link> or you can <link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow">build</link> it yourself.
If you build it yourself, the executable jar will be in <literal>spring-cloud-dataflow-server-local/target</literal></simpara>
<simpara>To run the Local Data Flow server Open a new terminal session:</simpara>
<screen>$cd  &lt;PATH/TO/SPRING-CLOUD-DATAFLOW-LOCAL-JAR&gt;
$java -jar spring-cloud-dataflow-server-local-&lt;VERSION&gt;.jar</screen>
<itemizedlist>
<listitem>
<simpara>Running instance of <link xlink:href="http://kafka.apache.org/downloads.html">Kafka</link></simpara>
</listitem>
<listitem>
<simpara>Running instance of <link xlink:href="http://cassandra.apache.org/">Apache Cassandra</link></simpara>
</listitem>
<listitem>
<simpara>A database utility tool such as <link xlink:href="http://dbeaver.jkiss.org/">DBeaver</link> to connect to the Cassandra instance. You might have to provide <literal>host</literal>, <literal>port</literal>, <literal>username</literal> and <literal>password</literal> depending on the Cassandra configuration you are using.</simpara>
</listitem>
<listitem>
<simpara>Create a keyspace and a <literal>book</literal> table in Cassandra using:</simpara>
<screen>CREATE KEYSPACE clouddata WITH REPLICATION = { 'class' : 'org.apache.cassandra.locator.SimpleStrategy', 'replication_factor': '1' } AND DURABLE_WRITES = true;
USE clouddata;
CREATE TABLE book  (
    id          uuid PRIMARY KEY,
    isbn        text,
    author      text,
    title       text
);</screen>
<orderedlist numeration="arabic">
<listitem>
<simpara><link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow/blob/master/spring-cloud-dataflow-docs/src/main/asciidoc/streams.adoc#register-a-stream-app">Register</link> the out-of-the-box applications for the Kafka binder</simpara>
<note>
<simpara>These samples assume that the Data Flow Server can access a remote Maven repository, <literal><link xlink:href="https://repo.spring.io/libs-release">repo.spring.io/libs-release</link></literal> by default. If your Data Flow server is running behind a firewall, or you are using a maven proxy preventing
access to public repositories, you will need to install the sample apps in your internal Maven repository and <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started-maven-configuration">configure</link>
the server accordingly.  The sample applications are typically registered using Data Flow&#8217;s bulk import facility. For example, the Shell command <literal>dataflow:&gt;app import --uri <link xlink:href="http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven">bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</link></literal> (The actual URI is release and binder specific so refer to the sample instructions for the actual URL).
The bulk import URI references a plain text file containing entries for all of the publicly available Spring Cloud Stream and Task applications published to <literal><link xlink:href="https://repo.spring.io">repo.spring.io</link></literal>. For example,
<literal>source.http=maven://org.springframework.cloud.stream.app:http-source-rabbit:1.2.0.RELEASE</literal> registers the <literal>http</literal> source app at the corresponding Maven address, relative to the remote repository(ies) configured for the
Data Flow server. The format is <literal>maven://&lt;groupId&gt;:&lt;artifactId&gt;:&lt;version&gt;</literal>  You will need to <link xlink:href="https://repo.spring.io/libs-release/org/springframework/cloud/stream/app/spring-cloud-stream-app-descriptor/Bacon.RELEASE/spring-cloud-stream-app-descriptor-Bacon.RELEASE.rabbit-apps-maven-repo-url.properties">download</link> the required apps or <link xlink:href="https://github.com/spring-cloud-stream-app-starters">build</link> them and then install them in your Maven repository, using whatever group, artifact, and version you choose. If you do
this, register individual apps using <literal>dataflow:&gt;app register&#8230;&#8203;</literal> using the <literal>maven://</literal> resource URI format corresponding to your installed app.</simpara>
</note>
<screen>dataflow:&gt;app import --uri http://bit.ly/Bacon-RELEASE-stream-applications-kafka-10-maven</screen>
</listitem>
<listitem>
<simpara>Create the stream</simpara>
<screen>dataflow:&gt;stream create cassandrastream --definition "http --server.port=8888 --spring.cloud.stream.bindings.output.contentType='application/json' | cassandra --ingestQuery='insert into book (id, isbn, title, author) values (uuid(), ?, ?, ?)' --keyspace=clouddata" --deploy

Created and deployed new stream 'cassandrastream'</screen>
<note>
<simpara>If Cassandra isn&#8217;t running on default port on <literal>localhost</literal> or if you need username and password to connect, use one of the following options to specify the necessary connection parameters: <literal>--username='&lt;USERNAME&gt;' --password='&lt;PASSWORD&gt;' --port=&lt;PORT&gt; --contact-points=&lt;LIST-OF-HOSTS&gt;</literal></simpara>
</note>
</listitem>
<listitem>
<simpara>Verify the stream is successfully deployed</simpara>
<screen>dataflow:&gt;stream list</screen>
</listitem>
<listitem>
<simpara>Notice that <literal>cassandrastream-http</literal> and <literal>cassandrastream-cassandra</literal> <link xlink:href="https://github.com/spring-cloud-stream-app-starters//">Spring Cloud Stream</link> applications are running as Spring Boot applications within the <literal>server</literal> as a collocated process.</simpara>
<screen>2015-12-15 15:52:31.576  INFO 18337 --- [nio-9393-exec-1] o.s.c.d.a.s.l.OutOfProcessModuleDeployer : deploying module org.springframework.cloud.stream.module:cassandra-sink:jar:exec:1.0.0.BUILD-SNAPSHOT instance 0
   Logs will be in /var/folders/c3/ctx7_rns6x30tq7rb76wzqwr0000gp/T/spring-cloud-data-flow-284240942697761420/cassandrastream.cassandra
2015-12-15 15:52:31.583  INFO 18337 --- [nio-9393-exec-1] o.s.c.d.a.s.l.OutOfProcessModuleDeployer : deploying module org.springframework.cloud.stream.module:http-source:jar:exec:1.0.0.BUILD-SNAPSHOT instance 0
   Logs will be in /var/folders/c3/ctx7_rns6x30tq7rb76wzqwr0000gp/T/spring-cloud-data-flow-284240942697761420/cassandrastream.http</screen>
</listitem>
<listitem>
<simpara>Post sample data pointing to the <literal>http</literal> endpoint: <literal><link xlink:href="http://localhost:8888">localhost:8888</link></literal> (<literal>8888</literal> is the <literal>server.port</literal> we specified for the <literal>http</literal> source in this case)</simpara>
<screen>dataflow:&gt;http post --contentType 'application/json' --data '{"isbn": "1599869772", "title": "The Art of War", "author": "Sun Tzu"}' --target http://localhost:8888
&gt; POST (application/json;charset=UTF-8) http://localhost:8888 {"isbn": "1599869772", "title": "The Art of War", "author": "Sun Tzu"}
&gt; 202 ACCEPTED</screen>
</listitem>
<listitem>
<simpara>Connect to the Cassandra instance and query the table <literal>clouddata.book</literal> to list the persisted records</simpara>
<screen>select * from clouddata.book;</screen>
</listitem>
<listitem>
<simpara>You&#8217;re done!</simpara>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="http-cassandra-cf">
<title>Using Cloud Foundry Server</title>
<section xml:id="_additional_prerequisites_2">
<title>Additional Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>Cloud Foundry instance</simpara>
</listitem>
<listitem>
<simpara>The Spring Cloud Data Flow Cloud Foundry Server</simpara>
</listitem>
</itemizedlist>
<simpara>The Cloud Foundry Data Flow Server is Spring Boot application available for <link xlink:href="http://cloud.spring.io/spring-cloud-dataflow/">download</link> or you can <link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow-server-cloudfoundry">build</link> it yourself.
If you build it yourself, the executable jar will be in <literal>spring-cloud-dataflow-server-cloudfoundry/target</literal></simpara>
<note>
<simpara>Although you can run the Data Flow Cloud Foundry Server locally and configure it to deploy to any Cloud Foundry instance, we will
deploy the server to Cloud Foundry as recommended.</simpara>
</note>
<orderedlist numeration="arabic">
<listitem>
<simpara>Verify that CF instance is reachable (Your endpoint urls will be different from what is shown here).</simpara>
<screen>$ cf api
API endpoint: https://api.system.io (API version: ...)

$ cf apps
Getting apps in org [your-org] / space [your-space] as user...
OK

No apps found</screen>
</listitem>
<listitem>
<simpara>Follow the instructions to deploy the <link xlink:href="https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current/reference/htmlsingle">Spring Cloud Data Flow Cloud Foundry server</link>. Don&#8217;t worry about creating a Redis service. We won&#8217;t need it. If you are familiar with Cloud Foundry
application manifests, we recommend creating a manifest for the the Data Flow server as shown <link xlink:href="https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current/reference/htmlsingle/#sample-manifest-template">here</link>.</simpara>
<warning>
<simpara>As of this writing, there is a typo on the <literal>SPRING_APPLICATION_JSON</literal> entry in the sample manifest. <literal>SPRING_APPLICATION_JSON</literal> must be followed by <literal>:</literal> and The JSON string must be
wrapped in single quotes. Alternatively, you can replace that line with <literal>MAVEN_REMOTE_REPOSITORIES_REPO1_URL: <link xlink:href="https://repo.spring.io/libs-snapshot">repo.spring.io/libs-snapshot</link></literal>.  If your Cloud Foundry installation is behind a firewall, you may need to install the stream apps used in this sample in your internal Maven repository and <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/1.3.0.M2/reference/htmlsingle/#getting-started-maven-configuration">configure</link> the server to access that repository.</simpara>
</warning>
</listitem>
<listitem>
<simpara>Once you have successfully executed <literal>cf push</literal>, verify the dataflow server is running</simpara>
<screen>$ cf apps
Getting apps in org [your-org] / space [your-space] as user...
OK

name                 requested state   instances   memory   disk   urls
dataflow-server      started           1/1         1G       1G     dataflow-server.app.io</screen>
</listitem>
<listitem>
<simpara>Notice that the <literal>dataflow-server</literal> application is started and ready for interaction via the url endpoint</simpara>
</listitem>
<listitem>
<simpara>Connect the <literal>shell</literal> with <literal>server</literal> running on Cloud Foundry, e.g., <literal><link xlink:href="http://dataflow-server.app.io">dataflow-server.app.io</link></literal></simpara>
<screen>$ cd &lt;PATH/TO/SPRING-CLOUD-DATAFLOW-SHELL-JAR&gt;
$ java -jar spring-cloud-dataflow-shell-&lt;VERSION&gt;.jar

  ____                              ____ _                __
 / ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
 \___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
  ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
 |____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
  ____ |_|    _          __|___/                 __________
 |  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
 | | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
 | |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
 |____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/


Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
server-unknown:&gt;</screen>
<screen>server-unknown:&gt;dataflow config server http://dataflow-server.app.io
Successfully targeted http://dataflow-server.app.io
dataflow:&gt;</screen>
<itemizedlist>
<listitem>
<simpara>Running instance of <literal>cassandra</literal> in Cloud Foundry or from another Cloud provider</simpara>
</listitem>
<listitem>
<simpara>A database utility tool such as <link xlink:href="http://dbeaver.jkiss.org/">DBeaver</link> to connect to the Cassandra instance. You might have to provide <literal>host</literal>, <literal>port</literal>, <literal>username</literal> and <literal>password</literal> depending on the Cassandra configuration you are using.</simpara>
</listitem>
<listitem>
<simpara>Create a <literal>book</literal> table in your Cassandra keyspace using:</simpara>
<screen>CREATE TABLE book  (
    id          uuid PRIMARY KEY,
    isbn        text,
    author      text,
    title       text
);</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow/blob/master/spring-cloud-dataflow-docs/src/main/asciidoc/streams.adoc#register-a-stream-app">Register</link> the out-of-the-box applications for the Rabbit binder</simpara>
<note>
<simpara>These samples assume that the Data Flow Server can access a remote Maven repository, <literal><link xlink:href="https://repo.spring.io/libs-release">repo.spring.io/libs-release</link></literal> by default. If your Data Flow server is running behind a firewall, or you are using a maven proxy preventing
access to public repositories, you will need to install the sample apps in your internal Maven repository and <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started-maven-configuration">configure</link>
the server accordingly.  The sample applications are typically registered using Data Flow&#8217;s bulk import facility. For example, the Shell command <literal>dataflow:&gt;app import --uri <link xlink:href="http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven">bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</link></literal> (The actual URI is release and binder specific so refer to the sample instructions for the actual URL).
The bulk import URI references a plain text file containing entries for all of the publicly available Spring Cloud Stream and Task applications published to <literal><link xlink:href="https://repo.spring.io">repo.spring.io</link></literal>. For example,
<literal>source.http=maven://org.springframework.cloud.stream.app:http-source-rabbit:1.2.0.RELEASE</literal> registers the <literal>http</literal> source app at the corresponding Maven address, relative to the remote repository(ies) configured for the
Data Flow server. The format is <literal>maven://&lt;groupId&gt;:&lt;artifactId&gt;:&lt;version&gt;</literal>  You will need to <link xlink:href="https://repo.spring.io/libs-release/org/springframework/cloud/stream/app/spring-cloud-stream-app-descriptor/Bacon.RELEASE/spring-cloud-stream-app-descriptor-Bacon.RELEASE.rabbit-apps-maven-repo-url.properties">download</link> the required apps or <link xlink:href="https://github.com/spring-cloud-stream-app-starters">build</link> them and then install them in your Maven repository, using whatever group, artifact, and version you choose. If you do
this, register individual apps using <literal>dataflow:&gt;app register&#8230;&#8203;</literal> using the <literal>maven://</literal> resource URI format corresponding to your installed app.</simpara>
</note>
<screen>dataflow:&gt;app import --uri http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</screen>
</listitem>
<listitem>
<simpara>Create the stream</simpara>
<screen>dataflow:&gt;stream create cassandrastream --definition "http --spring.cloud.stream.bindings.output.contentType='application/json' | cassandra --ingestQuery='insert into book (id, isbn, title, author) values (uuid(), ?, ?, ?)' --username='&lt;USERNAME&gt;' --password='&lt;PASSWORD&gt;' --port=&lt;PORT&gt; --contact-points=&lt;HOST&gt; --keyspace='&lt;KEYSPACE&gt;'" --deploy

Created and deployed new stream 'cassandrastream'</screen>
</listitem>
<listitem>
<simpara>Verify the stream is successfully deployed</simpara>
<screen>dataflow:&gt;stream list</screen>
</listitem>
<listitem>
<simpara>Notice that <literal>cassandrastream-http</literal> and <literal>cassandrastream-cassandra</literal> <link xlink:href="https://github.com/spring-cloud-stream-app-starters/">Spring Cloud Stream</link> applications are running as <emphasis>cloud-native</emphasis> (microservice) applications in Cloud Foundry</simpara>
<screen>$ cf apps
Getting apps in org [your-org] / space [your-space] as user...
OK

name                        requested state   instances   memory   disk   urls
cassandrastream-cassandra   started           1/1         1G       1G     cassandrastream-cassandra.app.io
cassandrastream-http        started           1/1         1G       1G     cassandrastream-http.app.io
dataflow-server             started           1/1         1G       1G     dataflow-server.app.io</screen>
</listitem>
<listitem>
<simpara>Lookup the <literal>url</literal> for <literal>cassandrastream-http</literal> application from the list above. Post sample data pointing to the <literal>http</literal> endpoint: <literal>&lt;YOUR-cassandrastream-http-APP-URL&gt;</literal></simpara>
<screen>http post --contentType 'application/json' --data '{"isbn": "1599869772", "title": "The Art of War", "author": "Sun Tzu"}' --target http://&lt;YOUR-cassandrastream-http-APP-URL&gt;
&gt; POST (application/json;charset=UTF-8) http://cassandrastream-http.app.io {"isbn": "1599869772", "title": "The Art of War", "author": "Sun Tzu"}
&gt; 202 ACCEPTED</screen>
</listitem>
<listitem>
<simpara>Connect to the Cassandra instance and query the table <literal>book</literal> to list the data inserted</simpara>
<screen>select * from book;</screen>
</listitem>
<listitem>
<simpara>Now, let&#8217;s try to take advantage of Pivotal Cloud Foundry&#8217;s platform capability. Let&#8217;s scale the <literal>cassandrastream-http</literal> application from 1 to 3 instances</simpara>
<screen>$ cf scale cassandrastream-http -i 3
Scaling app cassandrastream-http in org user-dataflow / space development as user...
OK</screen>
</listitem>
<listitem>
<simpara>Verify App instances (3/3) running successfully</simpara>
<screen>$ cf apps
Getting apps in org user-dataflow / space development as user...
OK

name                        requested state   instances   memory   disk   urls
cassandrastream-cassandra   started           1/1         1G       1G     cassandrastream-cassandra.app.io
cassandrastream-http        started           3/3         1G       1G     cassandrastream-http.app.io
dataflow-server             started           1/1         1G       1G     dataflow-server.app.io</screen>
</listitem>
<listitem>
<simpara>You&#8217;re done!</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="_summary">
<title>Summary</title>
<simpara>In this sample, you have learned:</simpara>
<itemizedlist>
<listitem>
<simpara>How to use Spring Cloud Data Flow&#8217;s <literal>Local</literal> and <literal>Cloud Foundry</literal> servers</simpara>
</listitem>
<listitem>
<simpara>How to use Spring Cloud Data Flow&#8217;s <literal>shell</literal></simpara>
</listitem>
<listitem>
<simpara>How to create streaming data pipeline to connect and write to <literal>Cassandra</literal></simpara>
</listitem>
<listitem>
<simpara>How to scale data microservice applications on <literal>Pivotal Cloud Foundry</literal></simpara>
</listitem>
</itemizedlist>
</section>
</section>
</section>
<section xml:id="spring-cloud-data-flow-samples-jdbc-overview">
<title>JDBC Samples</title>
<section xml:id="_http_to_mysql_demo">
<title>HTTP to MySQL Demo</title>
<simpara>In this demonstration, you will learn how to build a data pipeline using <link xlink:href="http://cloud.spring.io/spring-cloud-dataflow/">Spring Cloud Data Flow</link> to consume data from an <literal>http</literal> endpoint and write to MySQL database using <literal>jdbc</literal> sink.</simpara>
<simpara>We will take you through the steps to configure and Spring Cloud Data Flow server in either a <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started/">local</link> or <link xlink:href="https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current/reference/htmlsingle/#getting-started">Cloud Foundry</link> environment.</simpara>
<section xml:id="_prerequisites_2">
<title>Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>A Running Data Flow Shell</simpara>
</listitem>
</itemizedlist>
<simpara>The Spring Cloud Data Flow Shell is available for <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started-deploying-spring-cloud-dataflow">download</link> or you can <link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow">build</link> it yourself.</simpara>
<note>
<simpara>The Spring Cloud Data Flow Shell is a Spring Boot application that connects to the Data Flow Server’s REST API and supports a DSL that simplifies the process of defining a stream or task and managing its lifecycle. Most of these samples
use the shell. If you prefer, you can use the Data Flow UI <link xlink:href="http://localhost:9393/dashboard">localhost:9393/dashboard</link>, (or wherever it the server is hosted) to perform the same operations.</simpara>
</note>
<note>
<simpara>the Spring Cloud Data Flow Shell and Local server implementation are in the same repository and are both built by running <literal>./mvnw install</literal> from the project root directory. If you have already run the build, use the jar in <literal>spring-cloud-dataflow-shell/target</literal></simpara>
</note>
<simpara>To run the Shell open a new terminal session:</simpara>
<screen>$ cd &lt;PATH/TO/SPRING-CLOUD-DATAFLOW-SHELL-JAR&gt;
$ java -jar spring-cloud-dataflow-shell-&lt;VERSION&gt;.jar
  ____                              ____ _                __
 / ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
 \___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
  ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
 |____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
  ____ |_|    _          __|___/                 __________
 |  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
 | | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
 | |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
 |____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/


Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
dataflow:&gt;</screen>
<note>
<simpara>The shell will try to connect to a local server by default. If the Local Dataflow Server is not running you will see:</simpara>
</note>
<screen> ____                              ____ _                __
/ ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
\___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
 ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
|____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
 ____ |_|    _          __|___/                 __________
|  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
| | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
| |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
|____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/


Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
server unknown:&gt;</screen>
<simpara>Connect the <literal>shell</literal> to the <literal>server</literal> running on , e.g., <literal><link xlink:href="http://dataflow-server.app.io">dataflow-server.app.io</link></literal></simpara>
<screen>server unknown:&gt;dataflow config server http://dataflow-server.app.io
Successfully targeted http://dataflow-server.app.io
dataflow:&gt;</screen>
</section>
<section xml:id="_using_local_server">
<title>Using Local Server</title>
<section xml:id="_additional_prerequisites_3">
<title>Additional Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>A running local Data Flow Server</simpara>
</listitem>
</itemizedlist>
<simpara>The Local Data Flow Server is Spring Boot application available for <link xlink:href="http://cloud.spring.io/spring-cloud-dataflow/">download</link> or you can <link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow">build</link> it yourself.
If you build it yourself, the executable jar will be in <literal>spring-cloud-dataflow-server-local/target</literal></simpara>
<simpara>To run the Local Data Flow server Open a new terminal session:</simpara>
<screen>$cd  &lt;PATH/TO/SPRING-CLOUD-DATAFLOW-LOCAL-JAR&gt;
$java -jar spring-cloud-dataflow-server-local-&lt;VERSION&gt;.jar</screen>
<itemizedlist>
<listitem>
<simpara>Running instance of <link xlink:href="http://kafka.apache.org/downloads.html">Kafka</link></simpara>
</listitem>
<listitem>
<simpara>Running instance of <link xlink:href="http://www.mysql.com/">MySQL</link></simpara>
</listitem>
<listitem>
<simpara>A database utility tool such as <link xlink:href="http://dbeaver.jkiss.org/">DBeaver</link> or <link xlink:href="https://www.dbvis.com/">DbVisualizer</link></simpara>
</listitem>
<listitem>
<simpara>Create the <literal>test</literal> database with a <literal>names</literal> table (in MySQL) using:</simpara>
<screen>CREATE DATABASE test;
USE test;
CREATE TABLE names
(
	name varchar(255)
);</screen>
<orderedlist numeration="arabic">
<listitem>
<simpara><link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow/blob/master/spring-cloud-dataflow-docs/src/main/asciidoc/streams.adoc#register-a-stream-app">Register</link> the out-of-the-box applications for the Kafka binder</simpara>
<note>
<simpara>These samples assume that the Data Flow Server can access a remote Maven repository, <literal><link xlink:href="https://repo.spring.io/libs-release">repo.spring.io/libs-release</link></literal> by default. If your Data Flow server is running behind a firewall, or you are using a maven proxy preventing
access to public repositories, you will need to install the sample apps in your internal Maven repository and <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started-maven-configuration">configure</link>
the server accordingly.  The sample applications are typically registered using Data Flow&#8217;s bulk import facility. For example, the Shell command <literal>dataflow:&gt;app import --uri <link xlink:href="http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven">bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</link></literal> (The actual URI is release and binder specific so refer to the sample instructions for the actual URL).
The bulk import URI references a plain text file containing entries for all of the publicly available Spring Cloud Stream and Task applications published to <literal><link xlink:href="https://repo.spring.io">repo.spring.io</link></literal>. For example,
<literal>source.http=maven://org.springframework.cloud.stream.app:http-source-rabbit:1.2.0.RELEASE</literal> registers the <literal>http</literal> source app at the corresponding Maven address, relative to the remote repository(ies) configured for the
Data Flow server. The format is <literal>maven://&lt;groupId&gt;:&lt;artifactId&gt;:&lt;version&gt;</literal>  You will need to <link xlink:href="https://repo.spring.io/libs-release/org/springframework/cloud/stream/app/spring-cloud-stream-app-descriptor/Bacon.RELEASE/spring-cloud-stream-app-descriptor-Bacon.RELEASE.rabbit-apps-maven-repo-url.properties">download</link> the required apps or <link xlink:href="https://github.com/spring-cloud-stream-app-starters">build</link> them and then install them in your Maven repository, using whatever group, artifact, and version you choose. If you do
this, register individual apps using <literal>dataflow:&gt;app register&#8230;&#8203;</literal> using the <literal>maven://</literal> resource URI format corresponding to your installed app.</simpara>
</note>
<screen>dataflow:&gt;app import --uri http://bit.ly/Bacon-RELEASE-stream-applications-kafka-10-maven</screen>
</listitem>
<listitem>
<simpara>Create the stream</simpara>
<screen>dataflow:&gt;stream create --name mysqlstream --definition "http --server.port=8787 | jdbc --tableName=names --columns=name --spring.datasource.driver-class-name=org.mariadb.jdbc.Driver --spring.datasource.url='jdbc:mysql://localhost:3306/test'" --deploy

Created and deployed new stream 'mysqlstream'</screen>
<note>
<simpara>If MySQL isn&#8217;t running on default port on <literal>localhost</literal> or if you need username and password to connect, use one of the following options to specify the necessary connection parameters: <literal>--spring.datasource.url='jdbc:mysql://&lt;HOST&gt;:&lt;PORT&gt;/&lt;NAME&gt;' --spring.datasource.username=&lt;USERNAME&gt; --spring.datasource.password=&lt;PASSWORD&gt;</literal></simpara>
</note>
</listitem>
<listitem>
<simpara>Verify the stream is successfully deployed</simpara>
<screen>dataflow:&gt;stream list</screen>
</listitem>
<listitem>
<simpara>Notice that <literal>mysqlstream-http</literal> and <literal>mysqlstream-jdbc</literal> <link xlink:href="https://github.com/spring-cloud-stream-app-starters//">Spring Cloud Stream</link> applications are running as Spring Boot applications within the Local <literal>server</literal> as collocated processes.</simpara>
<screen>2016-05-03 09:29:55.918  INFO 65162 --- [nio-9393-exec-3] o.s.c.d.spi.local.LocalAppDeployer       : deploying app mysqlstream.jdbc instance 0
   Logs will be in /var/folders/c3/ctx7_rns6x30tq7rb76wzqwr0000gp/T/spring-cloud-dataflow-6850863945840320040/mysqlstream1-1462292995903/mysqlstream.jdbc
2016-05-03 09:29:55.939  INFO 65162 --- [nio-9393-exec-3] o.s.c.d.spi.local.LocalAppDeployer       : deploying app mysqlstream.http instance 0
   Logs will be in /var/folders/c3/ctx7_rns6x30tq7rb76wzqwr0000gp/T/spring-cloud-dataflow-6850863945840320040/mysqlstream-1462292995934/mysqlstream.http</screen>
</listitem>
<listitem>
<simpara>Post sample data pointing to the <literal>http</literal> endpoint: <literal><link xlink:href="http://localhost:8787">localhost:8787</link></literal> [<literal>8787</literal> is the <literal>server.port</literal> we specified for the <literal>http</literal> source in this case]</simpara>
</listitem>
</orderedlist>
<screen>dataflow:&gt;http post --contentType 'application/json' --target http://localhost:8787 --data "{\"name\": \"Foo\"}"
&gt; POST (application/json;charset=UTF-8) http://localhost:8787 {"name": "Spring Boot"}
&gt; 202 ACCEPTED</screen>
<simpara>+</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Connect to the MySQL instance and query the table <literal>test.names</literal> to list the new rows:</simpara>
<screen>select * from test.names;</screen>
</listitem>
<listitem>
<simpara>You&#8217;re done!</simpara>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="_using_cloud_foundry_server">
<title>Using Cloud Foundry Server</title>
<section xml:id="_additional_prerequisites_4">
<title>Additional Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>Cloud Foundry instance</simpara>
</listitem>
<listitem>
<simpara>The Spring Cloud Data Flow Cloud Foundry Server</simpara>
</listitem>
</itemizedlist>
<simpara>The Cloud Foundry Data Flow Server is Spring Boot application available for <link xlink:href="http://cloud.spring.io/spring-cloud-dataflow/">download</link> or you can <link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow-server-cloudfoundry">build</link> it yourself.
If you build it yourself, the executable jar will be in <literal>spring-cloud-dataflow-server-cloudfoundry/target</literal></simpara>
<note>
<simpara>Although you can run the Data Flow Cloud Foundry Server locally and configure it to deploy to any Cloud Foundry instance, we will
deploy the server to Cloud Foundry as recommended.</simpara>
</note>
<orderedlist numeration="arabic">
<listitem>
<simpara>Verify that CF instance is reachable (Your endpoint urls will be different from what is shown here).</simpara>
<screen>$ cf api
API endpoint: https://api.system.io (API version: ...)

$ cf apps
Getting apps in org [your-org] / space [your-space] as user...
OK

No apps found</screen>
</listitem>
<listitem>
<simpara>Follow the instructions to deploy the <link xlink:href="https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current/reference/htmlsingle">Spring Cloud Data Flow Cloud Foundry server</link>. Don&#8217;t worry about creating a Redis service. We won&#8217;t need it. If you are familiar with Cloud Foundry
application manifests, we recommend creating a manifest for the the Data Flow server as shown <link xlink:href="https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current/reference/htmlsingle/#sample-manifest-template">here</link>.</simpara>
<warning>
<simpara>As of this writing, there is a typo on the <literal>SPRING_APPLICATION_JSON</literal> entry in the sample manifest. <literal>SPRING_APPLICATION_JSON</literal> must be followed by <literal>:</literal> and The JSON string must be
wrapped in single quotes. Alternatively, you can replace that line with <literal>MAVEN_REMOTE_REPOSITORIES_REPO1_URL: <link xlink:href="https://repo.spring.io/libs-snapshot">repo.spring.io/libs-snapshot</link></literal>.  If your Cloud Foundry installation is behind a firewall, you may need to install the stream apps used in this sample in your internal Maven repository and <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/1.3.0.M2/reference/htmlsingle/#getting-started-maven-configuration">configure</link> the server to access that repository.</simpara>
</warning>
</listitem>
<listitem>
<simpara>Once you have successfully executed <literal>cf push</literal>, verify the dataflow server is running</simpara>
<screen>$ cf apps
Getting apps in org [your-org] / space [your-space] as user...
OK

name                 requested state   instances   memory   disk   urls
dataflow-server      started           1/1         1G       1G     dataflow-server.app.io</screen>
</listitem>
<listitem>
<simpara>Notice that the <literal>dataflow-server</literal> application is started and ready for interaction via the url endpoint</simpara>
</listitem>
<listitem>
<simpara>Connect the <literal>shell</literal> with <literal>server</literal> running on Cloud Foundry, e.g., <literal><link xlink:href="http://dataflow-server.app.io">dataflow-server.app.io</link></literal></simpara>
<screen>$ cd &lt;PATH/TO/SPRING-CLOUD-DATAFLOW-SHELL-JAR&gt;
$ java -jar spring-cloud-dataflow-shell-&lt;VERSION&gt;.jar

  ____                              ____ _                __
 / ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
 \___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
  ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
 |____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
  ____ |_|    _          __|___/                 __________
 |  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
 | | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
 | |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
 |____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/


Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
server-unknown:&gt;</screen>
<screen>server-unknown:&gt;dataflow config server http://dataflow-server.app.io
Successfully targeted http://dataflow-server.app.io
dataflow:&gt;</screen>
<itemizedlist>
<listitem>
<simpara>Running instance of <literal>rabbit</literal> in Cloud Foundry</simpara>
</listitem>
<listitem>
<simpara>Running instance of <literal>mysql</literal> in Cloud Foundry</simpara>
</listitem>
<listitem>
<simpara>A database utility tool such as <link xlink:href="http://dbeaver.jkiss.org/">DBeaver</link> or <link xlink:href="https://www.dbvis.com/">DbVisualizer</link></simpara>
</listitem>
<listitem>
<simpara>Create the <literal>names</literal> table (in MySQL) using:</simpara>
<screen>CREATE TABLE names
(
	name varchar(255)
);</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow/blob/master/spring-cloud-dataflow-docs/src/main/asciidoc/streams.adoc#register-a-stream-app">Register</link> the out-of-the-box applications for the Rabbit binder</simpara>
<note>
<simpara>These samples assume that the Data Flow Server can access a remote Maven repository, <literal><link xlink:href="https://repo.spring.io/libs-release">repo.spring.io/libs-release</link></literal> by default. If your Data Flow server is running behind a firewall, or you are using a maven proxy preventing
access to public repositories, you will need to install the sample apps in your internal Maven repository and <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started-maven-configuration">configure</link>
the server accordingly.  The sample applications are typically registered using Data Flow&#8217;s bulk import facility. For example, the Shell command <literal>dataflow:&gt;app import --uri <link xlink:href="http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven">bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</link></literal> (The actual URI is release and binder specific so refer to the sample instructions for the actual URL).
The bulk import URI references a plain text file containing entries for all of the publicly available Spring Cloud Stream and Task applications published to <literal><link xlink:href="https://repo.spring.io">repo.spring.io</link></literal>. For example,
<literal>source.http=maven://org.springframework.cloud.stream.app:http-source-rabbit:1.2.0.RELEASE</literal> registers the <literal>http</literal> source app at the corresponding Maven address, relative to the remote repository(ies) configured for the
Data Flow server. The format is <literal>maven://&lt;groupId&gt;:&lt;artifactId&gt;:&lt;version&gt;</literal>  You will need to <link xlink:href="https://repo.spring.io/libs-release/org/springframework/cloud/stream/app/spring-cloud-stream-app-descriptor/Bacon.RELEASE/spring-cloud-stream-app-descriptor-Bacon.RELEASE.rabbit-apps-maven-repo-url.properties">download</link> the required apps or <link xlink:href="https://github.com/spring-cloud-stream-app-starters">build</link> them and then install them in your Maven repository, using whatever group, artifact, and version you choose. If you do
this, register individual apps using <literal>dataflow:&gt;app register&#8230;&#8203;</literal> using the <literal>maven://</literal> resource URI format corresponding to your installed app.</simpara>
</note>
<screen>dataflow:&gt;app import --uri http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</screen>
</listitem>
<listitem>
<simpara>Create the stream</simpara>
<screen>dataflow:&gt;stream create --name mysqlstream --definition "http | jdbc --tableName=names --columns=name"
Created new stream 'mysqlstream'

dataflow:&gt;stream deploy --name mysqlstream --properties "app.jdbc.spring.cloud.deployer.cloudfoundry.services=mysql"
Deployed stream 'mysqlstream'</screen>
<note>
<simpara>By supplying  the <literal>app.jdbc.spring.cloud.deployer.cloudfoundry.services=mysql</literal> property, we are deploying the stream with <literal>jdbc-sink</literal> to automatically bind to <literal>mysql</literal> service and only this application in the stream gets the service binding. This also eliminates the requirement to supply <literal>datasource</literal> credentials in stream definition.</simpara>
</note>
</listitem>
<listitem>
<simpara>Verify the stream is successfully deployed</simpara>
<screen>dataflow:&gt;stream list</screen>
</listitem>
<listitem>
<simpara>Notice that <literal>mysqlstream-http</literal> and <literal>mysqlstream-jdbc</literal> <link xlink:href="https://github.com/spring-cloud-stream-app-starters/">Spring Cloud Stream</link> applications are running as <emphasis>cloud-native</emphasis> (microservice) applications in Cloud Foundry</simpara>
<screen>$ cf apps
Getting apps in org user-dataflow / space development as user...
OK

name                        requested state   instances   memory   disk   urls
mysqlstream-http            started           1/1         1G       1G     mysqlstream-http.app.io
mysqlstream-jdbc            started           1/1         1G       1G     mysqlstream-jdbc.app.io
dataflow-server             started           1/1         1G       1G     dataflow-server.app.io</screen>
</listitem>
<listitem>
<simpara>Lookup the <literal>url</literal> for <literal>mysqlstream-http</literal> application from the list above. Post sample data pointing to the <literal>http</literal> endpoint: <literal>&lt;YOUR-mysqlstream-http-APP-URL&gt;</literal></simpara>
<screen>http post --contentType 'application/json' --target http://mysqlstream-http.app.io --data "{\"name\": \"Bar"}"
&gt; POST (application/json;charset=UTF-8) http://mysqlstream-http.app.io {"name": "Bar"}
&gt; 202 ACCEPTED</screen>
</listitem>
<listitem>
<simpara>Connect to the MySQL instance and query the table <literal>names</literal> to list the new rows:</simpara>
<screen>select * from names;</screen>
</listitem>
<listitem>
<simpara>Now, let&#8217;s take advantage of Pivotal Cloud Foundry&#8217;s platform capability. Let&#8217;s scale the <literal>mysqlstream-http</literal> application from 1 to 3 instances</simpara>
<screen>$ cf scale mysqlstream-http -i 3
Scaling app mysqlstream-http in org user-dataflow / space development as user...
OK</screen>
</listitem>
<listitem>
<simpara>Verify App instances (3/3) running successfully</simpara>
<screen>$ cf apps
Getting apps in org user-dataflow / space development as user...
OK

name                        requested state   instances   memory   disk   urls
mysqlstream-http            started           3/3         1G       1G     mysqlstream-http.app.io
mysqlstream-jdbc            started           1/1         1G       1G     mysqlstream-jdbc.app.io
dataflow-server             started           1/1         1G       1G     dataflow-server.app.io</screen>
</listitem>
<listitem>
<simpara>You&#8217;re done!</simpara>
</listitem>
</orderedlist>
</section>
</section>
</section>
<section xml:id="_summary_2">
<title>Summary</title>
<simpara>In this sample, you have learned:</simpara>
<itemizedlist>
<listitem>
<simpara>How to use Spring Cloud Data Flow&#8217;s <literal>Local</literal> and <literal>Cloud Foundry</literal> servers</simpara>
</listitem>
<listitem>
<simpara>How to use Spring Cloud Data Flow&#8217;s <literal>shell</literal></simpara>
</listitem>
<listitem>
<simpara>How to create streaming data pipeline to connect and write to <literal>MySQL</literal></simpara>
</listitem>
<listitem>
<simpara>How to scale data microservice applications on <literal>Pivotal Cloud Foundry</literal></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="spring-cloud-data-flow-samples-gemfire-overview">
<title>GemFire Samples</title>
<section xml:id="spring-cloud-data-flow-samples-gemfire-http-overview">
<title>HTTP to Gemfire Demo</title>
<simpara>In this demonstration, you will learn how to build a data pipeline using <link xlink:href="http://cloud.spring.io/spring-cloud-dataflow/">Spring Cloud Data Flow</link> to consume data from an <literal>http</literal> endpoint and write to Gemfire using the <literal>gemfire</literal> sink.</simpara>
<simpara>We will take you through the steps to configure and run Spring Cloud Data Flow server in either a <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started/">local</link> or <link xlink:href="https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current/reference/htmlsingle/#getting-started">Cloud Foundry</link> environment.</simpara>
<note>
<simpara>For legacy reasons the <literal>gemfire</literal> Spring Cloud Stream Apps are named after <literal>Pivotal GemFire</literal>. The code base for the commercial product has since been open sourced as <literal>Apache Geode</literal>. These samples should work with compatible versions of Pivotal GemFire or Apache Geode. Herein we will refer to the installed IMDG simply as <literal>Geode</literal>.</simpara>
</note>
<section xml:id="_prerequisites_3">
<title>Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>A Running Data Flow Shell</simpara>
</listitem>
</itemizedlist>
<simpara>The Spring Cloud Data Flow Shell is available for <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started-deploying-spring-cloud-dataflow">download</link> or you can <link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow">build</link> it yourself.</simpara>
<note>
<simpara>The Spring Cloud Data Flow Shell is a Spring Boot application that connects to the Data Flow Server’s REST API and supports a DSL that simplifies the process of defining a stream or task and managing its lifecycle. Most of these samples
use the shell. If you prefer, you can use the Data Flow UI <link xlink:href="http://localhost:9393/dashboard">localhost:9393/dashboard</link>, (or wherever it the server is hosted) to perform the same operations.</simpara>
</note>
<note>
<simpara>the Spring Cloud Data Flow Shell and Local server implementation are in the same repository and are both built by running <literal>./mvnw install</literal> from the project root directory. If you have already run the build, use the jar in <literal>spring-cloud-dataflow-shell/target</literal></simpara>
</note>
<simpara>To run the Shell open a new terminal session:</simpara>
<screen>$ cd &lt;PATH/TO/SPRING-CLOUD-DATAFLOW-SHELL-JAR&gt;
$ java -jar spring-cloud-dataflow-shell-&lt;VERSION&gt;.jar
  ____                              ____ _                __
 / ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
 \___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
  ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
 |____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
  ____ |_|    _          __|___/                 __________
 |  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
 | | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
 | |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
 |____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/


Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
dataflow:&gt;</screen>
<note>
<simpara>The shell will try to connect to a local server by default. If the Local Dataflow Server is not running you will see:</simpara>
</note>
<screen> ____                              ____ _                __
/ ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
\___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
 ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
|____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
 ____ |_|    _          __|___/                 __________
|  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
| | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
| |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
|____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/


Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
server unknown:&gt;</screen>
<simpara>Connect the <literal>shell</literal> to the <literal>server</literal> running on , e.g., <literal><link xlink:href="http://dataflow-server.app.io">dataflow-server.app.io</link></literal></simpara>
<screen>server unknown:&gt;dataflow config server http://dataflow-server.app.io
Successfully targeted http://dataflow-server.app.io
dataflow:&gt;</screen>
<itemizedlist>
<listitem>
<simpara>A Geode installation with a locator and cache server running
Unresolved directive in streaming/gemfire/gemfire-http/overview.adoc - include::geode-setup.adoc[]</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="gemfire-http-local">
<title>Using the Local Server</title>
<section xml:id="_additional_prerequisites_5">
<title>Additional Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>A running local Data Flow Server</simpara>
</listitem>
</itemizedlist>
<simpara>The Local Data Flow Server is Spring Boot application available for <link xlink:href="http://cloud.spring.io/spring-cloud-dataflow/">download</link> or you can <link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow">build</link> it yourself.
If you build it yourself, the executable jar will be in <literal>spring-cloud-dataflow-server-local/target</literal></simpara>
<simpara>To run the Local Data Flow server Open a new terminal session:</simpara>
<screen>$cd  &lt;PATH/TO/SPRING-CLOUD-DATAFLOW-LOCAL-JAR&gt;
$java -jar spring-cloud-dataflow-server-local-&lt;VERSION&gt;.jar</screen>
<itemizedlist>
<listitem>
<simpara>A running instance of <link xlink:href="https://www.rabbitmq.com">Rabbit MQ</link></simpara>
</listitem>
</itemizedlist>
<simpara><?asciidoc-hr?></simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Use gfsh to start a locator and server</simpara>
<screen>gfsh&gt;start locator --name=locator1
gfsh&gt;start server --name=server1</screen>
</listitem>
<listitem>
<simpara>Create a region called <literal>Stocks</literal></simpara>
<screen>gfsh&gt;create region --name Stocks --type=REPLICATE</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="_use_the_shell_to_create_the_sample_stream">
<title>Use the Shell to create the sample stream</title>
<orderedlist numeration="arabic">
<listitem>
<simpara><link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow/blob/master/spring-cloud-dataflow-docs/src/main/asciidoc/streams.adoc#register-a-stream-app">Register</link> the out-of-the-box applications for the Rabbit binder</simpara>
<note>
<simpara>These samples assume that the Data Flow Server can access a remote Maven repository, <literal><link xlink:href="https://repo.spring.io/libs-release">repo.spring.io/libs-release</link></literal> by default. If your Data Flow server is running behind a firewall, or you are using a maven proxy preventing
access to public repositories, you will need to install the sample apps in your internal Maven repository and <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started-maven-configuration">configure</link>
the server accordingly.  The sample applications are typically registered using Data Flow&#8217;s bulk import facility. For example, the Shell command <literal>dataflow:&gt;app import --uri <link xlink:href="http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven">bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</link></literal> (The actual URI is release and binder specific so refer to the sample instructions for the actual URL).
The bulk import URI references a plain text file containing entries for all of the publicly available Spring Cloud Stream and Task applications published to <literal><link xlink:href="https://repo.spring.io">repo.spring.io</link></literal>. For example,
<literal>source.http=maven://org.springframework.cloud.stream.app:http-source-rabbit:1.2.0.RELEASE</literal> registers the <literal>http</literal> source app at the corresponding Maven address, relative to the remote repository(ies) configured for the
Data Flow server. The format is <literal>maven://&lt;groupId&gt;:&lt;artifactId&gt;:&lt;version&gt;</literal>  You will need to <link xlink:href="https://repo.spring.io/libs-release/org/springframework/cloud/stream/app/spring-cloud-stream-app-descriptor/Bacon.RELEASE/spring-cloud-stream-app-descriptor-Bacon.RELEASE.rabbit-apps-maven-repo-url.properties">download</link> the required apps or <link xlink:href="https://github.com/spring-cloud-stream-app-starters">build</link> them and then install them in your Maven repository, using whatever group, artifact, and version you choose. If you do
this, register individual apps using <literal>dataflow:&gt;app register&#8230;&#8203;</literal> using the <literal>maven://</literal> resource URI format corresponding to your installed app.</simpara>
</note>
<screen>dataflow:&gt;app import --uri http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</screen>
</listitem>
<listitem>
<simpara>Create the stream</simpara>
<simpara>This example creates an http endpoint to which we will post stock prices as a JSON document containing <literal>symbol</literal> and <literal>price</literal> fields.
The property <literal>--json=true</literal> to enable Geode&#8217;s JSON support and configures the sink to convert JSON String payloads to <link xlink:href="https://geode.apache.org/releases/latest/javadoc/org/apache/geode/pdx/PdxInstance.html">PdxInstance</link>, the recommended way
to store JSON documents in Geode. The <literal>keyExpression</literal> property is a SpEL expression used to extract the <literal>symbol</literal> value the PdxInstance to use as an entry key.</simpara>
<note>
<simpara>PDX serialization is very efficient and supports OQL queries without requiring a custom domain class.
Use of custom domain types requires these classes to be in the class path of both the stream apps and the cache server.
For this reason, the use of custom payload types is generally discouraged.</simpara>
</note>
<screen>dataflow:&gt;stream create --name stocks --definition "http --port=9090 | gemfire --json=true --regionName=Stocks --keyExpression=payload.getField('symbol')" --deploy
Created and deployed new stream 'stocks'</screen>
<note>
<simpara>If the Geode locator isn&#8217;t running on default port on <literal>localhost</literal>, add the options <literal>--connect-type=locator --host-addresses=&lt;host&gt;:&lt;port&gt;</literal>. If there are multiple
locators, you can provide a comma separated list of locator addresses. This is not necessary for the sample but is typical for production environments to enable fail-over.</simpara>
</note>
</listitem>
<listitem>
<simpara>Verify the stream is successfully deployed</simpara>
<screen>dataflow:&gt;stream list</screen>
</listitem>
<listitem>
<simpara>Post sample data pointing to the <literal>http</literal> endpoint: <literal><link xlink:href="http://localhost:9090">localhost:9090</link></literal> (<literal>9090</literal> is the <literal>port</literal> we specified for the <literal>http</literal> source)</simpara>
<screen>dataflow:&gt;http post --target http://localhost:9090 --contentType application/json --data '{"symbol":"VMW","price":117.06}'
&gt; POST (application/json) http://localhost:9090 {"symbol":"VMW","price":117.06}
&gt; 202 ACCEPTED</screen>
</listitem>
<listitem>
<simpara>Using <literal>gfsh</literal>, connect to the locator if not already connected, and verify the cache entry was created.</simpara>
<screen>gfsh&gt;get --key='VMW' --region=/Stocks
Result      : true
Key Class   : java.lang.String
Key         : VMW
Value Class : org.apache.geode.pdx.internal.PdxInstanceImpl

symbol | price
------ | ------
VMW    | 117.06</screen>
</listitem>
<listitem>
<simpara>You&#8217;re done!</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="gemfire-http-cf">
<title>Using the Cloud Foundry Server</title>
<section xml:id="_additional_prerequisites_6">
<title>Additional Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>A Cloud Foundry instance</simpara>
</listitem>
<listitem>
<simpara>Cloud Data Flow Cloud Foundry Server</simpara>
</listitem>
</itemizedlist>
<simpara>The Cloud Foundry Data Flow Server is Spring Boot application available for <link xlink:href="http://cloud.spring.io/spring-cloud-dataflow/">download</link> or you can <link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow-server-cloudfoundry">build</link> it yourself.
If you build it yourself, the executable jar will be in <literal>spring-cloud-dataflow-server-cloudfoundry/target</literal></simpara>
<note>
<simpara>Although you can run the Data Flow Cloud Foundry Server locally and configure it to deploy to any Cloud Foundry instance, we will
deploy the server to Cloud Foundry as recommended.</simpara>
</note>
<orderedlist numeration="arabic">
<listitem>
<simpara>Verify that CF instance is reachable (Your endpoint urls will be different from what is shown here).</simpara>
<screen>$ cf api
API endpoint: https://api.system.io (API version: ...)

$ cf apps
Getting apps in org [your-org] / space [your-space] as user...
OK

No apps found</screen>
</listitem>
<listitem>
<simpara>Follow the instructions to deploy the <link xlink:href="https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current/reference/htmlsingle">Spring Cloud Data Flow Cloud Foundry server</link>. Don&#8217;t worry about creating a Redis service. We won&#8217;t need it. If you are familiar with Cloud Foundry
application manifests, we recommend creating a manifest for the the Data Flow server as shown <link xlink:href="https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current/reference/htmlsingle/#sample-manifest-template">here</link>.</simpara>
<warning>
<simpara>As of this writing, there is a typo on the <literal>SPRING_APPLICATION_JSON</literal> entry in the sample manifest. <literal>SPRING_APPLICATION_JSON</literal> must be followed by <literal>:</literal> and The JSON string must be
wrapped in single quotes. Alternatively, you can replace that line with <literal>MAVEN_REMOTE_REPOSITORIES_REPO1_URL: <link xlink:href="https://repo.spring.io/libs-snapshot">repo.spring.io/libs-snapshot</link></literal>.  If your Cloud Foundry installation is behind a firewall, you may need to install the stream apps used in this sample in your internal Maven repository and <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/1.3.0.M2/reference/htmlsingle/#getting-started-maven-configuration">configure</link> the server to access that repository.</simpara>
</warning>
</listitem>
<listitem>
<simpara>Once you have successfully executed <literal>cf push</literal>, verify the dataflow server is running</simpara>
<screen>$ cf apps
Getting apps in org [your-org] / space [your-space] as user...
OK

name                 requested state   instances   memory   disk   urls
dataflow-server      started           1/1         1G       1G     dataflow-server.app.io</screen>
</listitem>
<listitem>
<simpara>Notice that the <literal>dataflow-server</literal> application is started and ready for interaction via the url endpoint</simpara>
</listitem>
<listitem>
<simpara>Connect the <literal>shell</literal> with <literal>server</literal> running on Cloud Foundry, e.g., <literal><link xlink:href="http://dataflow-server.app.io">dataflow-server.app.io</link></literal></simpara>
<screen>$ cd &lt;PATH/TO/SPRING-CLOUD-DATAFLOW-SHELL-JAR&gt;
$ java -jar spring-cloud-dataflow-shell-&lt;VERSION&gt;.jar

  ____                              ____ _                __
 / ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
 \___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
  ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
 |____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
  ____ |_|    _          __|___/                 __________
 |  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
 | | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
 | |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
 |____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/


Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
server-unknown:&gt;</screen>
<screen>server-unknown:&gt;dataflow config server http://dataflow-server.app.io
Successfully targeted http://dataflow-server.app.io
dataflow:&gt;</screen>
<itemizedlist>
<listitem>
<simpara>Running instance of a <literal>rabbit</literal> service in Cloud Foundry</simpara>
</listitem>
<listitem>
<simpara>Running instance of the <link xlink:href="https://docs.pivotal.io/p-cloud-cache/1-0/developer.html">Pivotal Cloud Cache for PCF</link> (PCC) service <literal>cloudcache</literal> in Cloud Foundry.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow/blob/master/spring-cloud-dataflow-docs/src/main/asciidoc/streams.adoc#register-a-stream-app">Register</link> the out-of-the-box applications for the Rabbit binder</simpara>
<note>
<simpara>These samples assume that the Data Flow Server can access a remote Maven repository, <literal><link xlink:href="https://repo.spring.io/libs-release">repo.spring.io/libs-release</link></literal> by default. If your Data Flow server is running behind a firewall, or you are using a maven proxy preventing
access to public repositories, you will need to install the sample apps in your internal Maven repository and <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started-maven-configuration">configure</link>
the server accordingly.  The sample applications are typically registered using Data Flow&#8217;s bulk import facility. For example, the Shell command <literal>dataflow:&gt;app import --uri <link xlink:href="http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven">bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</link></literal> (The actual URI is release and binder specific so refer to the sample instructions for the actual URL).
The bulk import URI references a plain text file containing entries for all of the publicly available Spring Cloud Stream and Task applications published to <literal><link xlink:href="https://repo.spring.io">repo.spring.io</link></literal>. For example,
<literal>source.http=maven://org.springframework.cloud.stream.app:http-source-rabbit:1.2.0.RELEASE</literal> registers the <literal>http</literal> source app at the corresponding Maven address, relative to the remote repository(ies) configured for the
Data Flow server. The format is <literal>maven://&lt;groupId&gt;:&lt;artifactId&gt;:&lt;version&gt;</literal>  You will need to <link xlink:href="https://repo.spring.io/libs-release/org/springframework/cloud/stream/app/spring-cloud-stream-app-descriptor/Bacon.RELEASE/spring-cloud-stream-app-descriptor-Bacon.RELEASE.rabbit-apps-maven-repo-url.properties">download</link> the required apps or <link xlink:href="https://github.com/spring-cloud-stream-app-starters">build</link> them and then install them in your Maven repository, using whatever group, artifact, and version you choose. If you do
this, register individual apps using <literal>dataflow:&gt;app register&#8230;&#8203;</literal> using the <literal>maven://</literal> resource URI format corresponding to your installed app.</simpara>
</note>
<screen>dataflow:&gt;app import --uri http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</screen>
</listitem>
<listitem>
<simpara>Get the PCC connection information</simpara>
<screen>$ cf service-key cloudcache my-service-key
Getting key my-service-key for service instance cloudcache as &lt;user&gt;...

{
 "locators": [
  "10.0.16.9[55221]",
  "10.0.16.11[55221]",
  "10.0.16.10[55221]"
 ],
 "urls": {
  "gfsh": "http://...",
  "pulse": "http://.../pulse"
 },
 "users": [
  {
   "password": &lt;password&gt;,
   "username": "cluster_operator"
  },
  {
   "password": &lt;password&gt;,
   "username": "developer"
  }
 ]
}</screen>
</listitem>
<listitem>
<simpara>Using <literal>gfsh</literal>, connect to the PCC instance as <literal>cluster_operator</literal> using the service key values and create the Stocks region.</simpara>
<screen>gfsh&gt;connect --use-http --url=&lt;gfsh-url&gt; --user=cluster_operator --password=&lt;cluster_operator_password&gt;
gfsh&gt;create region --name Stocks --type=REPLICATE</screen>
</listitem>
<listitem>
<simpara>Create the stream, connecting to the PCC instance as developer</simpara>
<simpara>This example creates an http endpoint to which we will post stock prices as a JSON document containing <literal>symbol</literal> and <literal>price</literal> fields.
The property <literal>--json=true</literal> to enable Geode&#8217;s JSON support and configures the sink to convert JSON String payloads to <link xlink:href="https://geode.apache.org/releases/latest/javadoc/org/apache/geode/pdx/PdxInstance.html">PdxInstance</link>, the recommended way
to store JSON documents in Geode. The <literal>keyExpression</literal> property is a SpEL expression used to extract the <literal>symbol</literal> value the PdxInstance to use as an entry key.</simpara>
<note>
<simpara>PDX serialization is very efficient and supports OQL queries without requiring a custom domain class.
Use of custom domain types requires these classes to be in the class path of both the stream apps and the cache server.
For this reason, the use of custom payload types is generally discouraged.</simpara>
</note>
<screen>dataflow:&gt;stream create --name stocks --definition "http --security.basic.enabled=false | gemfire --username=developer --password=&lt;developer-password&gt; --connect-type=locator --host-addresses=10.0.16.9:55221 --regionName=Stocks --keyExpression=payload.getField('symbol')" --deploy</screen>
</listitem>
<listitem>
<simpara>Verify the stream is successfully deployed</simpara>
<screen>dataflow:&gt;stream list</screen>
</listitem>
<listitem>
<simpara>Post sample data pointing to the <literal>http</literal> endpoint</simpara>
<simpara>Get the url of the http source using <literal>cf apps</literal></simpara>
<screen>dataflow:&gt;http post --target http://&lt;http source url&gt; --contentType application/json --data '{"symbol":"VMW","price":117.06}'
&gt; POST (application/json) http://... {"symbol":"VMW","price":117.06}
&gt; 202 ACCEPTED</screen>
</listitem>
<listitem>
<simpara>Using <literal>gfsh</literal>, connect to the PCC instance as <literal>cluster_operator</literal> using the service key values.</simpara>
<screen>gfsh&gt;connect --use-http --url=&lt;gfsh-url&gt; --user=cluster_operator --password=&lt;cluster_operator_password&gt;
gfsh&gt;get --key='VMW' --region=/Stocks
Result      : true
Key Class   : java.lang.String
Key         : VMW
Value Class : org.apache.geode.pdx.internal.PdxInstanceImpl

symbol | price
------ | ------
VMW    | 117.06</screen>
</listitem>
<listitem>
<simpara>You&#8217;re done!</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="_summary_3">
<title>Summary</title>
<simpara>In this sample, you have learned:</simpara>
<itemizedlist>
<listitem>
<simpara>How to use Spring Cloud Data Flow&#8217;s <literal>Local</literal> and <literal>Cloud Foundry</literal> servers</simpara>
</listitem>
<listitem>
<simpara>How to use Spring Cloud Data Flow&#8217;s <literal>shell</literal></simpara>
</listitem>
<listitem>
<simpara>How to create streaming data pipeline to connect and write to <literal>gemfire</literal></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="spring-cloud-data-flow-samples-gemfire-log-overview">
<title>Gemfire to Log Demo</title>
<simpara>In this demonstration, you will learn how to build a data pipeline using <link xlink:href="http://cloud.spring.io/spring-cloud-dataflow/">Spring Cloud Data Flow</link> to consume data from a <literal>gemfire</literal> endpoint and write to a log using the <literal>log</literal> sink.
The <literal>gemfire</literal> source creates a <link xlink:href="https://geode.apache.org/releases/latest/javadoc/org/apache/geode/cache/CacheListener.html">CacheListener</link> to monitor events for a region and publish a message whenever an entry is changed.</simpara>
<simpara>We will take you through the steps to configure and run Spring Cloud Data Flow server in either a <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started/">local</link> or <link xlink:href="https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current/reference/htmlsingle/#getting-started">Cloud Foundry</link> environment.</simpara>
<note>
<simpara>For legacy reasons the <literal>gemfire</literal> Spring Cloud Stream Apps are named after <literal>Pivotal GemFire</literal>. The code base for the commercial product has since been open sourced as <literal>Apache Geode</literal>. These samples should work with compatible versions of Pivotal GemFire or Apache Geode. Herein we will refer to the installed IMDG simply as <literal>Geode</literal>.</simpara>
</note>
<section xml:id="_prerequisites_4">
<title>Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>A Running Data Flow Shell</simpara>
</listitem>
</itemizedlist>
<simpara>The Spring Cloud Data Flow Shell is available for <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started-deploying-spring-cloud-dataflow">download</link> or you can <link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow">build</link> it yourself.</simpara>
<note>
<simpara>The Spring Cloud Data Flow Shell is a Spring Boot application that connects to the Data Flow Server’s REST API and supports a DSL that simplifies the process of defining a stream or task and managing its lifecycle. Most of these samples
use the shell. If you prefer, you can use the Data Flow UI <link xlink:href="http://localhost:9393/dashboard">localhost:9393/dashboard</link>, (or wherever it the server is hosted) to perform the same operations.</simpara>
</note>
<note>
<simpara>the Spring Cloud Data Flow Shell and Local server implementation are in the same repository and are both built by running <literal>./mvnw install</literal> from the project root directory. If you have already run the build, use the jar in <literal>spring-cloud-dataflow-shell/target</literal></simpara>
</note>
<simpara>To run the Shell open a new terminal session:</simpara>
<screen>$ cd &lt;PATH/TO/SPRING-CLOUD-DATAFLOW-SHELL-JAR&gt;
$ java -jar spring-cloud-dataflow-shell-&lt;VERSION&gt;.jar
  ____                              ____ _                __
 / ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
 \___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
  ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
 |____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
  ____ |_|    _          __|___/                 __________
 |  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
 | | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
 | |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
 |____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/


Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
dataflow:&gt;</screen>
<note>
<simpara>The shell will try to connect to a local server by default. If the Local Dataflow Server is not running you will see:</simpara>
</note>
<screen> ____                              ____ _                __
/ ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
\___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
 ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
|____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
 ____ |_|    _          __|___/                 __________
|  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
| | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
| |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
|____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/


Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
server unknown:&gt;</screen>
<simpara>Connect the <literal>shell</literal> to the <literal>server</literal> running on , e.g., <literal><link xlink:href="http://dataflow-server.app.io">dataflow-server.app.io</link></literal></simpara>
<screen>server unknown:&gt;dataflow config server http://dataflow-server.app.io
Successfully targeted http://dataflow-server.app.io
dataflow:&gt;</screen>
<itemizedlist>
<listitem>
<simpara>A Geode installation with a locator and cache server running
Unresolved directive in streaming/gemfire/gemfire-log/overview.adoc - include::geode-setup.adoc[]</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="gemfire-log-local">
<title>Using the Local Server</title>
<section xml:id="_additional_prerequisites_7">
<title>Additional Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>A Running Data Flow Server</simpara>
</listitem>
</itemizedlist>
<simpara>The Local Data Flow Server is Spring Boot application available for <link xlink:href="http://cloud.spring.io/spring-cloud-dataflow/">download</link> or you can <link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow">build</link> it yourself.
If you build it yourself, the executable jar will be in <literal>spring-cloud-dataflow-server-local/target</literal></simpara>
<simpara>To run the Local Data Flow server Open a new terminal session:</simpara>
<screen>$cd  &lt;PATH/TO/SPRING-CLOUD-DATAFLOW-LOCAL-JAR&gt;
$java -jar spring-cloud-dataflow-server-local-&lt;VERSION&gt;.jar</screen>
<itemizedlist>
<listitem>
<simpara>A running instance of <link xlink:href="https://www.rabbitmq.com">Rabbit MQ</link></simpara>
</listitem>
</itemizedlist>
<simpara><?asciidoc-hr?></simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Use gfsh to start a locator and server</simpara>
<screen>gfsh&gt;start locator --name=locator1
gfsh&gt;start server --name=server1</screen>
</listitem>
<listitem>
<simpara>Create a region called <literal>Test</literal></simpara>
<screen>gfsh&gt;create region --name Test --type=REPLICATE</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="_use_the_shell_to_create_the_sample_stream_2">
<title>Use the Shell to create the sample stream.</title>
<orderedlist numeration="arabic">
<listitem>
<simpara><link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow/blob/master/spring-cloud-dataflow-docs/src/main/asciidoc/streams.adoc#register-a-stream-app">Register</link> the out-of-the-box applications for the Rabbit binder</simpara>
<note>
<simpara>These samples assume that the Data Flow Server can access a remote Maven repository, <literal><link xlink:href="https://repo.spring.io/libs-release">repo.spring.io/libs-release</link></literal> by default. If your Data Flow server is running behind a firewall, or you are using a maven proxy preventing
access to public repositories, you will need to install the sample apps in your internal Maven repository and <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started-maven-configuration">configure</link>
the server accordingly.  The sample applications are typically registered using Data Flow&#8217;s bulk import facility. For example, the Shell command <literal>dataflow:&gt;app import --uri <link xlink:href="http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven">bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</link></literal> (The actual URI is release and binder specific so refer to the sample instructions for the actual URL).
The bulk import URI references a plain text file containing entries for all of the publicly available Spring Cloud Stream and Task applications published to <literal><link xlink:href="https://repo.spring.io">repo.spring.io</link></literal>. For example,
<literal>source.http=maven://org.springframework.cloud.stream.app:http-source-rabbit:1.2.0.RELEASE</literal> registers the <literal>http</literal> source app at the corresponding Maven address, relative to the remote repository(ies) configured for the
Data Flow server. The format is <literal>maven://&lt;groupId&gt;:&lt;artifactId&gt;:&lt;version&gt;</literal>  You will need to <link xlink:href="https://repo.spring.io/libs-release/org/springframework/cloud/stream/app/spring-cloud-stream-app-descriptor/Bacon.RELEASE/spring-cloud-stream-app-descriptor-Bacon.RELEASE.rabbit-apps-maven-repo-url.properties">download</link> the required apps or <link xlink:href="https://github.com/spring-cloud-stream-app-starters">build</link> them and then install them in your Maven repository, using whatever group, artifact, and version you choose. If you do
this, register individual apps using <literal>dataflow:&gt;app register&#8230;&#8203;</literal> using the <literal>maven://</literal> resource URI format corresponding to your installed app.</simpara>
</note>
<screen>dataflow:&gt;app import --uri http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</screen>
</listitem>
<listitem>
<simpara>Create the stream</simpara>
<simpara>This example creates an gemfire source to which will publish events on a region</simpara>
<screen>dataflow:&gt;stream create --name events --definition " gemfire --regionName=Test | log" --deploy
Created and deployed new stream 'events'</screen>
<note>
<simpara>If the Geode locator isn&#8217;t running on default port on <literal>localhost</literal>, add the options <literal>--connect-type=locator --host-addresses=&lt;host&gt;:&lt;port&gt;</literal>. If there are multiple
locators, you can provide a comma separated list of locator addresses. This is not necessary for the sample but is typical for production environments to enable fail-over.</simpara>
</note>
</listitem>
<listitem>
<simpara>Verify the stream is successfully deployed</simpara>
<screen>dataflow:&gt;stream list</screen>
</listitem>
<listitem>
<simpara>Monitor stdout for the log sink. When you deploy the stream, you will see log messages in the Data Flow server console like this</simpara>
<screen>2017-10-28 17:28:23.275  INFO 15603 --- [nio-9393-exec-2] o.s.c.d.spi.local.LocalAppDeployer       : Deploying app with deploymentId events.log instance 0.
   Logs will be in /var/folders/hd/5yqz2v2d3sxd3n879f4sg4gr0000gn/T/spring-cloud-dataflow-4093992067314402881/events-1509226103269/events.log
2017-10-28 17:28:23.277  INFO 15603 --- [nio-9393-exec-2] o.s.c.d.s.c.StreamDeploymentController   : Downloading resource URI [maven://org.springframework.cloud.stream.app:gemfire-source-rabbit:1.2.0.RELEASE]
2017-10-28 17:28:23.311  INFO 15603 --- [nio-9393-exec-2] o.s.c.d.s.c.StreamDeploymentController   : Deploying application named [gemfire] as part of stream named [events] with resource URI [maven://org.springframework.cloud.stream.app:gemfire-source-rabbit:1.2.0.RELEASE]
2017-10-28 17:28:23.318  INFO 15603 --- [nio-9393-exec-2] o.s.c.d.spi.local.LocalAppDeployer       : Deploying app with deploymentId events.gemfire instance 0.
   Logs will be in /var/folders/hd/5yqz2v2d3sxd3n879f4sg4gr0000gn/T/spring-cloud-dataflow-4093992067314402881/events-1509226103311/events.gemfire</screen>
<simpara>Copy the location of the <literal>log</literal> sink logs. This is a directory that ends in <literal>events.log</literal>. The log files will be in <literal>stdout_0.log</literal> under this directory. You can monitor the output of the log sink using <literal>tail</literal>, or something similar:</simpara>
<screen>$tail -f /var/folders/hd/5yqz2v2d3sxd3n879f4sg4gr0000gn/T/spring-cloud-dataflow-4093992067314402881/events-1509226103269/events.log/stdout_0.log</screen>
</listitem>
<listitem>
<simpara>Using <literal>gfsh</literal>, create and update some cache entries</simpara>
<screen>gfsh&gt;put --region /Test --key 1  --value "value 1"
gfsh&gt;put --region /Test --key 2  --value "value 2"
gfsh&gt;put --region /Test --key 3  --value "value 3"
gfsh&gt;put --region /Test --key 1  --value "new value 1"</screen>
</listitem>
<listitem>
<simpara>Observe the log output
You should see messages like:</simpara>
<screen>2017-10-28 17:28:52.893  INFO 18986 --- [emfire.events-1] log sink                               : value 1"
2017-10-28 17:28:52.893  INFO 18986 --- [emfire.events-1] log sink                               : value 2"
2017-10-28 17:28:52.893  INFO 18986 --- [emfire.events-1] log sink                               : value 3"
2017-10-28 17:28:52.893  INFO 18986 --- [emfire.events-1] log sink                               : new value 1"</screen>
<simpara>By default, the message payload contains the updated value. Depending on your application, you may need additional information. The data comes from <link xlink:href="https://geode.apache.org/releases/latest/javadoc/org/apache/geode/cache/EntryEvent.html">EntryEvent</link>. You
can access any fields using the source&#8217;s <literal>cache-event-expression</literal> property. This takes a SpEL expression bound to the EntryEvent. Try something like <literal>--cache-event-expression='{key:'+key+',new_value:'+newValue+'}'</literal> (HINT: You will need to destroy the stream and recreate it to
add this property, an exercise left to the reader). Now you should see log messages like:</simpara>
<screen>2017-10-28 17:28:52.893  INFO 18986 --- [emfire.events-1] log-sink                                 : {key:1,new_value:value 1}
2017-10-28 17:41:24.466  INFO 18986 --- [emfire.events-1] log-sink                                 : {key:2,new_value:value 2}</screen>
</listitem>
<listitem>
<simpara>You&#8217;re done!</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="gemfire-log-cf">
<title>Using the Cloud Foundry Server</title>
<section xml:id="_additional_prerequisites_8">
<title>Additional Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>A Cloud Foundry instance</simpara>
</listitem>
<listitem>
<simpara>The Spring Cloud Data Flow Cloud Foundry Server</simpara>
</listitem>
</itemizedlist>
<simpara>The Cloud Foundry Data Flow Server is Spring Boot application available for <link xlink:href="http://cloud.spring.io/spring-cloud-dataflow/">download</link> or you can <link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow-server-cloudfoundry">build</link> it yourself.
If you build it yourself, the executable jar will be in <literal>spring-cloud-dataflow-server-cloudfoundry/target</literal></simpara>
<note>
<simpara>Although you can run the Data Flow Cloud Foundry Server locally and configure it to deploy to any Cloud Foundry instance, we will
deploy the server to Cloud Foundry as recommended.</simpara>
</note>
<orderedlist numeration="arabic">
<listitem>
<simpara>Verify that CF instance is reachable (Your endpoint urls will be different from what is shown here).</simpara>
<screen>$ cf api
API endpoint: https://api.system.io (API version: ...)

$ cf apps
Getting apps in org [your-org] / space [your-space] as user...
OK

No apps found</screen>
</listitem>
<listitem>
<simpara>Follow the instructions to deploy the <link xlink:href="https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current/reference/htmlsingle">Spring Cloud Data Flow Cloud Foundry server</link>. Don&#8217;t worry about creating a Redis service. We won&#8217;t need it. If you are familiar with Cloud Foundry
application manifests, we recommend creating a manifest for the the Data Flow server as shown <link xlink:href="https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current/reference/htmlsingle/#sample-manifest-template">here</link>.</simpara>
<warning>
<simpara>As of this writing, there is a typo on the <literal>SPRING_APPLICATION_JSON</literal> entry in the sample manifest. <literal>SPRING_APPLICATION_JSON</literal> must be followed by <literal>:</literal> and The JSON string must be
wrapped in single quotes. Alternatively, you can replace that line with <literal>MAVEN_REMOTE_REPOSITORIES_REPO1_URL: <link xlink:href="https://repo.spring.io/libs-snapshot">repo.spring.io/libs-snapshot</link></literal>.  If your Cloud Foundry installation is behind a firewall, you may need to install the stream apps used in this sample in your internal Maven repository and <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/1.3.0.M2/reference/htmlsingle/#getting-started-maven-configuration">configure</link> the server to access that repository.</simpara>
</warning>
</listitem>
<listitem>
<simpara>Once you have successfully executed <literal>cf push</literal>, verify the dataflow server is running</simpara>
<screen>$ cf apps
Getting apps in org [your-org] / space [your-space] as user...
OK

name                 requested state   instances   memory   disk   urls
dataflow-server      started           1/1         1G       1G     dataflow-server.app.io</screen>
</listitem>
<listitem>
<simpara>Notice that the <literal>dataflow-server</literal> application is started and ready for interaction via the url endpoint</simpara>
</listitem>
<listitem>
<simpara>Connect the <literal>shell</literal> with <literal>server</literal> running on Cloud Foundry, e.g., <literal><link xlink:href="http://dataflow-server.app.io">dataflow-server.app.io</link></literal></simpara>
<screen>$ cd &lt;PATH/TO/SPRING-CLOUD-DATAFLOW-SHELL-JAR&gt;
$ java -jar spring-cloud-dataflow-shell-&lt;VERSION&gt;.jar

  ____                              ____ _                __
 / ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
 \___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
  ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
 |____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
  ____ |_|    _          __|___/                 __________
 |  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
 | | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
 | |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
 |____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/


Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
server-unknown:&gt;</screen>
<screen>server-unknown:&gt;dataflow config server http://dataflow-server.app.io
Successfully targeted http://dataflow-server.app.io
dataflow:&gt;</screen>
<itemizedlist>
<listitem>
<simpara>Running instance of a <literal>rabbit</literal> service in Cloud Foundry</simpara>
</listitem>
<listitem>
<simpara>Running instance of the <link xlink:href="https://docs.pivotal.io/p-cloud-cache/1-0/developer.html">Pivotal Cloud Cache for PCF</link> (PCC) service <literal>cloudcache</literal> in Cloud Foundry.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow/blob/master/spring-cloud-dataflow-docs/src/main/asciidoc/streams.adoc#register-a-stream-app">Register</link> the out-of-the-box applications for the Rabbit binder</simpara>
<note>
<simpara>These samples assume that the Data Flow Server can access a remote Maven repository, <literal><link xlink:href="https://repo.spring.io/libs-release">repo.spring.io/libs-release</link></literal> by default. If your Data Flow server is running behind a firewall, or you are using a maven proxy preventing
access to public repositories, you will need to install the sample apps in your internal Maven repository and <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started-maven-configuration">configure</link>
the server accordingly.  The sample applications are typically registered using Data Flow&#8217;s bulk import facility. For example, the Shell command <literal>dataflow:&gt;app import --uri <link xlink:href="http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven">bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</link></literal> (The actual URI is release and binder specific so refer to the sample instructions for the actual URL).
The bulk import URI references a plain text file containing entries for all of the publicly available Spring Cloud Stream and Task applications published to <literal><link xlink:href="https://repo.spring.io">repo.spring.io</link></literal>. For example,
<literal>source.http=maven://org.springframework.cloud.stream.app:http-source-rabbit:1.2.0.RELEASE</literal> registers the <literal>http</literal> source app at the corresponding Maven address, relative to the remote repository(ies) configured for the
Data Flow server. The format is <literal>maven://&lt;groupId&gt;:&lt;artifactId&gt;:&lt;version&gt;</literal>  You will need to <link xlink:href="https://repo.spring.io/libs-release/org/springframework/cloud/stream/app/spring-cloud-stream-app-descriptor/Bacon.RELEASE/spring-cloud-stream-app-descriptor-Bacon.RELEASE.rabbit-apps-maven-repo-url.properties">download</link> the required apps or <link xlink:href="https://github.com/spring-cloud-stream-app-starters">build</link> them and then install them in your Maven repository, using whatever group, artifact, and version you choose. If you do
this, register individual apps using <literal>dataflow:&gt;app register&#8230;&#8203;</literal> using the <literal>maven://</literal> resource URI format corresponding to your installed app.</simpara>
</note>
<screen>dataflow:&gt;app import --uri http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</screen>
</listitem>
<listitem>
<simpara>Get the PCC connection information</simpara>
<screen>$ cf service-key cloudcache my-service-key
Getting key my-service-key for service instance cloudcache as &lt;user&gt;...

{
 "locators": [
  "10.0.16.9[55221]",
  "10.0.16.11[55221]",
  "10.0.16.10[55221]"
 ],
 "urls": {
  "gfsh": "http://...",
  "pulse": "http://.../pulse"
 },
 "users": [
  {
   "password": &lt;password&gt;,
   "username": "cluster_operator"
  },
  {
   "password": &lt;password&gt;,
   "username": "developer"
  }
 ]
}</screen>
</listitem>
<listitem>
<simpara>Using <literal>gfsh</literal>, connect to the PCC instance as <literal>cluster_operator</literal> using the service key values and create the Test region.</simpara>
<screen>gfsh&gt;connect --use-http --url=&lt;gfsh-url&gt; --user=cluster_operator --password=&lt;cluster_operator_password&gt;
gfsh&gt;create region --name Test --type=REPLICATE</screen>
</listitem>
<listitem>
<simpara>Create the stream, connecting to the PCC instance as developer. This example creates an gemfire source to which will publish events on a region</simpara>
<screen>dataflow stream create --name events --definition " gemfire --username=developer --password=&lt;developer-password&gt; --connect-type=locator --host-addresses=10.0.16.9:55221 --regionName=Test | log" --deploy</screen>
</listitem>
<listitem>
<simpara>Verify the stream is successfully deployed</simpara>
<screen>dataflow:&gt;stream list</screen>
</listitem>
<listitem>
<simpara>Monitor stdout for the log sink</simpara>
<screen>cf logs &lt;log-sink-app-name&gt;</screen>
</listitem>
<listitem>
<simpara>Using <literal>gfsh</literal>, create and update some cache entries</simpara>
<screen>gfsh&gt;connect --use-http --url=&lt;gfsh-url&gt; --user=cluster_operator --password=&lt;cluster_operator_password&gt;
gfsh&gt;put --region /Test --key 1  --value "value 1"
gfsh&gt;put --region /Test --key 2  --value "value 2"
gfsh&gt;put --region /Test --key 3  --value "value 3"
gfsh&gt;put --region /Test --key 1  --value "new value 1"</screen>
</listitem>
<listitem>
<simpara>Observe the log output</simpara>
<simpara>You should see messages like:</simpara>
<screen>2017-10-28 17:28:52.893  INFO 18986 --- [emfire.events-1] log sink                               : value 1"
2017-10-28 17:28:52.893  INFO 18986 --- [emfire.events-1] log sink                               : value 2"
2017-10-28 17:28:52.893  INFO 18986 --- [emfire.events-1] log sink                               : value 3"
2017-10-28 17:28:52.893  INFO 18986 --- [emfire.events-1] log sink                               : new value 1"</screen>
<simpara>By default, the message payload contains the updated value. Depending on your application, you may need additional information. The data comes from <link xlink:href="https://geode.apache.org/releases/latest/javadoc/org/apache/geode/cache/EntryEvent.html">EntryEvent</link>. You
can access any fields using the source&#8217;s <literal>cache-event-expression</literal> property. This takes a SpEL expression bound to the EntryEvent. Try something like <literal>--cache-event-expression='{key:'+key+',new_value:'+newValue+'}'</literal> (HINT: You will need to destroy the stream and recreate it to
add this property, an exercise left to the reader). Now you should see log messages like:</simpara>
<screen>2017-10-28 17:28:52.893  INFO 18986 --- [emfire.events-1] log-sink                                 : {key:1,new_value:value 1}
2017-10-28 17:41:24.466  INFO 18986 --- [emfire.events-1] log-sink                                 : {key:2,new_value:value 2}</screen>
</listitem>
<listitem>
<simpara>You&#8217;re done!</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="_summary_4">
<title>Summary</title>
<simpara>In this sample, you have learned:</simpara>
<itemizedlist>
<listitem>
<simpara>How to use Spring Cloud Data Flow&#8217;s <literal>Local</literal> and <literal>Cloud Foundry</literal> servers</simpara>
</listitem>
<listitem>
<simpara>How to use Spring Cloud Data Flow&#8217;s <literal>shell</literal></simpara>
</listitem>
<listitem>
<simpara>How to create streaming data pipeline to connect and publish events from <literal>gemfire</literal></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="spring-cloud-data-flow-samples-gemfire-cq-log-overview">
<title>Gemfire CQ to Log Demo</title>
<simpara>In this demonstration, you will learn how to build a data pipeline using <link xlink:href="http://cloud.spring.io/spring-cloud-dataflow/">Spring Cloud Data Flow</link> to consume data from a <literal>gemfire-cq</literal> (Continuous Query) endpoint and write to a log using the <literal>log</literal> sink.
The <literal>gemfire-cq</literal> source creates a Continuous Query to monitor events for a region that match the query&#8217;s result set and publish a message whenever such an event is emitted. In this example, we simulate monitoring orders to trigger a process whenever
the quantity ordered is above a defined limit.</simpara>
<simpara>We will take you through the steps to configure and run Spring Cloud Data Flow server in either a <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started/">local</link> or <link xlink:href="https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current/reference/htmlsingle/#getting-started">Cloud Foundry</link> environment.</simpara>
<note>
<simpara>For legacy reasons the <literal>gemfire</literal> Spring Cloud Stream Apps are named after <literal>Pivotal GemFire</literal>. The code base for the commercial product has since been open sourced as <literal>Apache Geode</literal>. These samples should work with compatible versions of Pivotal GemFire or Apache Geode. Herein we will refer to the installed IMDG simply as <literal>Geode</literal>.</simpara>
</note>
<section xml:id="_prerequisites_5">
<title>Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>A Running Data Flow Shell</simpara>
</listitem>
</itemizedlist>
<simpara>The Spring Cloud Data Flow Shell is available for <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started-deploying-spring-cloud-dataflow">download</link> or you can <link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow">build</link> it yourself.</simpara>
<note>
<simpara>The Spring Cloud Data Flow Shell is a Spring Boot application that connects to the Data Flow Server’s REST API and supports a DSL that simplifies the process of defining a stream or task and managing its lifecycle. Most of these samples
use the shell. If you prefer, you can use the Data Flow UI <link xlink:href="http://localhost:9393/dashboard">localhost:9393/dashboard</link>, (or wherever it the server is hosted) to perform the same operations.</simpara>
</note>
<note>
<simpara>the Spring Cloud Data Flow Shell and Local server implementation are in the same repository and are both built by running <literal>./mvnw install</literal> from the project root directory. If you have already run the build, use the jar in <literal>spring-cloud-dataflow-shell/target</literal></simpara>
</note>
<simpara>To run the Shell open a new terminal session:</simpara>
<screen>$ cd &lt;PATH/TO/SPRING-CLOUD-DATAFLOW-SHELL-JAR&gt;
$ java -jar spring-cloud-dataflow-shell-&lt;VERSION&gt;.jar
  ____                              ____ _                __
 / ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
 \___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
  ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
 |____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
  ____ |_|    _          __|___/                 __________
 |  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
 | | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
 | |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
 |____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/


Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
dataflow:&gt;</screen>
<note>
<simpara>The shell will try to connect to a local server by default. If the Local Dataflow Server is not running you will see:</simpara>
</note>
<screen> ____                              ____ _                __
/ ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
\___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
 ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
|____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
 ____ |_|    _          __|___/                 __________
|  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
| | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
| |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
|____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/


Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
server unknown:&gt;</screen>
<simpara>Connect the <literal>shell</literal> to the <literal>server</literal> running on , e.g., <literal><link xlink:href="http://dataflow-server.app.io">dataflow-server.app.io</link></literal></simpara>
<screen>server unknown:&gt;dataflow config server http://dataflow-server.app.io
Successfully targeted http://dataflow-server.app.io
dataflow:&gt;</screen>
<itemizedlist>
<listitem>
<simpara>A Geode installation with a locator and cache server running
Unresolved directive in streaming/gemfire/gemfire-cq-log/overview.adoc - include::geode-setup.adoc[]</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="gemfire-cq-log-local">
<title>Using the Local Server</title>
<section xml:id="_additional_prerequisites_9">
<title>Additional Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>A Running Data Flow Server</simpara>
</listitem>
</itemizedlist>
<simpara>The Local Data Flow Server is Spring Boot application available for <link xlink:href="http://cloud.spring.io/spring-cloud-dataflow/">download</link> or you can <link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow">build</link> it yourself.
If you build it yourself, the executable jar will be in <literal>spring-cloud-dataflow-server-local/target</literal></simpara>
<simpara>To run the Local Data Flow server Open a new terminal session:</simpara>
<screen>$cd  &lt;PATH/TO/SPRING-CLOUD-DATAFLOW-LOCAL-JAR&gt;
$java -jar spring-cloud-dataflow-server-local-&lt;VERSION&gt;.jar</screen>
<itemizedlist>
<listitem>
<simpara>A running instance of <link xlink:href="https://www.rabbitmq.com">Rabbit MQ</link></simpara>
</listitem>
</itemizedlist>
<simpara><?asciidoc-hr?></simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Use gfsh to start a locator and server</simpara>
<screen>gfsh&gt;start locator --name=locator1
gfsh&gt;start server --name=server1</screen>
</listitem>
<listitem>
<simpara>Create a region called <literal>Orders</literal></simpara>
<screen>gfsh&gt;create region --name Orders --type=REPLICATE</screen>
<simpara>===== Use the Shell to create the sample stream.</simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow/blob/master/spring-cloud-dataflow-docs/src/main/asciidoc/streams.adoc#register-a-stream-app">Register</link> the out-of-the-box applications for the Rabbit binder</simpara>
<note>
<simpara>These samples assume that the Data Flow Server can access a remote Maven repository, <literal><link xlink:href="https://repo.spring.io/libs-release">repo.spring.io/libs-release</link></literal> by default. If your Data Flow server is running behind a firewall, or you are using a maven proxy preventing
access to public repositories, you will need to install the sample apps in your internal Maven repository and <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started-maven-configuration">configure</link>
the server accordingly.  The sample applications are typically registered using Data Flow&#8217;s bulk import facility. For example, the Shell command <literal>dataflow:&gt;app import --uri <link xlink:href="http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven">bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</link></literal> (The actual URI is release and binder specific so refer to the sample instructions for the actual URL).
The bulk import URI references a plain text file containing entries for all of the publicly available Spring Cloud Stream and Task applications published to <literal><link xlink:href="https://repo.spring.io">repo.spring.io</link></literal>. For example,
<literal>source.http=maven://org.springframework.cloud.stream.app:http-source-rabbit:1.2.0.RELEASE</literal> registers the <literal>http</literal> source app at the corresponding Maven address, relative to the remote repository(ies) configured for the
Data Flow server. The format is <literal>maven://&lt;groupId&gt;:&lt;artifactId&gt;:&lt;version&gt;</literal>  You will need to <link xlink:href="https://repo.spring.io/libs-release/org/springframework/cloud/stream/app/spring-cloud-stream-app-descriptor/Bacon.RELEASE/spring-cloud-stream-app-descriptor-Bacon.RELEASE.rabbit-apps-maven-repo-url.properties">download</link> the required apps or <link xlink:href="https://github.com/spring-cloud-stream-app-starters">build</link> them and then install them in your Maven repository, using whatever group, artifact, and version you choose. If you do
this, register individual apps using <literal>dataflow:&gt;app register&#8230;&#8203;</literal> using the <literal>maven://</literal> resource URI format corresponding to your installed app.</simpara>
</note>
<screen>dataflow:&gt;app import --uri http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</screen>
</listitem>
<listitem>
<simpara>Create the stream</simpara>
<simpara>This example creates an gemfire-cq source to which will publish events matching a query criteria on a region. In this case we will monitor the <literal>Orders</literal> region. For simplicity, we will avoid creating a data structure for the order.
Each cache entry contains an integer value representing the quantity of the ordered item. This stream will fire a message whenever the value&gt;999. By default, the source emits only the value. Here we will override that using the
<literal>cq-event-expression</literal> property.  This accepts a SpEL expression bound to a <link xlink:href="https://geode.apache.org/releases/latest/javadoc/org/apache/geode/cache/query/CqEvent.html">CQEvent</link>. To reference the entire CQEvent instace, we use <literal>#this</literal>.
In order to display the contents in the log, we will invoke <literal>toString()</literal> on the instance.</simpara>
<screen>dataflow:&gt;stream create --name orders --definition " gemfire-cq --query='SELECT * from /Orders o where o &gt; 999' --cq-event-expression=#this.toString() | log" --deploy
Created and deployed new stream 'events'</screen>
<note>
<simpara>If the Geode locator isn&#8217;t running on default port on <literal>localhost</literal>, add the options <literal>--connect-type=locator --host-addresses=&lt;host&gt;:&lt;port&gt;</literal>. If there are multiple
locators, you can provide a comma separated list of locator addresses. This is not necessary for the sample but is typical for production environments to enable fail-over.</simpara>
</note>
</listitem>
<listitem>
<simpara>Verify the stream is successfully deployed</simpara>
<screen>dataflow:&gt;stream list</screen>
</listitem>
<listitem>
<simpara>Monitor stdout for the log sink. When you deploy the stream, you will see log messages in the Data Flow server console like this</simpara>
<screen>2017-10-30 09:39:36.283  INFO 8167 --- [nio-9393-exec-5] o.s.c.d.spi.local.LocalAppDeployer       : Deploying app with deploymentId orders.log instance 0.
   Logs will be in /var/folders/hd/5yqz2v2d3sxd3n879f4sg4gr0000gn/T/spring-cloud-dataflow-5375107584795488581/orders-1509370775940/orders.log</screen>
<simpara>Copy the location of the <literal>log</literal> sink logs. This is a directory that ends in <literal>orders.log</literal>. The log files will be in <literal>stdout_0.log</literal> under this directory. You can monitor the output of the log sink using <literal>tail</literal>, or something similar:</simpara>
<screen>$tail -f /var/folders/hd/5yqz2v2d3sxd3n879f4sg4gr0000gn/T/spring-cloud-dataflow-5375107584795488581/orders-1509370775940/orders.log/stdout_0.log</screen>
</listitem>
<listitem>
<simpara>Using <literal>gfsh</literal>, create and update some cache entries</simpara>
<screen>gfsh&gt;put --region Orders  --value-class java.lang.Integer --key 01234 --value 1000
gfsh&gt;put --region Orders  --value-class java.lang.Integer --key 11234 --value 1005
gfsh&gt;put --region Orders  --value-class java.lang.Integer --key 21234 --value 100
gfsh&gt;put --region Orders  --value-class java.lang.Integer --key 31234 --value 999
gfsh&gt;put --region Orders  --value-class java.lang.Integer --key 21234 --value 1000</screen>
</listitem>
<listitem>
<simpara>Observe the log output
You should see messages like:</simpara>
<screen>2017-10-30 09:53:02.231  INFO 8563 --- [ire-cq.orders-1] log-sink                                 : CqEvent [CqName=GfCq1; base operation=CREATE; cq operation=CREATE; key=01234; value=1000]
2017-10-30 09:53:19.732  INFO 8563 --- [ire-cq.orders-1] log-sink                                 : CqEvent [CqName=GfCq1; base operation=CREATE; cq operation=CREATE; key=11234; value=1005]
2017-10-30 09:53:53.242  INFO 8563 --- [ire-cq.orders-1] log-sink                                 : CqEvent [CqName=GfCq1; base operation=UPDATE; cq operation=CREATE; key=21234; value=1000]</screen>
</listitem>
<listitem>
<simpara>Another interesting demonstration combines <literal>gemfire-cq</literal> with the <link xlink:href=":../http-gemfire/README.adoc">http-gemfire</link> example.</simpara>
</listitem>
</orderedlist>
<screen>dataflow:&gt; stream create --name stocks --definition "http --port=9090 | gemfire-json-server --regionName=Stocks --keyExpression=payload.getField('symbol')" --deploy
dataflow:&gt; stream create --name stock_watch --definition "gemfire-cq --query='Select * from /Stocks where symbol=''VMW''' | log" --deploy</screen>
<orderedlist numeration="arabic">
<listitem>
<simpara>You&#8217;re done!</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="gemfire-cq-log-cf">
<title>Using the Cloud Foundry Server</title>
<section xml:id="_additional_prerequisites_10">
<title>Additional Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>A Cloud Foundry instance</simpara>
</listitem>
<listitem>
<simpara>The Spring Cloud Data Flow Cloud Foundry Server</simpara>
</listitem>
</itemizedlist>
<simpara>The Cloud Foundry Data Flow Server is Spring Boot application available for <link xlink:href="http://cloud.spring.io/spring-cloud-dataflow/">download</link> or you can <link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow-server-cloudfoundry">build</link> it yourself.
If you build it yourself, the executable jar will be in <literal>spring-cloud-dataflow-server-cloudfoundry/target</literal></simpara>
<note>
<simpara>Although you can run the Data Flow Cloud Foundry Server locally and configure it to deploy to any Cloud Foundry instance, we will
deploy the server to Cloud Foundry as recommended.</simpara>
</note>
<orderedlist numeration="arabic">
<listitem>
<simpara>Verify that CF instance is reachable (Your endpoint urls will be different from what is shown here).</simpara>
<screen>$ cf api
API endpoint: https://api.system.io (API version: ...)

$ cf apps
Getting apps in org [your-org] / space [your-space] as user...
OK

No apps found</screen>
</listitem>
<listitem>
<simpara>Follow the instructions to deploy the <link xlink:href="https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current/reference/htmlsingle">Spring Cloud Data Flow Cloud Foundry server</link>. Don&#8217;t worry about creating a Redis service. We won&#8217;t need it. If you are familiar with Cloud Foundry
application manifests, we recommend creating a manifest for the the Data Flow server as shown <link xlink:href="https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current/reference/htmlsingle/#sample-manifest-template">here</link>.</simpara>
<warning>
<simpara>As of this writing, there is a typo on the <literal>SPRING_APPLICATION_JSON</literal> entry in the sample manifest. <literal>SPRING_APPLICATION_JSON</literal> must be followed by <literal>:</literal> and The JSON string must be
wrapped in single quotes. Alternatively, you can replace that line with <literal>MAVEN_REMOTE_REPOSITORIES_REPO1_URL: <link xlink:href="https://repo.spring.io/libs-snapshot">repo.spring.io/libs-snapshot</link></literal>.  If your Cloud Foundry installation is behind a firewall, you may need to install the stream apps used in this sample in your internal Maven repository and <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/1.3.0.M2/reference/htmlsingle/#getting-started-maven-configuration">configure</link> the server to access that repository.</simpara>
</warning>
</listitem>
<listitem>
<simpara>Once you have successfully executed <literal>cf push</literal>, verify the dataflow server is running</simpara>
<screen>$ cf apps
Getting apps in org [your-org] / space [your-space] as user...
OK

name                 requested state   instances   memory   disk   urls
dataflow-server      started           1/1         1G       1G     dataflow-server.app.io</screen>
</listitem>
<listitem>
<simpara>Notice that the <literal>dataflow-server</literal> application is started and ready for interaction via the url endpoint</simpara>
</listitem>
<listitem>
<simpara>Connect the <literal>shell</literal> with <literal>server</literal> running on Cloud Foundry, e.g., <literal><link xlink:href="http://dataflow-server.app.io">dataflow-server.app.io</link></literal></simpara>
<screen>$ cd &lt;PATH/TO/SPRING-CLOUD-DATAFLOW-SHELL-JAR&gt;
$ java -jar spring-cloud-dataflow-shell-&lt;VERSION&gt;.jar

  ____                              ____ _                __
 / ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
 \___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
  ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
 |____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
  ____ |_|    _          __|___/                 __________
 |  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
 | | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
 | |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
 |____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/


Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
server-unknown:&gt;</screen>
<screen>server-unknown:&gt;dataflow config server http://dataflow-server.app.io
Successfully targeted http://dataflow-server.app.io
dataflow:&gt;</screen>
<itemizedlist>
<listitem>
<simpara>Running instance of a <literal>rabbit</literal> service in Cloud Foundry</simpara>
</listitem>
<listitem>
<simpara>Running instance of the <link xlink:href="https://docs.pivotal.io/p-cloud-cache/1-0/developer.html">Pivotal Cloud Cache for PCF</link> (PCC) service <literal>cloudcache</literal> in Cloud Foundry.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow/blob/master/spring-cloud-dataflow-docs/src/main/asciidoc/streams.adoc#register-a-stream-app">Register</link> the out-of-the-box applications for the Rabbit binder</simpara>
<note>
<simpara>These samples assume that the Data Flow Server can access a remote Maven repository, <literal><link xlink:href="https://repo.spring.io/libs-release">repo.spring.io/libs-release</link></literal> by default. If your Data Flow server is running behind a firewall, or you are using a maven proxy preventing
access to public repositories, you will need to install the sample apps in your internal Maven repository and <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started-maven-configuration">configure</link>
the server accordingly.  The sample applications are typically registered using Data Flow&#8217;s bulk import facility. For example, the Shell command <literal>dataflow:&gt;app import --uri <link xlink:href="http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven">bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</link></literal> (The actual URI is release and binder specific so refer to the sample instructions for the actual URL).
The bulk import URI references a plain text file containing entries for all of the publicly available Spring Cloud Stream and Task applications published to <literal><link xlink:href="https://repo.spring.io">repo.spring.io</link></literal>. For example,
<literal>source.http=maven://org.springframework.cloud.stream.app:http-source-rabbit:1.2.0.RELEASE</literal> registers the <literal>http</literal> source app at the corresponding Maven address, relative to the remote repository(ies) configured for the
Data Flow server. The format is <literal>maven://&lt;groupId&gt;:&lt;artifactId&gt;:&lt;version&gt;</literal>  You will need to <link xlink:href="https://repo.spring.io/libs-release/org/springframework/cloud/stream/app/spring-cloud-stream-app-descriptor/Bacon.RELEASE/spring-cloud-stream-app-descriptor-Bacon.RELEASE.rabbit-apps-maven-repo-url.properties">download</link> the required apps or <link xlink:href="https://github.com/spring-cloud-stream-app-starters">build</link> them and then install them in your Maven repository, using whatever group, artifact, and version you choose. If you do
this, register individual apps using <literal>dataflow:&gt;app register&#8230;&#8203;</literal> using the <literal>maven://</literal> resource URI format corresponding to your installed app.</simpara>
</note>
<screen>dataflow:&gt;app import --uri http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</screen>
</listitem>
<listitem>
<simpara>Get the PCC connection information</simpara>
<screen>$ cf service-key cloudcache my-service-key
Getting key my-service-key for service instance cloudcache as &lt;user&gt;...

{
 "locators": [
  "10.0.16.9[55221]",
  "10.0.16.11[55221]",
  "10.0.16.10[55221]"
 ],
 "urls": {
  "gfsh": "http://...",
  "pulse": "http://.../pulse"
 },
 "users": [
  {
   "password": &lt;password&gt;,
   "username": "cluster_operator"
  },
  {
   "password": &lt;password&gt;,
   "username": "developer"
  }
 ]
}</screen>
</listitem>
<listitem>
<simpara>Using <literal>gfsh</literal>, connect to the PCC instance as <literal>cluster_operator</literal> using the service key values and create the Test region.</simpara>
<screen>gfsh&gt;connect --use-http --url=&lt;gfsh-url&gt; --user=cluster_operator --password=&lt;cluster_operator_password&gt;
gfsh&gt;create region --name Orders --type=REPLICATE</screen>
</listitem>
<listitem>
<simpara>Create the stream using the Data Flow Shell</simpara>
<simpara>This example creates an gemfire-cq source to which will publish events matching a query criteria on a region. In this case we will monitor the <literal>Orders</literal> region. For simplicity, we will avoid creating a data structure for the order.
Each cache entry contains an integer value representing the quantity of the ordered item. This stream will fire a message whenever the value&gt;999. By default, the source emits only the value. Here we will override that using the
<literal>cq-event-expression</literal> property.  This accepts a SpEL expression bound to a <link xlink:href="https://geode.apache.org/releases/latest/javadoc/org/apache/geode/cache/query/CqEvent.html">CQEvent</link>. To reference the entire CQEvent instace, we use <literal>#this</literal>.
In order to display the contents in the log, we will invoke <literal>toString()</literal> on the instance.</simpara>
<screen>dataflow:&gt;stream create --name orders --definition " gemfire-cq  --username=developer --password=&lt;developer-password&gt; --connect-type=locator --host-addresses=10.0.16.9:55221 --query='SELECT * from /Orders o where o &gt; 999' --cq-event-expression=#this.toString()  | log" --deploy
Created and deployed new stream 'events'</screen>
</listitem>
<listitem>
<simpara>Verify the stream is successfully deployed</simpara>
<screen>dataflow:&gt;stream list</screen>
</listitem>
<listitem>
<simpara>Monitor stdout for the log sink</simpara>
<screen>cf logs &lt;log-sink-app-name&gt;</screen>
</listitem>
<listitem>
<simpara>Using <literal>gfsh</literal>, create and update some cache entries</simpara>
<screen>gfsh&gt;connect --use-http --url=&lt;gfsh-url&gt; --user=cluster_operator --password=&lt;cluster_operator_password&gt;
gfsh&gt;put --region Orders  --value-class java.lang.Integer --key 01234 --value 1000
gfsh&gt;put --region Orders  --value-class java.lang.Integer --key 11234 --value 1005
gfsh&gt;put --region Orders  --value-class java.lang.Integer --key 21234 --value 100
gfsh&gt;put --region Orders  --value-class java.lang.Integer --key 31234 --value 999
gfsh&gt;put --region Orders  --value-class java.lang.Integer --key 21234 --value 1000</screen>
</listitem>
<listitem>
<simpara>Observe the log output
You should see messages like:</simpara>
<screen>2017-10-30 09:53:02.231  INFO 8563 --- [ire-cq.orders-1] log-sink                                 : CqEvent [CqName=GfCq1; base operation=CREATE; cq operation=CREATE; key=01234; value=1000]
2017-10-30 09:53:19.732  INFO 8563 --- [ire-cq.orders-1] log-sink                                 : CqEvent [CqName=GfCq1; base operation=CREATE; cq operation=CREATE; key=11234; value=1005]
2017-10-30 09:53:53.242  INFO 8563 --- [ire-cq.orders-1] log-sink                                 : CqEvent [CqName=GfCq1; base operation=UPDATE; cq operation=CREATE; key=21234; value=1000]</screen>
</listitem>
<listitem>
<simpara>Another interesting demonstration combines <literal>gemfire-cq</literal> with the <link xlink:href="../http-gemfire/README.adoc">http-gemfire</link> example.</simpara>
</listitem>
</orderedlist>
<screen>dataflow:&gt; stream create --name stocks --definition "http --port=9090 | gemfire-json-server --regionName=Stocks --keyExpression=payload.getField('symbol')" --deploy
dataflow:&gt; stream create --name stock_watch --definition "gemfire-cq --query='Select * from /Stocks where symbol=''VMW''' | log" --deploy</screen>
<orderedlist numeration="arabic">
<listitem>
<simpara>You&#8217;re done!</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="_summary_5">
<title>Summary</title>
<simpara>In this sample, you have learned:</simpara>
<itemizedlist>
<listitem>
<simpara>How to use Spring Cloud Data Flow&#8217;s <literal>Local</literal> and <literal>Cloud Foundry</literal> servers</simpara>
</listitem>
<listitem>
<simpara>How to use Spring Cloud Data Flow&#8217;s <literal>shell</literal></simpara>
</listitem>
<listitem>
<simpara>How to create streaming data pipeline to connect and publish CQ events from <literal>gemfire</literal></simpara>
</listitem>
</itemizedlist>
</section>
</section>
</section>
</chapter>
<chapter xml:id="_task_batch">
<title>Task / Batch</title>
<section xml:id="spring-cloud-data-flow-samples-task-overview">
<title>Task Samples</title>
<section xml:id="_batch_job_on_cloud_foundry">
<title>Batch Job on Cloud Foundry</title>
<simpara>In this demonstration, you will learn how to orchestrate short-lived data processing application (<emphasis>eg: Spring Batch Jobs</emphasis>) using <link xlink:href="http://cloud.spring.io/spring-cloud-task/">Spring Cloud Task</link> and <link xlink:href="http://cloud.spring.io/spring-cloud-dataflow/">Spring Cloud Data Flow</link> on Cloud Foundry.</simpara>
<section xml:id="_prerequisites_6">
<title>Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>Local <link xlink:href="https://pivotal.io/pcf-dev">PCFDev</link> instance</simpara>
</listitem>
<listitem>
<simpara>Local install of <link xlink:href="https://github.com/cloudfoundry/cli">cf CLI</link> command line tool</simpara>
</listitem>
<listitem>
<simpara>Running instance of mysql in PCFDev</simpara>
</listitem>
<listitem>
<simpara>A Running Data Flow Shell</simpara>
</listitem>
</itemizedlist>
<simpara>The Spring Cloud Data Flow Shell is available for <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started-deploying-spring-cloud-dataflow">download</link> or you can <link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow">build</link> it yourself.</simpara>
<note>
<simpara>The Spring Cloud Data Flow Shell is a Spring Boot application that connects to the Data Flow Server’s REST API and supports a DSL that simplifies the process of defining a stream or task and managing its lifecycle. Most of these samples
use the shell. If you prefer, you can use the Data Flow UI <link xlink:href="http://localhost:9393/dashboard">localhost:9393/dashboard</link>, (or wherever it the server is hosted) to perform the same operations.</simpara>
</note>
<note>
<simpara>the Spring Cloud Data Flow Shell and Local server implementation are in the same repository and are both built by running <literal>./mvnw install</literal> from the project root directory. If you have already run the build, use the jar in <literal>spring-cloud-dataflow-shell/target</literal></simpara>
</note>
<simpara>To run the Shell open a new terminal session:</simpara>
<screen>$ cd &lt;PATH/TO/SPRING-CLOUD-DATAFLOW-SHELL-JAR&gt;
$ java -jar spring-cloud-dataflow-shell-&lt;VERSION&gt;.jar
  ____                              ____ _                __
 / ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
 \___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
  ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
 |____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
  ____ |_|    _          __|___/                 __________
 |  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
 | | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
 | |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
 |____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/


Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
dataflow:&gt;</screen>
<note>
<simpara>The shell will try to connect to a local server by default. If the Local Dataflow Server is not running you will see:</simpara>
</note>
<screen> ____                              ____ _                __
/ ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
\___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
 ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
|____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
 ____ |_|    _          __|___/                 __________
|  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
| | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
| |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
|____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/


Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
server unknown:&gt;</screen>
<simpara>Connect the <literal>shell</literal> to the <literal>server</literal> running on , e.g., <literal><link xlink:href="http://dataflow-server.app.io">dataflow-server.app.io</link></literal></simpara>
<screen>server unknown:&gt;dataflow config server http://dataflow-server.app.io
Successfully targeted http://dataflow-server.app.io
dataflow:&gt;</screen>
<itemizedlist>
<listitem>
<simpara>The Spring Cloud Data Flow Cloud Foundry Server running in PCFDev</simpara>
</listitem>
</itemizedlist>
<simpara>The Cloud Foundry Data Flow Server is Spring Boot application available for <link xlink:href="http://cloud.spring.io/spring-cloud-dataflow/">download</link> or you can <link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow-server-cloudfoundry">build</link> it yourself.
If you build it yourself, the executable jar will be in <literal>spring-cloud-dataflow-server-cloudfoundry/target</literal></simpara>
<note>
<simpara>Although you can run the Data Flow Cloud Foundry Server locally and configure it to deploy to any Cloud Foundry instance, we will
deploy the server to Cloud Foundry as recommended.</simpara>
</note>
<orderedlist numeration="arabic">
<listitem>
<simpara>Verify that CF instance is reachable (Your endpoint urls will be different from what is shown here).</simpara>
<screen>$ cf api
API endpoint: https://api.system.io (API version: ...)

$ cf apps
Getting apps in org [your-org] / space [your-space] as user...
OK

No apps found</screen>
</listitem>
<listitem>
<simpara>Follow the instructions to deploy the <link xlink:href="https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current/reference/htmlsingle">Spring Cloud Data Flow Cloud Foundry server</link>. Don&#8217;t worry about creating a Redis service. We won&#8217;t need it. If you are familiar with Cloud Foundry
application manifests, we recommend creating a manifest for the the Data Flow server as shown <link xlink:href="https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current/reference/htmlsingle/#sample-manifest-template">here</link>.</simpara>
<warning>
<simpara>As of this writing, there is a typo on the <literal>SPRING_APPLICATION_JSON</literal> entry in the sample manifest. <literal>SPRING_APPLICATION_JSON</literal> must be followed by <literal>:</literal> and The JSON string must be
wrapped in single quotes. Alternatively, you can replace that line with <literal>MAVEN_REMOTE_REPOSITORIES_REPO1_URL: <link xlink:href="https://repo.spring.io/libs-snapshot">repo.spring.io/libs-snapshot</link></literal>.  If your Cloud Foundry installation is behind a firewall, you may need to install the stream apps used in this sample in your internal Maven repository and <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/1.3.0.M2/reference/htmlsingle/#getting-started-maven-configuration">configure</link> the server to access that repository.</simpara>
</warning>
</listitem>
<listitem>
<simpara>Once you have successfully executed <literal>cf push</literal>, verify the dataflow server is running</simpara>
<screen>$ cf apps
Getting apps in org [your-org] / space [your-space] as user...
OK

name                 requested state   instances   memory   disk   urls
dataflow-server      started           1/1         1G       1G     dataflow-server.app.io</screen>
</listitem>
<listitem>
<simpara>Notice that the <literal>dataflow-server</literal> application is started and ready for interaction via the url endpoint</simpara>
</listitem>
<listitem>
<simpara>Connect the <literal>shell</literal> with <literal>server</literal> running on Cloud Foundry, e.g., <literal><link xlink:href="http://dataflow-server.app.io">dataflow-server.app.io</link></literal></simpara>
<screen>$ cd &lt;PATH/TO/SPRING-CLOUD-DATAFLOW-SHELL-JAR&gt;
$ java -jar spring-cloud-dataflow-shell-&lt;VERSION&gt;.jar

  ____                              ____ _                __
 / ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
 \___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
  ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
 |____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
  ____ |_|    _          __|___/                 __________
 |  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
 | | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
 | |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
 |____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/


Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
server-unknown:&gt;</screen>
<screen>server-unknown:&gt;dataflow config server http://dataflow-server.app.io
Successfully targeted http://dataflow-server.app.io
dataflow:&gt;</screen>
<note>
<simpara>PCF 1.7.12 or greater is required to run Tasks on Spring Cloud Data Flow. As of this writing, PCFDev and PWS supports builds upon this version.</simpara>
</note>
</listitem>
<listitem>
<simpara>Task support needs to be enabled on pcf-dev. Being logged as <literal>admin</literal>, issue the following command:</simpara>
<screen>cf enable-feature-flag task_creation
Setting status of task_creation as admin...

OK

Feature task_creation Enabled.</screen>
<note>
<simpara>For this sample, all you need is the <literal>mysql</literal> service and in PCFDev, the <literal>mysql</literal> service comes with a different plan. From CF CLI, create the service by: <literal>cf create-service p-mysql 512mb mysql</literal> and bind this service to <literal>dataflow-server</literal> by: <literal>cf bind-service dataflow-server mysql</literal>.</simpara>
</note>
<note>
<simpara>All the apps deployed to PCFDev start with low memory by default. It is recommended to change it to at least 768MB for <literal>dataflow-server</literal>. Ditto for every app spawned <emphasis role="strong">by</emphasis> Spring Cloud Data Flow. Change the memory by: <literal>cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_MEMORY 512</literal>. Likewise, we would have to skip SSL validation by: <literal>cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SKIP_SSL_VALIDATION true</literal>.</simpara>
</note>
</listitem>
<listitem>
<simpara>Tasks in Spring Cloud Data Flow require an RDBMS to host "task repository" (see <link xlink:href="http://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#spring-cloud-dataflow-task-repository">here</link> for more details), so let&#8217;s instruct the Spring Cloud Data Flow server to bind the <literal>mysql</literal> service to each deployed task:</simpara>
<screen>$ cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_SERVICES mysql
$ cf restage dataflow-server</screen>
<note>
<simpara>We only need <literal>mysql</literal> service for this sample.</simpara>
</note>
</listitem>
<listitem>
<simpara>As a recap, here is what you should see as configuration for the Spring Cloud Data Flow server:</simpara>
<screen>cf env dataflow-server

....
User-Provided:
SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_DOMAIN: local.pcfdev.io
SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_MEMORY: 512
SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_ORG: pcfdev-org
SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_PASSWORD: pass
SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SKIP_SSL_VALIDATION: false
SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SPACE: pcfdev-space
SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_SERVICES: mysql
SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_URL: https://api.local.pcfdev.io
SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_USERNAME: user

No running env variables have been set

No staging env variables have been set</screen>
</listitem>
<listitem>
<simpara>Notice that <literal>dataflow-server</literal> application is started and ready for interaction via <literal><link xlink:href="http://dataflow-server.local.pcfdev.io">dataflow-server.local.pcfdev.io</link></literal> endpoint</simpara>
</listitem>
<listitem>
<simpara>Build and register the batch-job <link xlink:href="https://github.com/spring-cloud/spring-cloud-task/tree/master/spring-cloud-task-samples/batch-job">example</link> from Spring Cloud Task samples. For convenience, the final <link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow-samples/raw/master/src/main/asciidoc/tasks/simple-batch-job/batch-job-1.0.0.BUILD-SNAPSHOT.jar">uber-jar artifact</link> is provided with this sample.</simpara>
<screen>dataflow:&gt;app register --type task --name simple_batch_job --uri https://github.com/spring-cloud/spring-cloud-dataflow-samples/raw/master/tasks/simple-batch-job/batch-job-1.3.0.BUILD-SNAPSHOT.jar</screen>
</listitem>
<listitem>
<simpara>Create the task with <literal>simple-batch-job</literal> application</simpara>
<screen>dataflow:&gt;task create foo --definition "simple_batch_job"</screen>
<note>
<simpara>Unlike Streams, the Task definitions don&#8217;t require explicit deployment. They can be launched on-demand, scheduled, or triggered by streams.</simpara>
</note>
</listitem>
<listitem>
<simpara>Verify there&#8217;s <emphasis role="strong">still</emphasis> no Task applications running on PCFDev - they are listed only after the initial launch/staging attempt on PCF</simpara>
<screen>$ cf apps
Getting apps in org pcfdev-org / space pcfdev-space as user...
OK

name              requested state   instances   memory   disk   urls
dataflow-server   started           1/1         768M     512M   dataflow-server.local.pcfdev.io</screen>
</listitem>
<listitem>
<simpara>Let&#8217;s launch <literal>foo</literal></simpara>
<screen>dataflow:&gt;task launch foo</screen>
</listitem>
<listitem>
<simpara>Verify the execution of <literal>foo</literal> by tailing the logs</simpara>
<screen>$ cf logs foo
Retrieving logs for app foo in org pcfdev-org / space pcfdev-space as user...

2016-08-14T18:48:54.22-0700 [APP/TASK/foo/0]OUT Creating container
2016-08-14T18:48:55.47-0700 [APP/TASK/foo/0]OUT

2016-08-14T18:49:06.59-0700 [APP/TASK/foo/0]OUT 2016-08-15 01:49:06.598  INFO 14 --- [           main] o.s.b.c.l.support.SimpleJobLauncher      : Job: [SimpleJob: [name=job1]] launched with the following parameters: [{}]

...
...

2016-08-14T18:49:06.78-0700 [APP/TASK/foo/0]OUT 2016-08-15 01:49:06.785  INFO 14 --- [           main] o.s.b.c.l.support.SimpleJobLauncher      : Job: [SimpleJob: [name=job1]] completed with the following parameters: [{}] and the following status: [COMPLETED]

...
...

2016-08-14T18:49:07.36-0700 [APP/TASK/foo/0]OUT 2016-08-15 01:49:07.363  INFO 14 --- [           main] o.s.b.c.l.support.SimpleJobLauncher      : Job: [SimpleJob: [name=job2]] launched with the following parameters: [{}]

...
...

2016-08-14T18:49:07.53-0700 [APP/TASK/foo/0]OUT 2016-08-15 01:49:07.536  INFO 14 --- [           main] o.s.b.c.l.support.SimpleJobLauncher      : Job: [SimpleJob: [name=job2]] completed with the following parameters: [{}] and the following status: [COMPLETED]

...
...

2016-08-14T18:49:07.71-0700 [APP/TASK/foo/0]OUT Exit status 0
2016-08-14T18:49:07.78-0700 [APP/TASK/foo/0]OUT Destroying container
2016-08-14T18:49:08.47-0700 [APP/TASK/foo/0]OUT Successfully destroyed container</screen>
<note>
<simpara>Verify <literal>job1</literal> and <literal>job2</literal> operations embeddded in <literal>simple-batch-job</literal> application are launched independently and they returned with the status <literal>COMPLETED</literal>.</simpara>
</note>
<note>
<simpara>Unlike LRPs in Cloud Foundry, tasks are short-lived, so the logs aren&#8217;t always available. They are generated only when the Task application runs; at the end of Task operation, the container that ran the Task application is destroyed to free-up resources.</simpara>
</note>
</listitem>
<listitem>
<simpara>List Tasks in Cloud Foundry</simpara>
<screen>$ cf apps
Getting apps in org pcfdev-org / space pcfdev-space as user...
OK

name              requested state   instances   memory   disk   urls
dataflow-server   started           1/1         768M     512M   dataflow-server.local.pcfdev.io
foo               stopped           0/1         1G       1G</screen>
</listitem>
<listitem>
<simpara>Verify Task execution details</simpara>
<screen>dataflow:&gt;task execution list
╔══════════════════════════╤══╤════════════════════════════╤════════════════════════════╤═════════╗
║        Task Name         │ID│         Start Time         │          End Time          │Exit Code║
╠══════════════════════════╪══╪════════════════════════════╪════════════════════════════╪═════════╣
║foo                       │1 │Sun Aug 14 18:49:05 PDT 2016│Sun Aug 14 18:49:07 PDT 2016│0        ║
╚══════════════════════════╧══╧════════════════════════════╧════════════════════════════╧═════════╝</screen>
</listitem>
<listitem>
<simpara>Verify Job execution details</simpara>
<screen>dataflow:&gt;job execution list
╔═══╤═══════╤═════════╤════════════════════════════╤═════════════════════╤══════════════════╗
║ID │Task ID│Job Name │         Start Time         │Step Execution Count │Definition Status ║
╠═══╪═══════╪═════════╪════════════════════════════╪═════════════════════╪══════════════════╣
║2  │1      │job2     │Sun Aug 14 18:49:07 PDT 2016│1                    │Destroyed         ║
║1  │1      │job1     │Sun Aug 14 18:49:06 PDT 2016│1                    │Destroyed         ║
╚═══╧═══════╧═════════╧════════════════════════════╧═════════════════════╧══════════════════╝</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="_summary_6">
<title>Summary</title>
<simpara>In this sample, you have learned:</simpara>
<itemizedlist>
<listitem>
<simpara>How to register and orchestrate Spring Batch jobs in Spring Cloud Data Flow</simpara>
</listitem>
<listitem>
<simpara>How to use the <literal>cf</literal> CLI in the context of Task applications orchestrated by Spring Cloud Data Flow</simpara>
</listitem>
<listitem>
<simpara>How to verify task executions and task repository</simpara>
</listitem>
</itemizedlist>
</section>
</section>
</chapter>
</book>