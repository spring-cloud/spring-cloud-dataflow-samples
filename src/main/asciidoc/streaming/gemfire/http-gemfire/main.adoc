[[spring-cloud-data-flow-samples-gemfire-http-overview]]
:sectnums:
:docs_dir: ../../..

=== HTTP to Gemfire Demo

In this demonstration, you will learn how to build a data pipeline using https://cloud.spring.io/spring-cloud-dataflow/[Spring Cloud Data Flow] to consume data from an `http` endpoint and write to Gemfire using the `gemfire` sink.

We will take you through the steps to configure and run Spring Cloud Data Flow server in either a https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started/[local] or https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current/reference/htmlsingle/#getting-started[Cloud Foundry] environment.

NOTE: For legacy reasons the `gemfire` Spring Cloud Stream Apps are named after `Pivotal GemFire`. The code base for the commercial product has since been open sourced as `Apache Geode`. These samples should work with compatible versions of Pivotal GemFire or Apache Geode. Herein we will refer to the installed IMDG simply as `Geode`.

==== Prerequisites
* A Running Data Flow Shell
include::{docs_dir}/shell.adoc[]
* A Geode installation with a locator and cache server running
include::{docs_dir}/geode-setup.adoc[]

include::local.adoc[]

include::pcf.adoc[]

==== Summary

In this sample, you have learned:

* How to use Spring Cloud Data Flow's `Local` and `Cloud Foundry` servers
* How to use Spring Cloud Data Flow's `shell`
* How to create streaming data pipeline to connect and write to `gemfire`
