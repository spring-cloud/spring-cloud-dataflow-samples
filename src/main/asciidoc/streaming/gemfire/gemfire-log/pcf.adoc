[[gemfire-log-cf]]
===== Using the Cloud Foundry Server
====== Additional Prerequisites
* A Cloud Foundry instance

* The Spring Cloud Data Flow Cloud Foundry Server

include::{docs_dir}/cloudfoundry-server.adoc[]

* Running instance of a `rabbit` service in Cloud Foundry

* Running instance of the https://docs.pivotal.io/p-cloud-cache/1-0/developer.html[Pivotal Cloud Cache for PCF] (PCC) service `cloudcache` in Cloud Foundry.

+
. https://github.com/spring-cloud/spring-cloud-dataflow/blob/master/spring-cloud-dataflow-docs/src/main/asciidoc/streams.adoc#register-a-stream-app[Register] the out-of-the-box applications for the Rabbit binder
+
include::{docs_dir}/maven-access.adoc[]
+
```
dataflow:>app import --uri http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven
```
+
. Get the PCC connection information
+
```
$ cf service-key cloudcache my-service-key
Getting key my-service-key for service instance cloudcache as <user>...

{
 "locators": [
  "10.0.16.9[55221]",
  "10.0.16.11[55221]",
  "10.0.16.10[55221]"
 ],
 "urls": {
  "gfsh": "http://...",
  "pulse": "http://.../pulse"
 },
 "users": [
  {
   "password": <password>,
   "username": "cluster_operator"
  },
  {
   "password": <password>,
   "username": "developer"
  }
 ]
}
```
+
. Using `gfsh`, connect to the PCC instance as `cluster_operator` using the service key values and create the Test region.
+
```
gfsh>connect --use-http --url=<gfsh-url> --user=cluster_operator --password=<cluster_operator_password>
gfsh>create region --name Test --type=REPLICATE
```
+
. Create the stream, connecting to the PCC instance as developer. This example creates an gemfire source to which will publish events on a region
+
```
dataflow stream create --name events --definition " gemfire --username=developer --password=<developer-password> --connect-type=locator --host-addresses=10.0.16.9:55221 --regionName=Test | log" --deploy
```

. Verify the stream is successfully deployed
+
```
dataflow:>stream list
```
+
. Monitor stdout for the log sink
+
```
cf logs <log-sink-app-name>
```
+
. Using `gfsh`, create and update some cache entries
+
```
gfsh>connect --use-http --url=<gfsh-url> --user=cluster_operator --password=<cluster_operator_password>
gfsh>put --region /Test --key 1  --value "value 1"
gfsh>put --region /Test --key 2  --value "value 2"
gfsh>put --region /Test --key 3  --value "value 3"
gfsh>put --region /Test --key 1  --value "new value 1"
```
+
. Observe the log output
+
You should see messages like:
+
```
2017-10-28 17:28:52.893  INFO 18986 --- [emfire.events-1] log sink                               : value 1"
2017-10-28 17:28:52.893  INFO 18986 --- [emfire.events-1] log sink                               : value 2"
2017-10-28 17:28:52.893  INFO 18986 --- [emfire.events-1] log sink                               : value 3"
2017-10-28 17:28:52.893  INFO 18986 --- [emfire.events-1] log sink                               : new value 1"
```
+
By default, the message payload contains the updated value. Depending on your application, you may need additional information. The data comes from https://geode.apache.org/releases/latest/javadoc/org/apache/geode/cache/EntryEvent.html[EntryEvent]. You
can access any fields using the source's `cache-event-expression` property. This takes a SpEL expression bound to the EntryEvent. Try something like `--cache-event-expression='{key:'\+key+',new_value:'\+newValue+'}'` (HINT: You will need to destroy the stream and recreate it to
add this property, an exercise left to the reader). Now you should see log messages like:
+
```
2017-10-28 17:28:52.893  INFO 18986 --- [emfire.events-1] log-sink                                 : {key:1,new_value:value 1}
2017-10-28 17:41:24.466  INFO 18986 --- [emfire.events-1] log-sink                                 : {key:2,new_value:value 2}
```
+
. You're done!
