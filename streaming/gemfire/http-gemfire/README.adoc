:sectnums:

= HTTP to Gemfire Demo

In this demonstration, you will learn how to build a data pipeline using http://cloud.spring.io/spring-cloud-dataflow/[Spring Cloud Data Flow] to consume data from an `http` endpoint and write to Gemfire using the `gemfire` sink.

We will take you through the steps to configure and run Spring Cloud Data Flow server in either a https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started/[local] or https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current/reference/htmlsingle/#getting-started[Cloud Foundry] environment.

NOTE: For legacy reasons the `gemfire` Spring Cloud Stream Apps are named after `Pivotal GemFire`. The code base for the commercial product has since been open sourced as `Apache Geode`. These samples should work with compatible versions of Pivotal GemFire or Apache Geode. Herein we will refer to the installed IMDG simply as `Geode`.

== Prerequisites
* A Running Data Flow Shell

The Spring Cloud Data Flow Shell is available for https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started-deploying-spring-cloud-dataflow[download] or you can https://github.com/spring-cloud/spring-cloud-dataflow[build] it yourself.


NOTE: The Spring Cloud Data Flow Shell is a Spring Boot application that connects to the Data Flow Serverâ€™s REST API and supports a DSL that simplifies the process of defining a stream or task and managing its lifecycle. Most of these samples
use the shell. If you prefer, you can use the Data Flow UI http://localhost:9393/dashboard, (or wherever it the server is hosted) to perform the same operations.

NOTE: the Spring Cloud Data Flow Shell and Local server implementation are in the same repository and are both built by running `./mvnw install` from the project root directory. If you have already run the build, use the jar in `spring-cloud-dataflow-shell/target`

To run the Shell open a new terminal session:
```
$ cd <PATH/TO/SPRING-CLOUD-DATAFLOW-SHELL-JAR>
$ java -jar spring-cloud-dataflow-shell-<VERSION>.jar
  ____                              ____ _                __
 / ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
 \___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
  ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
 |____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
  ____ |_|    _          __|___/                 __________
 |  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
 | | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
 | |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
 |____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/


Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
dataflow:>
```

NOTE: The shell will try to connect to a local server by default. If the Local Dataflow Server is not running you will see:

```
 ____                              ____ _                __
/ ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
\___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
 ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
|____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
 ____ |_|    _          __|___/                 __________
|  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
| | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
| |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
|____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/


Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
server unknown:>
```

Connect the `shell` to the `server` running on , e.g., `http://dataflow-server.app.io`


```
server unknown:>dataflow config server http://dataflow-server.app.io
Successfully targeted http://dataflow-server.app.io
dataflow:>
```
* A Geode installation with a locator and cache server running

If you do not have access an existing Geode installation, install http://geode.apache.org[Apache Geode] or
http://geode.apache.org/[Pivotal Gemfire] and start the `gfsh` CLI in a separate terminal.
```
    _________________________     __
   / _____/ ______/ ______/ /____/ /
  / /  __/ /___  /_____  / _____  /
 / /__/ / ____/  _____/ / /    / /
/______/_/      /______/_/    /_/    1.2.1

Monitor and Manage Apache Geode
gfsh>
```

== Using the Local Server

=== Running the Sample Locally
==== Additional Prerequisites
* A Running Data Flow Server

The Local Data Flow Server is Spring Boot application available for http://cloud.spring.io/spring-cloud-dataflow/[download] or you can https://github.com/spring-cloud/spring-cloud-dataflow[build] it yourself.
If you build it yourself, the executable jar will be in `spring-cloud-dataflow-server-local/target`

To run the Local Data Flow server Open a new terminal session:
```
$cd  <PATH/TO/SPRING-CLOUD-DATAFLOW-LOCAL-JAR>
$java -jar spring-cloud-dataflow-server-local-<VERSION>.jar
```
* A running instance of https://www.rabbitmq.com[Rabbit MQ]

'''

. Use gfsh to start a locator and server
+
```
gfsh>start locator --name=locator1
gfsh>start server --name=server1

```
. Create a region called `Stocks`
+
```
gfsh>create region --name Stocks --type=REPLICATE
```
+
Use the Shell to create the sample stream.

. https://github.com/spring-cloud/spring-cloud-dataflow/blob/master/spring-cloud-dataflow-docs/src/main/asciidoc/streams.adoc#register-a-stream-app[Register] the out-of-the-box applications for the Rabbit binder
+
NOTE: These samples assume that the Data Flow Server can access a remote Maven repository, `https://repo.spring.io/libs-release` by default. If your Data Flow server is running behind a firewall, or you are using a maven proxy preventing
access to public repositories, you will need to install the sample apps in your internal Maven repository and https://docs.spring.io/spring-cloud-dataflow/docs/1.3.0.M2/reference/htmlsingle/#getting-started-maven-configuration[configure]
the server accordingly.  The sample applications are typically registered using Data Flow's bulk import facility. For example, the Shell command `dataflow:>app import --uri http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven` (The actual URI is release and binder specific so refer to the sample instructions for the actual URL).
The bulk import URI references a plain text file containing entries for all of the publicly available Spring Cloud Stream and Task applications published to `https://repo.spring.io`. For example,
`source.http=maven://org.springframework.cloud.stream.app:http-source-rabbit:1.2.0.RELEASE` registers the `http` source app at the corresponding Maven address, relative to the remote repository(ies) configured for the
Data Flow server. The format is `maven://<groupId>:<artifactId>:<version>`  You will need to https://repo.spring.io/libs-release/org/springframework/cloud/stream/app/spring-cloud-stream-app-descriptor/Bacon.RELEASE/spring-cloud-stream-app-descriptor-Bacon.RELEASE.rabbit-apps-maven-repo-url.properties[download] the required apps or https://github.com/spring-cloud-stream-app-starters[build] them and then install them in your Maven repository, using whatever group, artifact, and version you choose. If you do
this, register individual apps using `dataflow:>app register...` using the `maven://` resource URI format corresponding to your installed app.
+
```
dataflow:>app import --uri http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven
```
. Create the stream
+
This example creates an http endpoint to which we will post stock prices as a JSON document containing `symbol` and `price` fields.
The property `--json=true` to enable Geode's JSON support and configures the sink to convert JSON String payloads to https://geode.apache.org/releases/latest/javadoc/org/apache/geode/pdx/PdxInstance.html[PdxInstance], the recommended way
to store JSON documents in Geode. The `keyExpression` property is a SpEL expression used to extract the `symbol` value the PdxInstance to use as an entry key.
+
NOTE: PDX serialization is very efficient and supports OQL queries without requiring a custom domain class.
Use of custom domain types requires these classes to be in the class path of both the stream apps and the cache server.
For this reason, the use of custom payload types is generally discouraged.
+
```
dataflow:>stream create --name stocks --definition "http --port=9090 | gemfire --json=true --regionName=Stocks --keyExpression=payload.getField('symbol')" --deploy
Created and deployed new stream 'stocks'
```
NOTE: If the Geode locator isn't running on default port on `localhost`, add the options `--connect-type=locator --host-addresses=<host>:<port>`. If there are multiple
locators, you can provide a comma separated list of locator addresses. This is not necessary for the sample but is typical for production environments to enable fail-over.

. Verify the stream is successfully deployed
+
```
dataflow:>stream list
```

. Post sample data pointing to the `http` endpoint: `http://localhost:9090` (`9090` is the `port` we specified for the `http` source)
+
```
dataflow:>http post --target http://localhost:9090 --contentType application/json --data '{"symbol":"VMW","price":117.06}'
> POST (application/json) http://localhost:9090 {"symbol":"VMW","price":117.06}
> 202 ACCEPTED
```
+
. Using `gfsh`, connect to the locator if not already connected, and verify the cache entry was created.
+
```
gfsh>get --key='VMW' --region=/Stocks
Result      : true
Key Class   : java.lang.String
Key         : VMW
Value Class : org.apache.geode.pdx.internal.PdxInstanceImpl

symbol | price
------ | ------
VMW    | 117.06
```
+
. You're done!

== Using the Cloud Foundry Server

=== Running the Sample in Cloud Foundry

==== Additional Prerequisites
* The Spring Cloud Data Flow Cloud Foundry Server


The Cloud Foundry Data Flow Server is Spring Boot application available for http://cloud.spring.io/spring-cloud-dataflow/[download] or you can https://github.com/spring-cloud/spring-cloud-dataflow-server-cloudfoundry[build] it yourself.
If you build it yourself, the executable jar will be in `spring-cloud-dataflow-server-cloudfoundry/target`

NOTE: Although you can run the Data Flow Cloud Foundry Server locally and configure it to deploy to any Cloud Foundry space, we will
deploy the server to Cloud Foundry as recommended.

* Running instance of a `rabbit` service in Cloud Foundry

* Running instance of the https://docs.pivotal.io/p-cloud-cache/1-0/developer.html[Pivotal Cloud Cache for PCF] (PCC) service `cloudcache` in Cloud Foundry.

'''

. Verify that CF instance is reachable (Your endpoint urls will be different from what is shown here).
+

```
$ cf api
API endpoint: https://api.system.io (API version: ...)

$ cf apps
Getting apps in org [your-org] / space [your-space] as user...
OK

No apps found
```
. Follow the instructions to deploy the https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current/reference/htmlsingle[Spring Cloud Data Flow Cloud Foundry server]. Don't worry about creating a Redis service. We won't need it. If you are familiar with Cloud Foundry
application manifests, we recommend creating a manifest for the the Data Flow server as shown https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current-SNAPSHOT/reference/htmlsingle/#sample-manifest-template[here].
+
WARNING: As of this writing, there is a typo on the `SPRING_APPLICATION_JSON` entry in the sample manifest. `SPRING_APPLICATION_JSON` must be followed by `:` and The JSON string must be
wrapped in single quotes. Alternatively, you can replace that line with `MAVEN_REMOTE_REPOSITORIES_REPO1_URL: https://repo.spring.io/libs-snapshot`.  If your Cloud Foundry installation is behind a firewall, you may need to install the stream apps used in this sample in your internal Maven repository and https://docs.spring.io/spring-cloud-dataflow/docs/1.3.0.M2/reference/htmlsingle/#getting-started-maven-configuration[configure] the server to access that repository.
. Once you have successfully executed `cf push`, verify the dataflow server is running
+

```
$ cf apps
Getting apps in org [your-org] / space [your-space] as user...
OK

name                 requested state   instances   memory   disk   urls
dataflow-server      started           1/1         1G       1G     dataflow-server.app.io
```

. Notice that the `dataflow-server` application is started and ready for interaction via the url endpoint

. Connect the `shell` with `server` running on Cloud Foundry, e.g., `http://dataflow-server.app.io`
+
```
$ cd <PATH/TO/SPRING-CLOUD-DATAFLOW-SHELL-JAR>
$ java -jar spring-cloud-dataflow-shell-<VERSION>.jar

  ____                              ____ _                __
 / ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
 \___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
  ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
 |____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
  ____ |_|    _          __|___/                 __________
 |  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
 | | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
 | |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
 |____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/


Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
server-unknown:>
```
+
```
server-unknown:>dataflow config server http://dataflow-server.app.io
Successfully targeted http://dataflow-server.app.io
dataflow:>
```
. https://github.com/spring-cloud/spring-cloud-dataflow/blob/master/spring-cloud-dataflow-docs/src/main/asciidoc/streams.adoc#register-a-stream-app[Register] the out-of-the-box applications for the Rabbit binder
+
NOTE: These samples assume that the Data Flow Server can access a remote Maven repository, `https://repo.spring.io/libs-release` by default. If your Data Flow server is running behind a firewall, or you are using a maven proxy preventing
access to public repositories, you will need to install the sample apps in your internal Maven repository and https://docs.spring.io/spring-cloud-dataflow/docs/1.3.0.M2/reference/htmlsingle/#getting-started-maven-configuration[configure]
the server accordingly.  The sample applications are typically registered using Data Flow's bulk import facility. For example, the Shell command `dataflow:>app import --uri http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven` (The actual URI is release and binder specific so refer to the sample instructions for the actual URL).
The bulk import URI references a plain text file containing entries for all of the publicly available Spring Cloud Stream and Task applications published to `https://repo.spring.io`. For example,
`source.http=maven://org.springframework.cloud.stream.app:http-source-rabbit:1.2.0.RELEASE` registers the `http` source app at the corresponding Maven address, relative to the remote repository(ies) configured for the
Data Flow server. The format is `maven://<groupId>:<artifactId>:<version>`  You will need to https://repo.spring.io/libs-release/org/springframework/cloud/stream/app/spring-cloud-stream-app-descriptor/Bacon.RELEASE/spring-cloud-stream-app-descriptor-Bacon.RELEASE.rabbit-apps-maven-repo-url.properties[download] the required apps or https://github.com/spring-cloud-stream-app-starters[build] them and then install them in your Maven repository, using whatever group, artifact, and version you choose. If you do
this, register individual apps using `dataflow:>app register...` using the `maven://` resource URI format corresponding to your installed app.
+
```
dataflow:>app import --uri http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven
```
+
. Get the PCC connection information
+
```
$ cf service-key cloudcache my-service-key
Getting key my-service-key for service instance cloudcache as <user>...

{
 "locators": [
  "10.0.16.9[55221]",
  "10.0.16.11[55221]",
  "10.0.16.10[55221]"
 ],
 "urls": {
  "gfsh": "http://...",
  "pulse": "http://.../pulse"
 },
 "users": [
  {
   "password": <password>,
   "username": "cluster_operator"
  },
  {
   "password": <password>,
   "username": "developer"
  }
 ]
}
```
+
. Using `gfsh`, connect to the PCC instance as `cluster_operator` using the service key values and create the Stocks region.
+
```
gfsh>connect --use-http --url=<gfsh-url> --user=cluster_operator --password=<cluster_operator_password>
gfsh>create region --name Stocks --type=REPLICATE
```
+
. Create the stream, connecting to the PCC instance as developer
+
This example creates an http endpoint to which we will post stock prices as a JSON document containing `symbol` and `price` fields.
The property `--json=true` to enable Geode's JSON support and configures the sink to convert JSON String payloads to https://geode.apache.org/releases/latest/javadoc/org/apache/geode/pdx/PdxInstance.html[PdxInstance], the recommended way
to store JSON documents in Geode. The `keyExpression` property is a SpEL expression used to extract the `symbol` value the PdxInstance to use as an entry key.
+
NOTE: PDX serialization is very efficient and supports OQL queries without requiring a custom domain class.
Use of custom domain types requires these classes to be in the class path of both the stream apps and the cache server.
For this reason, the use of custom payload types is generally discouraged.
+
```
dataflow:>stream create --name stocks --definition "http --security.basic.enabled=false | gemfire --username=developer --password=<developer-password> --connect-type=locator --host-addresses=10.0.16.9:55221 --regionName=Stocks --keyExpression=payload.getField('symbol')" --deploy
```

. Verify the stream is successfully deployed
+
```
dataflow:>stream list
```

. Post sample data pointing to the `http` endpoint
+

Get the url of the http source using `cf apps`
+
```
dataflow:>http post --target http://<http source url> --contentType application/json --data '{"symbol":"VMW","price":117.06}'
> POST (application/json) http://... {"symbol":"VMW","price":117.06}
> 202 ACCEPTED
```
+
. Using `gfsh`, connect to the PCC instance as `cluster_operator` using the service key values.
+
```
gfsh>connect --use-http --url=<gfsh-url> --user=cluster_operator --password=<cluster_operator_password>
gfsh>get --key='VMW' --region=/Stocks
Result      : true
Key Class   : java.lang.String
Key         : VMW
Value Class : org.apache.geode.pdx.internal.PdxInstanceImpl

symbol | price
------ | ------
VMW    | 117.06
```
+
. You're done!

:!sectnums:
== Summary

In this sample, you have learned:

* How to use Spring Cloud Data Flow's `Local` and `Cloud Foundry` servers
* How to use Spring Cloud Data Flow's `shell`
* How to create streaming data pipeline to connect and write to `gemfire`
