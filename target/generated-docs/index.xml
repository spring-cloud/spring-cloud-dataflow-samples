<?xml version="1.0" encoding="UTF-8"?>
<?asciidoc-toc?>
<?asciidoc-numbered?>
<book xmlns="http://docbook.org/ns/docbook" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:lang="en">
<info>
<title>Spring Cloud Data Flow Samples</title>
<date>2017-10-31</date>
<productname>Spring Cloud Data Flow Samples</productname>
<releaseinfo>1.2.0.BUILD-SNAPSHOT</releaseinfo>
<copyright>
	<year>2013-2017</year>
	<holder>Pivotal Software, Inc.</holder>
</copyright>
<legalnotice>
	<para>
		Copies of this document may be made for your own use and for distribution to
		others, provided that you do not charge any fee for such copies and further
		provided that each copy contains this Copyright Notice, whether distributed in
		print or electronically.
	</para>
</legalnotice>
<cover>
	<mediaobject>
		<imageobject>
			<imagedata fileref="images/cover.png">
				<info>
					<othercredit>
						<orgname>Pivotal Software, Inc.</orgname>
					</othercredit>
				</info>
			</imagedata>
		</imageobject>
	</mediaobject>
</cover>
</info>
<chapter xml:id="spring-cloud-data-flow-samples-overview">
<title>Overview</title>
<simpara>This guide contains samples and demonstrations of how to build data microservices applications with <link xlink:href="https://cloud.spring.io/spring-cloud-dataflow/">Spring Cloud Data Flow</link>.</simpara>
</chapter>
<chapter xml:id="_streaming">
<title>Streaming</title>
<section xml:id="spring-cloud-data-flow-samples-cassandra-overview">
<title>Cassandra Samples</title>
<section xml:id="spring-cloud-data-flow-samples-http-cassandra-overview">
<title>HTTP to Cassandra Demo</title>
<simpara>In this demonstration, you will learn how to build a data pipeline using <link xlink:href="http://cloud.spring.io/spring-cloud-dataflow/">Spring Cloud Data Flow</link> to consume data from an <emphasis>HTTP</emphasis> endpoint and write the payload to a <emphasis>Cassandra</emphasis> database.</simpara>
<simpara>We will take you through the steps to configure and Spring Cloud Data Flow server in either a <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started/">local</link> or <link xlink:href="https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current/reference/htmlsingle/#getting-started">Cloud Foundry</link> environment.</simpara>
<section xml:id="_prerequisites">
<title>Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>A Running Data Flow Shell</simpara>
</listitem>
</itemizedlist>
<simpara>The Spring Cloud Data Flow Shell is available for <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started-deploying-spring-cloud-dataflow">download</link> or you can <link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow">build</link> it yourself.</simpara>
<note>
<simpara>the Spring Cloud Data Flow Shell and Local server implementation are in the same repository and are both built by running <literal>./mvnw install</literal> from the project root directory. If you have already run the build, use the jar in <literal>spring-cloud-dataflow-shell/target</literal></simpara>
</note>
<simpara>To run the Shell open a new terminal session:</simpara>
<screen>$ cd &lt;PATH/TO/SPRING-CLOUD-DATAFLOW-SHELL-JAR&gt;
$ java -jar spring-cloud-dataflow-shell-&lt;VERSION&gt;.jar
  ____                              ____ _                __
 / ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
 \___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
  ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
 |____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
  ____ |_|    _          __|___/                 __________
 |  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
 | | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
 | |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
 |____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/


Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
dataflow:&gt;</screen>
<note>
<simpara>The Spring Cloud Data Flow Shell is a Spring Boot application that connects to the Data Flow Server’s REST API and supports a DSL that simplifies the process of defining a stream or task and managing its lifecycle. Most of these samples
use the shell. If you prefer, you can use the Data Flow UI <link xlink:href="http://localhost:9393/dashboard">localhost:9393/dashboard</link>, (or wherever it the server is hosted) to perform the same operations.</simpara>
</note>
</section>
<section xml:id="http-cassandra-local">
<title>Using the Local Server</title>
<section xml:id="_additional_prerequisites">
<title>Additional Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>A running local Data Flow Server</simpara>
</listitem>
</itemizedlist>
<simpara>The Local Data Flow Server is Spring Boot application available for <link xlink:href="http://cloud.spring.io/spring-cloud-dataflow/">download</link> or you can <link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow">build</link> it yourself.
If you build it yourself, the executable jar will be in <literal>spring-cloud-dataflow-server-local/target</literal></simpara>
<simpara>To run the Local Data Flow server Open a new terminal session:</simpara>
<screen>$cd  &lt;PATH/TO/SPRING-CLOUD-DATAFLOW-LOCAL-JAR&gt;
$java -jar spring-cloud-dataflow-server-local-&lt;VERSION&gt;.jar</screen>
<itemizedlist>
<listitem>
<simpara>Running instance of <link xlink:href="http://kafka.apache.org/downloads.html">Kafka</link></simpara>
</listitem>
<listitem>
<simpara>Running instance of <link xlink:href="http://cassandra.apache.org/">Apache Cassandra</link></simpara>
</listitem>
<listitem>
<simpara>A database utility tool such as <link xlink:href="http://dbeaver.jkiss.org/">DBeaver</link> to connect to the Cassandra instance. You might have to provide <literal>host</literal>, <literal>port</literal>, <literal>username</literal> and <literal>password</literal> depending on the Cassandra configuration you are using.</simpara>
</listitem>
<listitem>
<simpara>Create a keyspace and a <literal>book</literal> table in Cassandra using:</simpara>
<screen>CREATE KEYSPACE clouddata WITH REPLICATION = { 'class' : 'org.apache.cassandra.locator.SimpleStrategy', 'replication_factor': '1' } AND DURABLE_WRITES = true;
USE clouddata;
CREATE TABLE book  (
    id          uuid PRIMARY KEY,
    isbn        text,
    author      text,
    title       text
);</screen>
<orderedlist numeration="arabic">
<listitem>
<simpara><link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow/blob/master/spring-cloud-dataflow-docs/src/main/asciidoc/streams.adoc#register-a-stream-app">Register</link> the out-of-the-box applications for the Kafka binder</simpara>
<note>
<simpara>These samples assume that the Data Flow Server can access a remote Maven repository, <literal><link xlink:href="https://repo.spring.io/libs-release">repo.spring.io/libs-release</link></literal> by default. If your Data Flow server is running behind a firewall, or you are using a maven proxy preventing
access to public repositories, you will need to install the sample apps in your internal Maven repository and <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started-maven-configuration">configure</link>
the server accordingly.  The sample applications are typically registered using Data Flow&#8217;s bulk import facility. For example, the Shell command <literal>dataflow:&gt;app import --uri <link xlink:href="http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven">bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</link></literal> (The actual URI is release and binder specific so refer to the sample instructions for the actual URL).
The bulk import URI references a plain text file containing entries for all of the publicly available Spring Cloud Stream and Task applications published to <literal><link xlink:href="https://repo.spring.io">repo.spring.io</link></literal>. For example,
<literal>source.http=maven://org.springframework.cloud.stream.app:http-source-rabbit:1.2.0.RELEASE</literal> registers the <literal>http</literal> source app at the corresponding Maven address, relative to the remote repository(ies) configured for the
Data Flow server. The format is <literal>maven://&lt;groupId&gt;:&lt;artifactId&gt;:&lt;version&gt;</literal>  You will need to <link xlink:href="https://repo.spring.io/libs-release/org/springframework/cloud/stream/app/spring-cloud-stream-app-descriptor/Bacon.RELEASE/spring-cloud-stream-app-descriptor-Bacon.RELEASE.rabbit-apps-maven-repo-url.properties">download</link> the required apps or <link xlink:href="https://github.com/spring-cloud-stream-app-starters">build</link> them and then install them in your Maven repository, using whatever group, artifact, and version you choose. If you do
this, register individual apps using <literal>dataflow:&gt;app register&#8230;&#8203;</literal> using the <literal>maven://</literal> resource URI format corresponding to your installed app.</simpara>
</note>
<screen>dataflow:&gt;app import --uri http://bit.ly/Bacon-RELEASE-stream-applications-kafka-10-maven</screen>
</listitem>
<listitem>
<simpara>Create the stream</simpara>
<screen>dataflow:&gt;stream create cassandrastream --definition "http --server.port=8888 --spring.cloud.stream.bindings.output.contentType='application/json' | cassandra --ingestQuery='insert into book (id, isbn, title, author) values (uuid(), ?, ?, ?)' --keyspace=clouddata" --deploy

Created and deployed new stream 'cassandrastream'</screen>
<note>
<simpara>If Cassandra isn&#8217;t running on default port on <literal>localhost</literal> or if you need username and password to connect, use one of the following options to specify the necessary connection parameters: <literal>--username='&lt;USERNAME&gt;' --password='&lt;PASSWORD&gt;' --port=&lt;PORT&gt; --contact-points=&lt;LIST-OF-HOSTS&gt;</literal></simpara>
</note>
</listitem>
<listitem>
<simpara>Verify the stream is successfully deployed</simpara>
<screen>dataflow:&gt;stream list</screen>
</listitem>
<listitem>
<simpara>Notice that <literal>cassandrastream-http</literal> and <literal>cassandrastream-cassandra</literal> <link xlink:href="https://github.com/spring-cloud-stream-app-starters//">Spring Cloud Stream</link> applications are running as Spring Boot applications within the <literal>server</literal> as a collocated process.</simpara>
<screen>2015-12-15 15:52:31.576  INFO 18337 --- [nio-9393-exec-1] o.s.c.d.a.s.l.OutOfProcessModuleDeployer : deploying module org.springframework.cloud.stream.module:cassandra-sink:jar:exec:1.0.0.BUILD-SNAPSHOT instance 0
   Logs will be in /var/folders/c3/ctx7_rns6x30tq7rb76wzqwr0000gp/T/spring-cloud-data-flow-284240942697761420/cassandrastream.cassandra
2015-12-15 15:52:31.583  INFO 18337 --- [nio-9393-exec-1] o.s.c.d.a.s.l.OutOfProcessModuleDeployer : deploying module org.springframework.cloud.stream.module:http-source:jar:exec:1.0.0.BUILD-SNAPSHOT instance 0
   Logs will be in /var/folders/c3/ctx7_rns6x30tq7rb76wzqwr0000gp/T/spring-cloud-data-flow-284240942697761420/cassandrastream.http</screen>
</listitem>
<listitem>
<simpara>Post sample data pointing to the <literal>http</literal> endpoint: <literal><link xlink:href="http://localhost:8888">localhost:8888</link></literal> (<literal>8888</literal> is the <literal>server.port</literal> we specified for the <literal>http</literal> source in this case)</simpara>
<screen>dataflow:&gt;http post --contentType 'application/json' --data '{"isbn": "1599869772", "title": "The Art of War", "author": "Sun Tzu"}' --target http://localhost:8888
&gt; POST (application/json;charset=UTF-8) http://localhost:8888 {"isbn": "1599869772", "title": "The Art of War", "author": "Sun Tzu"}
&gt; 202 ACCEPTED</screen>
</listitem>
<listitem>
<simpara>Connect to the Cassandra instance and query the table <literal>clouddata.book</literal> to list the persisted records</simpara>
<screen>select * from clouddata.book;</screen>
</listitem>
<listitem>
<simpara>You&#8217;re done!</simpara>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="http-cassandra-cf">
<title>Using Cloud Foundry Server</title>
<section xml:id="_additional_prerequisites_2">
<title>Additional Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>Cloud Foundry instance</simpara>
</listitem>
<listitem>
<simpara>The Spring Cloud Data Flow Cloud Foundry Server</simpara>
</listitem>
</itemizedlist>
<simpara>The Cloud Foundry Data Flow Server is Spring Boot application available for <link xlink:href="http://cloud.spring.io/spring-cloud-dataflow/">download</link> or you can <link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow-server-cloudfoundry">build</link> it yourself.
If you build it yourself, the executable jar will be in <literal>spring-cloud-dataflow-server-cloudfoundry/target</literal></simpara>
<note>
<simpara>Although you can run the Data Flow Cloud Foundry Server locally and configure it to deploy to any Cloud Foundry instance, we will
deploy the server to Cloud Foundry as recommended.</simpara>
</note>
<orderedlist numeration="arabic">
<listitem>
<simpara>Verify that CF instance is reachable (Your endpoint urls will be different from what is shown here).</simpara>
<screen>$ cf api
API endpoint: https://api.system.io (API version: ...)

$ cf apps
Getting apps in org [your-org] / space [your-space] as user...
OK

No apps found</screen>
</listitem>
<listitem>
<simpara>Follow the instructions to deploy the <link xlink:href="https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current/reference/htmlsingle">Spring Cloud Data Flow Cloud Foundry server</link>. Don&#8217;t worry about creating a Redis service. We won&#8217;t need it. If you are familiar with Cloud Foundry
application manifests, we recommend creating a manifest for the the Data Flow server as shown <link xlink:href="https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current/reference/htmlsingle/#sample-manifest-template">here</link>.</simpara>
<warning>
<simpara>As of this writing, there is a typo on the <literal>SPRING_APPLICATION_JSON</literal> entry in the sample manifest. <literal>SPRING_APPLICATION_JSON</literal> must be followed by <literal>:</literal> and The JSON string must be
wrapped in single quotes. Alternatively, you can replace that line with <literal>MAVEN_REMOTE_REPOSITORIES_REPO1_URL: <link xlink:href="https://repo.spring.io/libs-snapshot">repo.spring.io/libs-snapshot</link></literal>.  If your Cloud Foundry installation is behind a firewall, you may need to install the stream apps used in this sample in your internal Maven repository and <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/1.3.0.M2/reference/htmlsingle/#getting-started-maven-configuration">configure</link> the server to access that repository.</simpara>
</warning>
</listitem>
<listitem>
<simpara>Once you have successfully executed <literal>cf push</literal>, verify the dataflow server is running</simpara>
<screen>$ cf apps
Getting apps in org [your-org] / space [your-space] as user...
OK

name                 requested state   instances   memory   disk   urls
dataflow-server      started           1/1         1G       1G     dataflow-server.app.io</screen>
</listitem>
<listitem>
<simpara>Notice that the <literal>dataflow-server</literal> application is started and ready for interaction via the url endpoint</simpara>
</listitem>
<listitem>
<simpara>Connect the <literal>shell</literal> with <literal>server</literal> running on Cloud Foundry, e.g., <literal><link xlink:href="http://dataflow-server.app.io">dataflow-server.app.io</link></literal></simpara>
<screen>$ cd &lt;PATH/TO/SPRING-CLOUD-DATAFLOW-SHELL-JAR&gt;
$ java -jar spring-cloud-dataflow-shell-&lt;VERSION&gt;.jar

  ____                              ____ _                __
 / ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
 \___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
  ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
 |____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
  ____ |_|    _          __|___/                 __________
 |  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
 | | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
 | |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
 |____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/


Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
server-unknown:&gt;</screen>
<screen>server-unknown:&gt;dataflow config server http://dataflow-server.app.io
Successfully targeted http://dataflow-server.app.io
dataflow:&gt;</screen>
<itemizedlist>
<listitem>
<simpara>Running instance of <literal>cassandra</literal> in Cloud Foundry or from another Cloud provider</simpara>
</listitem>
<listitem>
<simpara>A database utility tool such as <link xlink:href="http://dbeaver.jkiss.org/">DBeaver</link> to connect to the Cassandra instance. You might have to provide <literal>host</literal>, <literal>port</literal>, <literal>username</literal> and <literal>password</literal> depending on the Cassandra configuration you are using.</simpara>
</listitem>
<listitem>
<simpara>Create a <literal>book</literal> table in your Cassandra keyspace using:</simpara>
<screen>CREATE TABLE book  (
    id          uuid PRIMARY KEY,
    isbn        text,
    author      text,
    title       text
);</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow/blob/master/spring-cloud-dataflow-docs/src/main/asciidoc/streams.adoc#register-a-stream-app">Register</link> the out-of-the-box applications for the Rabbit binder</simpara>
<note>
<simpara>These samples assume that the Data Flow Server can access a remote Maven repository, <literal><link xlink:href="https://repo.spring.io/libs-release">repo.spring.io/libs-release</link></literal> by default. If your Data Flow server is running behind a firewall, or you are using a maven proxy preventing
access to public repositories, you will need to install the sample apps in your internal Maven repository and <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started-maven-configuration">configure</link>
the server accordingly.  The sample applications are typically registered using Data Flow&#8217;s bulk import facility. For example, the Shell command <literal>dataflow:&gt;app import --uri <link xlink:href="http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven">bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</link></literal> (The actual URI is release and binder specific so refer to the sample instructions for the actual URL).
The bulk import URI references a plain text file containing entries for all of the publicly available Spring Cloud Stream and Task applications published to <literal><link xlink:href="https://repo.spring.io">repo.spring.io</link></literal>. For example,
<literal>source.http=maven://org.springframework.cloud.stream.app:http-source-rabbit:1.2.0.RELEASE</literal> registers the <literal>http</literal> source app at the corresponding Maven address, relative to the remote repository(ies) configured for the
Data Flow server. The format is <literal>maven://&lt;groupId&gt;:&lt;artifactId&gt;:&lt;version&gt;</literal>  You will need to <link xlink:href="https://repo.spring.io/libs-release/org/springframework/cloud/stream/app/spring-cloud-stream-app-descriptor/Bacon.RELEASE/spring-cloud-stream-app-descriptor-Bacon.RELEASE.rabbit-apps-maven-repo-url.properties">download</link> the required apps or <link xlink:href="https://github.com/spring-cloud-stream-app-starters">build</link> them and then install them in your Maven repository, using whatever group, artifact, and version you choose. If you do
this, register individual apps using <literal>dataflow:&gt;app register&#8230;&#8203;</literal> using the <literal>maven://</literal> resource URI format corresponding to your installed app.</simpara>
</note>
<screen>dataflow:&gt;app import --uri http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</screen>
</listitem>
<listitem>
<simpara>Create the stream</simpara>
<screen>dataflow:&gt;stream create cassandrastream --definition "http --spring.cloud.stream.bindings.output.contentType='application/json' | cassandra --ingestQuery='insert into book (id, isbn, title, author) values (uuid(), ?, ?, ?)' --username='&lt;USERNAME&gt;' --password='&lt;PASSWORD&gt;' --port=&lt;PORT&gt; --contact-points=&lt;HOST&gt; --keyspace='&lt;KEYSPACE&gt;'" --deploy

Created and deployed new stream 'cassandrastream'</screen>
</listitem>
<listitem>
<simpara>Verify the stream is successfully deployed</simpara>
<screen>dataflow:&gt;stream list</screen>
</listitem>
<listitem>
<simpara>Notice that <literal>cassandrastream-http</literal> and <literal>cassandrastream-cassandra</literal> <link xlink:href="https://github.com/spring-cloud-stream-app-starters/">Spring Cloud Stream</link> applications are running as <emphasis>cloud-native</emphasis> (microservice) applications in Cloud Foundry</simpara>
<screen>$ cf apps
Getting apps in org [your-org] / space [your-space] as user...
OK

name                        requested state   instances   memory   disk   urls
cassandrastream-cassandra   started           1/1         1G       1G     cassandrastream-cassandra.app.io
cassandrastream-http        started           1/1         1G       1G     cassandrastream-http.app.io
dataflow-server             started           1/1         1G       1G     dataflow-server.app.io</screen>
</listitem>
<listitem>
<simpara>Lookup the <literal>url</literal> for <literal>cassandrastream-http</literal> application from the list above. Post sample data pointing to the <literal>http</literal> endpoint: <literal>&lt;YOUR-cassandrastream-http-APP-URL&gt;</literal></simpara>
<screen>http post --contentType 'application/json' --data '{"isbn": "1599869772", "title": "The Art of War", "author": "Sun Tzu"}' --target http://&lt;YOUR-cassandrastream-http-APP-URL&gt;
&gt; POST (application/json;charset=UTF-8) http://cassandrastream-http.app.io {"isbn": "1599869772", "title": "The Art of War", "author": "Sun Tzu"}
&gt; 202 ACCEPTED</screen>
</listitem>
<listitem>
<simpara>Connect to the Cassandra instance and query the table <literal>book</literal> to list the data inserted</simpara>
<screen>select * from book;</screen>
</listitem>
<listitem>
<simpara>Now, let&#8217;s try to take advantage of Pivotal Cloud Foundry&#8217;s platform capability. Let&#8217;s scale the <literal>cassandrastream-http</literal> application from 1 to 3 instances</simpara>
<screen>$ cf scale cassandrastream-http -i 3
Scaling app cassandrastream-http in org user-dataflow / space development as user...
OK</screen>
</listitem>
<listitem>
<simpara>Verify App instances (3/3) running successfully</simpara>
<screen>$ cf apps
Getting apps in org user-dataflow / space development as user...
OK

name                        requested state   instances   memory   disk   urls
cassandrastream-cassandra   started           1/1         1G       1G     cassandrastream-cassandra.app.io
cassandrastream-http        started           3/3         1G       1G     cassandrastream-http.app.io
dataflow-server             started           1/1         1G       1G     dataflow-server.app.io</screen>
</listitem>
<listitem>
<simpara>You&#8217;re done!</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="_summary">
<title>Summary</title>
<simpara>In this sample, you have learned:</simpara>
<itemizedlist>
<listitem>
<simpara>How to use Spring Cloud Data Flow&#8217;s <literal>Local</literal> and <literal>Cloud Foundry</literal> servers</simpara>
</listitem>
<listitem>
<simpara>How to use Spring Cloud Data Flow&#8217;s <literal>shell</literal></simpara>
</listitem>
<listitem>
<simpara>How to create streaming data pipeline to connect and write to <literal>Cassandra</literal></simpara>
</listitem>
<listitem>
<simpara>How to scale data microservice applications on <literal>Pivotal Cloud Foundry</literal></simpara>
</listitem>
</itemizedlist>
</section>
</section>
</section>
<section xml:id="spring-cloud-data-flow-samples-jdbc-overview">
<title>JDBC Samples</title>
<section xml:id="_http_to_mysql_demo">
<title>HTTP to MySQL Demo</title>
<simpara>In this demonstration, you will learn how to build a data pipeline using <link xlink:href="http://cloud.spring.io/spring-cloud-dataflow/">Spring Cloud Data Flow</link> to consume data from an <literal>http</literal> endpoint and write to MySQL database using <literal>jdbc</literal> sink.</simpara>
<simpara>We will take you through the steps to configure and Spring Cloud Data Flow server in either a <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started/">local</link> or <link xlink:href="https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current/reference/htmlsingle/#getting-started">Cloud Foundry</link> environment.</simpara>
<section xml:id="_prerequisites_2">
<title>Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>A Running Data Flow Shell</simpara>
</listitem>
</itemizedlist>
<simpara>The Spring Cloud Data Flow Shell is available for <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started-deploying-spring-cloud-dataflow">download</link> or you can <link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow">build</link> it yourself.</simpara>
<note>
<simpara>the Spring Cloud Data Flow Shell and Local server implementation are in the same repository and are both built by running <literal>./mvnw install</literal> from the project root directory. If you have already run the build, use the jar in <literal>spring-cloud-dataflow-shell/target</literal></simpara>
</note>
<simpara>To run the Shell open a new terminal session:</simpara>
<screen>$ cd &lt;PATH/TO/SPRING-CLOUD-DATAFLOW-SHELL-JAR&gt;
$ java -jar spring-cloud-dataflow-shell-&lt;VERSION&gt;.jar
  ____                              ____ _                __
 / ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
 \___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
  ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
 |____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
  ____ |_|    _          __|___/                 __________
 |  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
 | | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
 | |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
 |____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/


Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
dataflow:&gt;</screen>
<note>
<simpara>The Spring Cloud Data Flow Shell is a Spring Boot application that connects to the Data Flow Server’s REST API and supports a DSL that simplifies the process of defining a stream or task and managing its lifecycle. Most of these samples
use the shell. If you prefer, you can use the Data Flow UI <link xlink:href="http://localhost:9393/dashboard">localhost:9393/dashboard</link>, (or wherever it the server is hosted) to perform the same operations.</simpara>
</note>
</section>
<section xml:id="_using_local_server">
<title>Using Local Server</title>
<section xml:id="_additional_prerequisites_3">
<title>Additional Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>A running local Data Flow Server</simpara>
</listitem>
</itemizedlist>
<simpara>The Local Data Flow Server is Spring Boot application available for <link xlink:href="http://cloud.spring.io/spring-cloud-dataflow/">download</link> or you can <link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow">build</link> it yourself.
If you build it yourself, the executable jar will be in <literal>spring-cloud-dataflow-server-local/target</literal></simpara>
<simpara>To run the Local Data Flow server Open a new terminal session:</simpara>
<screen>$cd  &lt;PATH/TO/SPRING-CLOUD-DATAFLOW-LOCAL-JAR&gt;
$java -jar spring-cloud-dataflow-server-local-&lt;VERSION&gt;.jar</screen>
<itemizedlist>
<listitem>
<simpara>Running instance of <link xlink:href="http://kafka.apache.org/downloads.html">Kafka</link></simpara>
</listitem>
<listitem>
<simpara>Running instance of <link xlink:href="http://www.mysql.com/">MySQL</link></simpara>
</listitem>
<listitem>
<simpara>A database utility tool such as <link xlink:href="http://dbeaver.jkiss.org/">DBeaver</link> or <link xlink:href="https://www.dbvis.com/">DbVisualizer</link></simpara>
</listitem>
<listitem>
<simpara>Create the <literal>test</literal> database with a <literal>names</literal> table (in MySQL) using:</simpara>
<screen>CREATE DATABASE test;
USE test;
CREATE TABLE names
(
	name varchar(255)
);</screen>
<orderedlist numeration="arabic">
<listitem>
<simpara><link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow/blob/master/spring-cloud-dataflow-docs/src/main/asciidoc/streams.adoc#register-a-stream-app">Register</link> the out-of-the-box applications for the Kafka binder</simpara>
<note>
<simpara>These samples assume that the Data Flow Server can access a remote Maven repository, <literal><link xlink:href="https://repo.spring.io/libs-release">repo.spring.io/libs-release</link></literal> by default. If your Data Flow server is running behind a firewall, or you are using a maven proxy preventing
access to public repositories, you will need to install the sample apps in your internal Maven repository and <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started-maven-configuration">configure</link>
the server accordingly.  The sample applications are typically registered using Data Flow&#8217;s bulk import facility. For example, the Shell command <literal>dataflow:&gt;app import --uri <link xlink:href="http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven">bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</link></literal> (The actual URI is release and binder specific so refer to the sample instructions for the actual URL).
The bulk import URI references a plain text file containing entries for all of the publicly available Spring Cloud Stream and Task applications published to <literal><link xlink:href="https://repo.spring.io">repo.spring.io</link></literal>. For example,
<literal>source.http=maven://org.springframework.cloud.stream.app:http-source-rabbit:1.2.0.RELEASE</literal> registers the <literal>http</literal> source app at the corresponding Maven address, relative to the remote repository(ies) configured for the
Data Flow server. The format is <literal>maven://&lt;groupId&gt;:&lt;artifactId&gt;:&lt;version&gt;</literal>  You will need to <link xlink:href="https://repo.spring.io/libs-release/org/springframework/cloud/stream/app/spring-cloud-stream-app-descriptor/Bacon.RELEASE/spring-cloud-stream-app-descriptor-Bacon.RELEASE.rabbit-apps-maven-repo-url.properties">download</link> the required apps or <link xlink:href="https://github.com/spring-cloud-stream-app-starters">build</link> them and then install them in your Maven repository, using whatever group, artifact, and version you choose. If you do
this, register individual apps using <literal>dataflow:&gt;app register&#8230;&#8203;</literal> using the <literal>maven://</literal> resource URI format corresponding to your installed app.</simpara>
</note>
<screen>dataflow:&gt;app import --uri http://bit.ly/Bacon-RELEASE-stream-applications-kafka-10-maven</screen>
</listitem>
<listitem>
<simpara>Create the stream</simpara>
<screen>dataflow:&gt;stream create --name mysqlstream --definition "http --server.port=8787 | jdbc --tableName=names --columns=name --spring.datasource.driver-class-name=org.mariadb.jdbc.Driver --spring.datasource.url='jdbc:mysql://localhost:3306/test'" --deploy

Created and deployed new stream 'mysqlstream'</screen>
<note>
<simpara>If MySQL isn&#8217;t running on default port on <literal>localhost</literal> or if you need username and password to connect, use one of the following options to specify the necessary connection parameters: <literal>--spring.datasource.url='jdbc:mysql://&lt;HOST&gt;:&lt;PORT&gt;/&lt;NAME&gt;' --spring.datasource.username=&lt;USERNAME&gt; --spring.datasource.password=&lt;PASSWORD&gt;</literal></simpara>
</note>
</listitem>
<listitem>
<simpara>Verify the stream is successfully deployed</simpara>
<screen>dataflow:&gt;stream list</screen>
</listitem>
<listitem>
<simpara>Notice that <literal>mysqlstream-http</literal> and <literal>mysqlstream-jdbc</literal> <link xlink:href="https://github.com/spring-cloud-stream-app-starters//">Spring Cloud Stream</link> applications are running as Spring Boot applications within the Local <literal>server</literal> as collocated processes.</simpara>
<screen>2016-05-03 09:29:55.918  INFO 65162 --- [nio-9393-exec-3] o.s.c.d.spi.local.LocalAppDeployer       : deploying app mysqlstream.jdbc instance 0
   Logs will be in /var/folders/c3/ctx7_rns6x30tq7rb76wzqwr0000gp/T/spring-cloud-dataflow-6850863945840320040/mysqlstream1-1462292995903/mysqlstream.jdbc
2016-05-03 09:29:55.939  INFO 65162 --- [nio-9393-exec-3] o.s.c.d.spi.local.LocalAppDeployer       : deploying app mysqlstream.http instance 0
   Logs will be in /var/folders/c3/ctx7_rns6x30tq7rb76wzqwr0000gp/T/spring-cloud-dataflow-6850863945840320040/mysqlstream-1462292995934/mysqlstream.http</screen>
</listitem>
<listitem>
<simpara>Post sample data pointing to the <literal>http</literal> endpoint: <literal><link xlink:href="http://localhost:8787">localhost:8787</link></literal> [<literal>8787</literal> is the <literal>server.port</literal> we specified for the <literal>http</literal> source in this case]</simpara>
</listitem>
</orderedlist>
<screen>dataflow:&gt;http post --contentType 'application/json' --target http://localhost:8787 --data "{\"name\": \"Foo\"}"
&gt; POST (application/json;charset=UTF-8) http://localhost:8787 {"name": "Spring Boot"}
&gt; 202 ACCEPTED</screen>
<simpara>+</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Connect to the MySQL instance and query the table <literal>test.names</literal> to list the new rows:</simpara>
<screen>select * from test.names;</screen>
</listitem>
<listitem>
<simpara>You&#8217;re done!</simpara>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="_using_cloud_foundry_server">
<title>Using Cloud Foundry Server</title>
<section xml:id="_additional_prerequisites_4">
<title>Additional Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>Cloud Foundry instance</simpara>
</listitem>
<listitem>
<simpara>The Spring Cloud Data Flow Cloud Foundry Server</simpara>
</listitem>
</itemizedlist>
<simpara>The Cloud Foundry Data Flow Server is Spring Boot application available for <link xlink:href="http://cloud.spring.io/spring-cloud-dataflow/">download</link> or you can <link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow-server-cloudfoundry">build</link> it yourself.
If you build it yourself, the executable jar will be in <literal>spring-cloud-dataflow-server-cloudfoundry/target</literal></simpara>
<note>
<simpara>Although you can run the Data Flow Cloud Foundry Server locally and configure it to deploy to any Cloud Foundry instance, we will
deploy the server to Cloud Foundry as recommended.</simpara>
</note>
<orderedlist numeration="arabic">
<listitem>
<simpara>Verify that CF instance is reachable (Your endpoint urls will be different from what is shown here).</simpara>
<screen>$ cf api
API endpoint: https://api.system.io (API version: ...)

$ cf apps
Getting apps in org [your-org] / space [your-space] as user...
OK

No apps found</screen>
</listitem>
<listitem>
<simpara>Follow the instructions to deploy the <link xlink:href="https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current/reference/htmlsingle">Spring Cloud Data Flow Cloud Foundry server</link>. Don&#8217;t worry about creating a Redis service. We won&#8217;t need it. If you are familiar with Cloud Foundry
application manifests, we recommend creating a manifest for the the Data Flow server as shown <link xlink:href="https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current/reference/htmlsingle/#sample-manifest-template">here</link>.</simpara>
<warning>
<simpara>As of this writing, there is a typo on the <literal>SPRING_APPLICATION_JSON</literal> entry in the sample manifest. <literal>SPRING_APPLICATION_JSON</literal> must be followed by <literal>:</literal> and The JSON string must be
wrapped in single quotes. Alternatively, you can replace that line with <literal>MAVEN_REMOTE_REPOSITORIES_REPO1_URL: <link xlink:href="https://repo.spring.io/libs-snapshot">repo.spring.io/libs-snapshot</link></literal>.  If your Cloud Foundry installation is behind a firewall, you may need to install the stream apps used in this sample in your internal Maven repository and <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/1.3.0.M2/reference/htmlsingle/#getting-started-maven-configuration">configure</link> the server to access that repository.</simpara>
</warning>
</listitem>
<listitem>
<simpara>Once you have successfully executed <literal>cf push</literal>, verify the dataflow server is running</simpara>
<screen>$ cf apps
Getting apps in org [your-org] / space [your-space] as user...
OK

name                 requested state   instances   memory   disk   urls
dataflow-server      started           1/1         1G       1G     dataflow-server.app.io</screen>
</listitem>
<listitem>
<simpara>Notice that the <literal>dataflow-server</literal> application is started and ready for interaction via the url endpoint</simpara>
</listitem>
<listitem>
<simpara>Connect the <literal>shell</literal> with <literal>server</literal> running on Cloud Foundry, e.g., <literal><link xlink:href="http://dataflow-server.app.io">dataflow-server.app.io</link></literal></simpara>
<screen>$ cd &lt;PATH/TO/SPRING-CLOUD-DATAFLOW-SHELL-JAR&gt;
$ java -jar spring-cloud-dataflow-shell-&lt;VERSION&gt;.jar

  ____                              ____ _                __
 / ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
 \___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
  ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
 |____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
  ____ |_|    _          __|___/                 __________
 |  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
 | | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
 | |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
 |____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/


Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
server-unknown:&gt;</screen>
<screen>server-unknown:&gt;dataflow config server http://dataflow-server.app.io
Successfully targeted http://dataflow-server.app.io
dataflow:&gt;</screen>
<itemizedlist>
<listitem>
<simpara>Running instance of <literal>rabbit</literal> in Cloud Foundry</simpara>
</listitem>
<listitem>
<simpara>Running instance of <literal>mysql</literal> in Cloud Foundry</simpara>
</listitem>
<listitem>
<simpara>A database utility tool such as <link xlink:href="http://dbeaver.jkiss.org/">DBeaver</link> or <link xlink:href="https://www.dbvis.com/">DbVisualizer</link></simpara>
</listitem>
<listitem>
<simpara>Create the <literal>names</literal> table (in MySQL) using:</simpara>
<screen>CREATE TABLE names
(
	name varchar(255)
);</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow/blob/master/spring-cloud-dataflow-docs/src/main/asciidoc/streams.adoc#register-a-stream-app">Register</link> the out-of-the-box applications for the Rabbit binder</simpara>
<note>
<simpara>These samples assume that the Data Flow Server can access a remote Maven repository, <literal><link xlink:href="https://repo.spring.io/libs-release">repo.spring.io/libs-release</link></literal> by default. If your Data Flow server is running behind a firewall, or you are using a maven proxy preventing
access to public repositories, you will need to install the sample apps in your internal Maven repository and <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started-maven-configuration">configure</link>
the server accordingly.  The sample applications are typically registered using Data Flow&#8217;s bulk import facility. For example, the Shell command <literal>dataflow:&gt;app import --uri <link xlink:href="http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven">bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</link></literal> (The actual URI is release and binder specific so refer to the sample instructions for the actual URL).
The bulk import URI references a plain text file containing entries for all of the publicly available Spring Cloud Stream and Task applications published to <literal><link xlink:href="https://repo.spring.io">repo.spring.io</link></literal>. For example,
<literal>source.http=maven://org.springframework.cloud.stream.app:http-source-rabbit:1.2.0.RELEASE</literal> registers the <literal>http</literal> source app at the corresponding Maven address, relative to the remote repository(ies) configured for the
Data Flow server. The format is <literal>maven://&lt;groupId&gt;:&lt;artifactId&gt;:&lt;version&gt;</literal>  You will need to <link xlink:href="https://repo.spring.io/libs-release/org/springframework/cloud/stream/app/spring-cloud-stream-app-descriptor/Bacon.RELEASE/spring-cloud-stream-app-descriptor-Bacon.RELEASE.rabbit-apps-maven-repo-url.properties">download</link> the required apps or <link xlink:href="https://github.com/spring-cloud-stream-app-starters">build</link> them and then install them in your Maven repository, using whatever group, artifact, and version you choose. If you do
this, register individual apps using <literal>dataflow:&gt;app register&#8230;&#8203;</literal> using the <literal>maven://</literal> resource URI format corresponding to your installed app.</simpara>
</note>
<screen>dataflow:&gt;app import --uri http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</screen>
</listitem>
<listitem>
<simpara>Create the stream</simpara>
<screen>dataflow:&gt;stream create --name mysqlstream --definition "http | jdbc --tableName=names --columns=name"
Created new stream 'mysqlstream'

dataflow:&gt;stream deploy --name mysqlstream --properties "app.jdbc.spring.cloud.deployer.cloudfoundry.services=mysql"
Deployed stream 'mysqlstream'</screen>
<note>
<simpara>By supplying  the <literal>app.jdbc.spring.cloud.deployer.cloudfoundry.services=mysql</literal> property, we are deploying the stream with <literal>jdbc-sink</literal> to automatically bind to <literal>mysql</literal> service and only this application in the stream gets the service binding. This also eliminates the requirement to supply <literal>datasource</literal> credentials in stream definition.</simpara>
</note>
</listitem>
<listitem>
<simpara>Verify the stream is successfully deployed</simpara>
<screen>dataflow:&gt;stream list</screen>
</listitem>
<listitem>
<simpara>Notice that <literal>mysqlstream-http</literal> and <literal>mysqlstream-jdbc</literal> <link xlink:href="https://github.com/spring-cloud-stream-app-starters/">Spring Cloud Stream</link> applications are running as <emphasis>cloud-native</emphasis> (microservice) applications in Cloud Foundry</simpara>
<screen>$ cf apps
Getting apps in org user-dataflow / space development as user...
OK

name                        requested state   instances   memory   disk   urls
mysqlstream-http            started           1/1         1G       1G     mysqlstream-http.app.io
mysqlstream-jdbc            started           1/1         1G       1G     mysqlstream-jdbc.app.io
dataflow-server             started           1/1         1G       1G     dataflow-server.app.io</screen>
</listitem>
<listitem>
<simpara>Lookup the <literal>url</literal> for <literal>mysqlstream-http</literal> application from the list above. Post sample data pointing to the <literal>http</literal> endpoint: <literal>&lt;YOUR-mysqlstream-http-APP-URL&gt;</literal></simpara>
<screen>http post --contentType 'application/json' --target http://mysqlstream-http.app.io --data "{\"name\": \"Bar"}"
&gt; POST (application/json;charset=UTF-8) http://mysqlstream-http.app.io {"name": "Bar"}
&gt; 202 ACCEPTED</screen>
</listitem>
<listitem>
<simpara>Connect to the MySQL instance and query the table <literal>names</literal> to list the new rows:</simpara>
<screen>select * from names;</screen>
</listitem>
<listitem>
<simpara>Now, let&#8217;s take advantage of Pivotal Cloud Foundry&#8217;s platform capability. Let&#8217;s scale the <literal>mysqlstream-http</literal> application from 1 to 3 instances</simpara>
<screen>$ cf scale mysqlstream-http -i 3
Scaling app mysqlstream-http in org user-dataflow / space development as user...
OK</screen>
</listitem>
<listitem>
<simpara>Verify App instances (3/3) running successfully</simpara>
<screen>$ cf apps
Getting apps in org user-dataflow / space development as user...
OK

name                        requested state   instances   memory   disk   urls
mysqlstream-http            started           3/3         1G       1G     mysqlstream-http.app.io
mysqlstream-jdbc            started           1/1         1G       1G     mysqlstream-jdbc.app.io
dataflow-server             started           1/1         1G       1G     dataflow-server.app.io</screen>
</listitem>
<listitem>
<simpara>You&#8217;re done!</simpara>
</listitem>
</orderedlist>
</section>
</section>
</section>
<section xml:id="_summary_2">
<title>Summary</title>
<simpara>In this sample, you have learned:</simpara>
<itemizedlist>
<listitem>
<simpara>How to use Spring Cloud Data Flow&#8217;s <literal>Local</literal> and <literal>Cloud Foundry</literal> servers</simpara>
</listitem>
<listitem>
<simpara>How to use Spring Cloud Data Flow&#8217;s <literal>shell</literal></simpara>
</listitem>
<listitem>
<simpara>How to create streaming data pipeline to connect and write to <literal>MySQL</literal></simpara>
</listitem>
<listitem>
<simpara>How to scale data microservice applications on <literal>Pivotal Cloud Foundry</literal></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="spring-cloud-data-flow-samples-gemfire-overview">
<title>GemFire Samples</title>
<section xml:id="spring-cloud-data-flow-samples-gemfire-http-overview">
<title>HTTP to Gemfire Demo</title>
<simpara>In this demonstration, you will learn how to build a data pipeline using <link xlink:href="http://cloud.spring.io/spring-cloud-dataflow/">Spring Cloud Data Flow</link> to consume data from an <literal>http</literal> endpoint and write to Gemfire using the <literal>gemfire</literal> sink.</simpara>
<simpara>We will take you through the steps to configure and run Spring Cloud Data Flow server in either a <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started/">local</link> or <link xlink:href="https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current/reference/htmlsingle/#getting-started">Cloud Foundry</link> environment.</simpara>
<note>
<simpara>For legacy reasons the <literal>gemfire</literal> Spring Cloud Stream Apps are named after <literal>Pivotal GemFire</literal>. The code base for the commercial product has since been open sourced as <literal>Apache Geode</literal>. These samples should work with compatible versions of Pivotal GemFire or Apache Geode. Herein we will refer to the installed IMDG simply as <literal>Geode</literal>.</simpara>
</note>
<section xml:id="_prerequisites_3">
<title>Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>A Running Data Flow Shell</simpara>
</listitem>
</itemizedlist>
<simpara>The Spring Cloud Data Flow Shell is available for <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started-deploying-spring-cloud-dataflow">download</link> or you can <link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow">build</link> it yourself.</simpara>
<note>
<simpara>the Spring Cloud Data Flow Shell and Local server implementation are in the same repository and are both built by running <literal>./mvnw install</literal> from the project root directory. If you have already run the build, use the jar in <literal>spring-cloud-dataflow-shell/target</literal></simpara>
</note>
<simpara>To run the Shell open a new terminal session:</simpara>
<screen>$ cd &lt;PATH/TO/SPRING-CLOUD-DATAFLOW-SHELL-JAR&gt;
$ java -jar spring-cloud-dataflow-shell-&lt;VERSION&gt;.jar
  ____                              ____ _                __
 / ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
 \___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
  ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
 |____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
  ____ |_|    _          __|___/                 __________
 |  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
 | | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
 | |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
 |____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/


Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
dataflow:&gt;</screen>
<note>
<simpara>The Spring Cloud Data Flow Shell is a Spring Boot application that connects to the Data Flow Server’s REST API and supports a DSL that simplifies the process of defining a stream or task and managing its lifecycle. Most of these samples
use the shell. If you prefer, you can use the Data Flow UI <link xlink:href="http://localhost:9393/dashboard">localhost:9393/dashboard</link>, (or wherever it the server is hosted) to perform the same operations.</simpara>
</note>
<itemizedlist>
<listitem>
<simpara>A Geode installation with a locator and cache server running
Unresolved directive in streaming/gemfire/gemfire-http/overview.adoc - include::geode-setup.adoc[]</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="gemfire-http-local">
<title>Using the Local Server</title>
<section xml:id="_additional_prerequisites_5">
<title>Additional Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>A running local Data Flow Server</simpara>
</listitem>
</itemizedlist>
<simpara>The Local Data Flow Server is Spring Boot application available for <link xlink:href="http://cloud.spring.io/spring-cloud-dataflow/">download</link> or you can <link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow">build</link> it yourself.
If you build it yourself, the executable jar will be in <literal>spring-cloud-dataflow-server-local/target</literal></simpara>
<simpara>To run the Local Data Flow server Open a new terminal session:</simpara>
<screen>$cd  &lt;PATH/TO/SPRING-CLOUD-DATAFLOW-LOCAL-JAR&gt;
$java -jar spring-cloud-dataflow-server-local-&lt;VERSION&gt;.jar</screen>
<itemizedlist>
<listitem>
<simpara>A running instance of <link xlink:href="https://www.rabbitmq.com">Rabbit MQ</link></simpara>
</listitem>
</itemizedlist>
<simpara><?asciidoc-hr?></simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Use gfsh to start a locator and server</simpara>
<screen>gfsh&gt;start locator --name=locator1
gfsh&gt;start server --name=server1</screen>
</listitem>
<listitem>
<simpara>Create a region called <literal>Stocks</literal></simpara>
<screen>gfsh&gt;create region --name Stocks --type=REPLICATE</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="_use_the_shell_to_create_the_sample_stream">
<title>Use the Shell to create the sample stream</title>
<orderedlist numeration="arabic">
<listitem>
<simpara><link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow/blob/master/spring-cloud-dataflow-docs/src/main/asciidoc/streams.adoc#register-a-stream-app">Register</link> the out-of-the-box applications for the Rabbit binder</simpara>
<note>
<simpara>These samples assume that the Data Flow Server can access a remote Maven repository, <literal><link xlink:href="https://repo.spring.io/libs-release">repo.spring.io/libs-release</link></literal> by default. If your Data Flow server is running behind a firewall, or you are using a maven proxy preventing
access to public repositories, you will need to install the sample apps in your internal Maven repository and <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started-maven-configuration">configure</link>
the server accordingly.  The sample applications are typically registered using Data Flow&#8217;s bulk import facility. For example, the Shell command <literal>dataflow:&gt;app import --uri <link xlink:href="http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven">bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</link></literal> (The actual URI is release and binder specific so refer to the sample instructions for the actual URL).
The bulk import URI references a plain text file containing entries for all of the publicly available Spring Cloud Stream and Task applications published to <literal><link xlink:href="https://repo.spring.io">repo.spring.io</link></literal>. For example,
<literal>source.http=maven://org.springframework.cloud.stream.app:http-source-rabbit:1.2.0.RELEASE</literal> registers the <literal>http</literal> source app at the corresponding Maven address, relative to the remote repository(ies) configured for the
Data Flow server. The format is <literal>maven://&lt;groupId&gt;:&lt;artifactId&gt;:&lt;version&gt;</literal>  You will need to <link xlink:href="https://repo.spring.io/libs-release/org/springframework/cloud/stream/app/spring-cloud-stream-app-descriptor/Bacon.RELEASE/spring-cloud-stream-app-descriptor-Bacon.RELEASE.rabbit-apps-maven-repo-url.properties">download</link> the required apps or <link xlink:href="https://github.com/spring-cloud-stream-app-starters">build</link> them and then install them in your Maven repository, using whatever group, artifact, and version you choose. If you do
this, register individual apps using <literal>dataflow:&gt;app register&#8230;&#8203;</literal> using the <literal>maven://</literal> resource URI format corresponding to your installed app.</simpara>
</note>
<screen>dataflow:&gt;app import --uri http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</screen>
</listitem>
<listitem>
<simpara>Create the stream</simpara>
<simpara>This example creates an http endpoint to which we will post stock prices as a JSON document containing <literal>symbol</literal> and <literal>price</literal> fields.
The property <literal>--json=true</literal> to enable Geode&#8217;s JSON support and configures the sink to convert JSON String payloads to <link xlink:href="https://geode.apache.org/releases/latest/javadoc/org/apache/geode/pdx/PdxInstance.html">PdxInstance</link>, the recommended way
to store JSON documents in Geode. The <literal>keyExpression</literal> property is a SpEL expression used to extract the <literal>symbol</literal> value the PdxInstance to use as an entry key.</simpara>
<note>
<simpara>PDX serialization is very efficient and supports OQL queries without requiring a custom domain class.
Use of custom domain types requires these classes to be in the class path of both the stream apps and the cache server.
For this reason, the use of custom payload types is generally discouraged.</simpara>
</note>
<screen>dataflow:&gt;stream create --name stocks --definition "http --port=9090 | gemfire --json=true --regionName=Stocks --keyExpression=payload.getField('symbol')" --deploy
Created and deployed new stream 'stocks'</screen>
<note>
<simpara>If the Geode locator isn&#8217;t running on default port on <literal>localhost</literal>, add the options <literal>--connect-type=locator --host-addresses=&lt;host&gt;:&lt;port&gt;</literal>. If there are multiple
locators, you can provide a comma separated list of locator addresses. This is not necessary for the sample but is typical for production environments to enable fail-over.</simpara>
</note>
</listitem>
<listitem>
<simpara>Verify the stream is successfully deployed</simpara>
<screen>dataflow:&gt;stream list</screen>
</listitem>
<listitem>
<simpara>Post sample data pointing to the <literal>http</literal> endpoint: <literal><link xlink:href="http://localhost:9090">localhost:9090</link></literal> (<literal>9090</literal> is the <literal>port</literal> we specified for the <literal>http</literal> source)</simpara>
<screen>dataflow:&gt;http post --target http://localhost:9090 --contentType application/json --data '{"symbol":"VMW","price":117.06}'
&gt; POST (application/json) http://localhost:9090 {"symbol":"VMW","price":117.06}
&gt; 202 ACCEPTED</screen>
</listitem>
<listitem>
<simpara>Using <literal>gfsh</literal>, connect to the locator if not already connected, and verify the cache entry was created.</simpara>
<screen>gfsh&gt;get --key='VMW' --region=/Stocks
Result      : true
Key Class   : java.lang.String
Key         : VMW
Value Class : org.apache.geode.pdx.internal.PdxInstanceImpl

symbol | price
------ | ------
VMW    | 117.06</screen>
</listitem>
<listitem>
<simpara>You&#8217;re done!</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="gemfire-http-cf">
<title>Using the Cloud Foundry Server</title>
<section xml:id="_additional_prerequisites_6">
<title>Additional Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>A Cloud Foundry instance</simpara>
</listitem>
<listitem>
<simpara>Cloud Data Flow Cloud Foundry Server</simpara>
</listitem>
</itemizedlist>
<simpara>The Cloud Foundry Data Flow Server is Spring Boot application available for <link xlink:href="http://cloud.spring.io/spring-cloud-dataflow/">download</link> or you can <link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow-server-cloudfoundry">build</link> it yourself.
If you build it yourself, the executable jar will be in <literal>spring-cloud-dataflow-server-cloudfoundry/target</literal></simpara>
<note>
<simpara>Although you can run the Data Flow Cloud Foundry Server locally and configure it to deploy to any Cloud Foundry instance, we will
deploy the server to Cloud Foundry as recommended.</simpara>
</note>
<orderedlist numeration="arabic">
<listitem>
<simpara>Verify that CF instance is reachable (Your endpoint urls will be different from what is shown here).</simpara>
<screen>$ cf api
API endpoint: https://api.system.io (API version: ...)

$ cf apps
Getting apps in org [your-org] / space [your-space] as user...
OK

No apps found</screen>
</listitem>
<listitem>
<simpara>Follow the instructions to deploy the <link xlink:href="https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current/reference/htmlsingle">Spring Cloud Data Flow Cloud Foundry server</link>. Don&#8217;t worry about creating a Redis service. We won&#8217;t need it. If you are familiar with Cloud Foundry
application manifests, we recommend creating a manifest for the the Data Flow server as shown <link xlink:href="https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current/reference/htmlsingle/#sample-manifest-template">here</link>.</simpara>
<warning>
<simpara>As of this writing, there is a typo on the <literal>SPRING_APPLICATION_JSON</literal> entry in the sample manifest. <literal>SPRING_APPLICATION_JSON</literal> must be followed by <literal>:</literal> and The JSON string must be
wrapped in single quotes. Alternatively, you can replace that line with <literal>MAVEN_REMOTE_REPOSITORIES_REPO1_URL: <link xlink:href="https://repo.spring.io/libs-snapshot">repo.spring.io/libs-snapshot</link></literal>.  If your Cloud Foundry installation is behind a firewall, you may need to install the stream apps used in this sample in your internal Maven repository and <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/1.3.0.M2/reference/htmlsingle/#getting-started-maven-configuration">configure</link> the server to access that repository.</simpara>
</warning>
</listitem>
<listitem>
<simpara>Once you have successfully executed <literal>cf push</literal>, verify the dataflow server is running</simpara>
<screen>$ cf apps
Getting apps in org [your-org] / space [your-space] as user...
OK

name                 requested state   instances   memory   disk   urls
dataflow-server      started           1/1         1G       1G     dataflow-server.app.io</screen>
</listitem>
<listitem>
<simpara>Notice that the <literal>dataflow-server</literal> application is started and ready for interaction via the url endpoint</simpara>
</listitem>
<listitem>
<simpara>Connect the <literal>shell</literal> with <literal>server</literal> running on Cloud Foundry, e.g., <literal><link xlink:href="http://dataflow-server.app.io">dataflow-server.app.io</link></literal></simpara>
<screen>$ cd &lt;PATH/TO/SPRING-CLOUD-DATAFLOW-SHELL-JAR&gt;
$ java -jar spring-cloud-dataflow-shell-&lt;VERSION&gt;.jar

  ____                              ____ _                __
 / ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
 \___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
  ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
 |____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
  ____ |_|    _          __|___/                 __________
 |  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
 | | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
 | |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
 |____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/


Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
server-unknown:&gt;</screen>
<screen>server-unknown:&gt;dataflow config server http://dataflow-server.app.io
Successfully targeted http://dataflow-server.app.io
dataflow:&gt;</screen>
<itemizedlist>
<listitem>
<simpara>Running instance of a <literal>rabbit</literal> service in Cloud Foundry</simpara>
</listitem>
<listitem>
<simpara>Running instance of the <link xlink:href="https://docs.pivotal.io/p-cloud-cache/1-0/developer.html">Pivotal Cloud Cache for PCF</link> (PCC) service <literal>cloudcache</literal> in Cloud Foundry.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow/blob/master/spring-cloud-dataflow-docs/src/main/asciidoc/streams.adoc#register-a-stream-app">Register</link> the out-of-the-box applications for the Rabbit binder</simpara>
<note>
<simpara>These samples assume that the Data Flow Server can access a remote Maven repository, <literal><link xlink:href="https://repo.spring.io/libs-release">repo.spring.io/libs-release</link></literal> by default. If your Data Flow server is running behind a firewall, or you are using a maven proxy preventing
access to public repositories, you will need to install the sample apps in your internal Maven repository and <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started-maven-configuration">configure</link>
the server accordingly.  The sample applications are typically registered using Data Flow&#8217;s bulk import facility. For example, the Shell command <literal>dataflow:&gt;app import --uri <link xlink:href="http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven">bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</link></literal> (The actual URI is release and binder specific so refer to the sample instructions for the actual URL).
The bulk import URI references a plain text file containing entries for all of the publicly available Spring Cloud Stream and Task applications published to <literal><link xlink:href="https://repo.spring.io">repo.spring.io</link></literal>. For example,
<literal>source.http=maven://org.springframework.cloud.stream.app:http-source-rabbit:1.2.0.RELEASE</literal> registers the <literal>http</literal> source app at the corresponding Maven address, relative to the remote repository(ies) configured for the
Data Flow server. The format is <literal>maven://&lt;groupId&gt;:&lt;artifactId&gt;:&lt;version&gt;</literal>  You will need to <link xlink:href="https://repo.spring.io/libs-release/org/springframework/cloud/stream/app/spring-cloud-stream-app-descriptor/Bacon.RELEASE/spring-cloud-stream-app-descriptor-Bacon.RELEASE.rabbit-apps-maven-repo-url.properties">download</link> the required apps or <link xlink:href="https://github.com/spring-cloud-stream-app-starters">build</link> them and then install them in your Maven repository, using whatever group, artifact, and version you choose. If you do
this, register individual apps using <literal>dataflow:&gt;app register&#8230;&#8203;</literal> using the <literal>maven://</literal> resource URI format corresponding to your installed app.</simpara>
</note>
<screen>dataflow:&gt;app import --uri http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</screen>
</listitem>
<listitem>
<simpara>Get the PCC connection information</simpara>
<screen>$ cf service-key cloudcache my-service-key
Getting key my-service-key for service instance cloudcache as &lt;user&gt;...

{
 "locators": [
  "10.0.16.9[55221]",
  "10.0.16.11[55221]",
  "10.0.16.10[55221]"
 ],
 "urls": {
  "gfsh": "http://...",
  "pulse": "http://.../pulse"
 },
 "users": [
  {
   "password": &lt;password&gt;,
   "username": "cluster_operator"
  },
  {
   "password": &lt;password&gt;,
   "username": "developer"
  }
 ]
}</screen>
</listitem>
<listitem>
<simpara>Using <literal>gfsh</literal>, connect to the PCC instance as <literal>cluster_operator</literal> using the service key values and create the Stocks region.</simpara>
<screen>gfsh&gt;connect --use-http --url=&lt;gfsh-url&gt; --user=cluster_operator --password=&lt;cluster_operator_password&gt;
gfsh&gt;create region --name Stocks --type=REPLICATE</screen>
</listitem>
<listitem>
<simpara>Create the stream, connecting to the PCC instance as developer</simpara>
<simpara>This example creates an http endpoint to which we will post stock prices as a JSON document containing <literal>symbol</literal> and <literal>price</literal> fields.
The property <literal>--json=true</literal> to enable Geode&#8217;s JSON support and configures the sink to convert JSON String payloads to <link xlink:href="https://geode.apache.org/releases/latest/javadoc/org/apache/geode/pdx/PdxInstance.html">PdxInstance</link>, the recommended way
to store JSON documents in Geode. The <literal>keyExpression</literal> property is a SpEL expression used to extract the <literal>symbol</literal> value the PdxInstance to use as an entry key.</simpara>
<note>
<simpara>PDX serialization is very efficient and supports OQL queries without requiring a custom domain class.
Use of custom domain types requires these classes to be in the class path of both the stream apps and the cache server.
For this reason, the use of custom payload types is generally discouraged.</simpara>
</note>
<screen>dataflow:&gt;stream create --name stocks --definition "http --security.basic.enabled=false | gemfire --username=developer --password=&lt;developer-password&gt; --connect-type=locator --host-addresses=10.0.16.9:55221 --regionName=Stocks --keyExpression=payload.getField('symbol')" --deploy</screen>
</listitem>
<listitem>
<simpara>Verify the stream is successfully deployed</simpara>
<screen>dataflow:&gt;stream list</screen>
</listitem>
<listitem>
<simpara>Post sample data pointing to the <literal>http</literal> endpoint</simpara>
<simpara>Get the url of the http source using <literal>cf apps</literal></simpara>
<screen>dataflow:&gt;http post --target http://&lt;http source url&gt; --contentType application/json --data '{"symbol":"VMW","price":117.06}'
&gt; POST (application/json) http://... {"symbol":"VMW","price":117.06}
&gt; 202 ACCEPTED</screen>
</listitem>
<listitem>
<simpara>Using <literal>gfsh</literal>, connect to the PCC instance as <literal>cluster_operator</literal> using the service key values.</simpara>
<screen>gfsh&gt;connect --use-http --url=&lt;gfsh-url&gt; --user=cluster_operator --password=&lt;cluster_operator_password&gt;
gfsh&gt;get --key='VMW' --region=/Stocks
Result      : true
Key Class   : java.lang.String
Key         : VMW
Value Class : org.apache.geode.pdx.internal.PdxInstanceImpl

symbol | price
------ | ------
VMW    | 117.06</screen>
</listitem>
<listitem>
<simpara>You&#8217;re done!</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="_summary_3">
<title>Summary</title>
<simpara>In this sample, you have learned:</simpara>
<itemizedlist>
<listitem>
<simpara>How to use Spring Cloud Data Flow&#8217;s <literal>Local</literal> and <literal>Cloud Foundry</literal> servers</simpara>
</listitem>
<listitem>
<simpara>How to use Spring Cloud Data Flow&#8217;s <literal>shell</literal></simpara>
</listitem>
<listitem>
<simpara>How to create streaming data pipeline to connect and write to <literal>gemfire</literal></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="spring-cloud-data-flow-samples-gemfire-log-overview">
<title>Gemfire to Log Demo</title>
<simpara>In this demonstration, you will learn how to build a data pipeline using <link xlink:href="http://cloud.spring.io/spring-cloud-dataflow/">Spring Cloud Data Flow</link> to consume data from a <literal>gemfire</literal> endpoint and write to a log using the <literal>log</literal> sink.
The <literal>gemfire</literal> source creates a <link xlink:href="https://geode.apache.org/releases/latest/javadoc/org/apache/geode/cache/CacheListener.html">CacheListener</link> to monitor events for a region and publish a message whenever an entry is changed.</simpara>
<simpara>We will take you through the steps to configure and run Spring Cloud Data Flow server in either a <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started/">local</link> or <link xlink:href="https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current/reference/htmlsingle/#getting-started">Cloud Foundry</link> environment.</simpara>
<note>
<simpara>For legacy reasons the <literal>gemfire</literal> Spring Cloud Stream Apps are named after <literal>Pivotal GemFire</literal>. The code base for the commercial product has since been open sourced as <literal>Apache Geode</literal>. These samples should work with compatible versions of Pivotal GemFire or Apache Geode. Herein we will refer to the installed IMDG simply as <literal>Geode</literal>.</simpara>
</note>
<section xml:id="_prerequisites_4">
<title>Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>A Running Data Flow Shell</simpara>
</listitem>
</itemizedlist>
<simpara>The Spring Cloud Data Flow Shell is available for <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started-deploying-spring-cloud-dataflow">download</link> or you can <link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow">build</link> it yourself.</simpara>
<note>
<simpara>the Spring Cloud Data Flow Shell and Local server implementation are in the same repository and are both built by running <literal>./mvnw install</literal> from the project root directory. If you have already run the build, use the jar in <literal>spring-cloud-dataflow-shell/target</literal></simpara>
</note>
<simpara>To run the Shell open a new terminal session:</simpara>
<screen>$ cd &lt;PATH/TO/SPRING-CLOUD-DATAFLOW-SHELL-JAR&gt;
$ java -jar spring-cloud-dataflow-shell-&lt;VERSION&gt;.jar
  ____                              ____ _                __
 / ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
 \___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
  ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
 |____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
  ____ |_|    _          __|___/                 __________
 |  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
 | | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
 | |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
 |____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/


Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
dataflow:&gt;</screen>
<note>
<simpara>The Spring Cloud Data Flow Shell is a Spring Boot application that connects to the Data Flow Server’s REST API and supports a DSL that simplifies the process of defining a stream or task and managing its lifecycle. Most of these samples
use the shell. If you prefer, you can use the Data Flow UI <link xlink:href="http://localhost:9393/dashboard">localhost:9393/dashboard</link>, (or wherever it the server is hosted) to perform the same operations.</simpara>
</note>
<itemizedlist>
<listitem>
<simpara>A Geode installation with a locator and cache server running
Unresolved directive in streaming/gemfire/gemfire-log/overview.adoc - include::geode-setup.adoc[]</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="gemfire-log-local">
<title>Using the Local Server</title>
<section xml:id="_additional_prerequisites_7">
<title>Additional Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>A Running Data Flow Server</simpara>
</listitem>
</itemizedlist>
<simpara>The Local Data Flow Server is Spring Boot application available for <link xlink:href="http://cloud.spring.io/spring-cloud-dataflow/">download</link> or you can <link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow">build</link> it yourself.
If you build it yourself, the executable jar will be in <literal>spring-cloud-dataflow-server-local/target</literal></simpara>
<simpara>To run the Local Data Flow server Open a new terminal session:</simpara>
<screen>$cd  &lt;PATH/TO/SPRING-CLOUD-DATAFLOW-LOCAL-JAR&gt;
$java -jar spring-cloud-dataflow-server-local-&lt;VERSION&gt;.jar</screen>
<itemizedlist>
<listitem>
<simpara>A running instance of <link xlink:href="https://www.rabbitmq.com">Rabbit MQ</link></simpara>
</listitem>
</itemizedlist>
<simpara><?asciidoc-hr?></simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Use gfsh to start a locator and server</simpara>
<screen>gfsh&gt;start locator --name=locator1
gfsh&gt;start server --name=server1</screen>
</listitem>
<listitem>
<simpara>Create a region called <literal>Test</literal></simpara>
<screen>gfsh&gt;create region --name Test --type=REPLICATE</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="_use_the_shell_to_create_the_sample_stream_2">
<title>Use the Shell to create the sample stream.</title>
<orderedlist numeration="arabic">
<listitem>
<simpara><link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow/blob/master/spring-cloud-dataflow-docs/src/main/asciidoc/streams.adoc#register-a-stream-app">Register</link> the out-of-the-box applications for the Rabbit binder</simpara>
<note>
<simpara>These samples assume that the Data Flow Server can access a remote Maven repository, <literal><link xlink:href="https://repo.spring.io/libs-release">repo.spring.io/libs-release</link></literal> by default. If your Data Flow server is running behind a firewall, or you are using a maven proxy preventing
access to public repositories, you will need to install the sample apps in your internal Maven repository and <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started-maven-configuration">configure</link>
the server accordingly.  The sample applications are typically registered using Data Flow&#8217;s bulk import facility. For example, the Shell command <literal>dataflow:&gt;app import --uri <link xlink:href="http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven">bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</link></literal> (The actual URI is release and binder specific so refer to the sample instructions for the actual URL).
The bulk import URI references a plain text file containing entries for all of the publicly available Spring Cloud Stream and Task applications published to <literal><link xlink:href="https://repo.spring.io">repo.spring.io</link></literal>. For example,
<literal>source.http=maven://org.springframework.cloud.stream.app:http-source-rabbit:1.2.0.RELEASE</literal> registers the <literal>http</literal> source app at the corresponding Maven address, relative to the remote repository(ies) configured for the
Data Flow server. The format is <literal>maven://&lt;groupId&gt;:&lt;artifactId&gt;:&lt;version&gt;</literal>  You will need to <link xlink:href="https://repo.spring.io/libs-release/org/springframework/cloud/stream/app/spring-cloud-stream-app-descriptor/Bacon.RELEASE/spring-cloud-stream-app-descriptor-Bacon.RELEASE.rabbit-apps-maven-repo-url.properties">download</link> the required apps or <link xlink:href="https://github.com/spring-cloud-stream-app-starters">build</link> them and then install them in your Maven repository, using whatever group, artifact, and version you choose. If you do
this, register individual apps using <literal>dataflow:&gt;app register&#8230;&#8203;</literal> using the <literal>maven://</literal> resource URI format corresponding to your installed app.</simpara>
</note>
<screen>dataflow:&gt;app import --uri http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</screen>
</listitem>
<listitem>
<simpara>Create the stream</simpara>
<simpara>This example creates an gemfire source to which will publish events on a region</simpara>
<screen>dataflow:&gt;stream create --name events --definition " gemfire --regionName=Test | log" --deploy
Created and deployed new stream 'events'</screen>
<note>
<simpara>If the Geode locator isn&#8217;t running on default port on <literal>localhost</literal>, add the options <literal>--connect-type=locator --host-addresses=&lt;host&gt;:&lt;port&gt;</literal>. If there are multiple
locators, you can provide a comma separated list of locator addresses. This is not necessary for the sample but is typical for production environments to enable fail-over.</simpara>
</note>
</listitem>
<listitem>
<simpara>Verify the stream is successfully deployed</simpara>
<screen>dataflow:&gt;stream list</screen>
</listitem>
<listitem>
<simpara>Monitor stdout for the log sink. When you deploy the stream, you will see log messages in the Data Flow server console like this</simpara>
<screen>2017-10-28 17:28:23.275  INFO 15603 --- [nio-9393-exec-2] o.s.c.d.spi.local.LocalAppDeployer       : Deploying app with deploymentId events.log instance 0.
   Logs will be in /var/folders/hd/5yqz2v2d3sxd3n879f4sg4gr0000gn/T/spring-cloud-dataflow-4093992067314402881/events-1509226103269/events.log
2017-10-28 17:28:23.277  INFO 15603 --- [nio-9393-exec-2] o.s.c.d.s.c.StreamDeploymentController   : Downloading resource URI [maven://org.springframework.cloud.stream.app:gemfire-source-rabbit:1.2.0.RELEASE]
2017-10-28 17:28:23.311  INFO 15603 --- [nio-9393-exec-2] o.s.c.d.s.c.StreamDeploymentController   : Deploying application named [gemfire] as part of stream named [events] with resource URI [maven://org.springframework.cloud.stream.app:gemfire-source-rabbit:1.2.0.RELEASE]
2017-10-28 17:28:23.318  INFO 15603 --- [nio-9393-exec-2] o.s.c.d.spi.local.LocalAppDeployer       : Deploying app with deploymentId events.gemfire instance 0.
   Logs will be in /var/folders/hd/5yqz2v2d3sxd3n879f4sg4gr0000gn/T/spring-cloud-dataflow-4093992067314402881/events-1509226103311/events.gemfire</screen>
<simpara>Copy the location of the <literal>log</literal> sink logs. This is a directory that ends in <literal>events.log</literal>. The log files will be in <literal>stdout_0.log</literal> under this directory. You can monitor the output of the log sink using <literal>tail</literal>, or something similar:</simpara>
<screen>$tail -f /var/folders/hd/5yqz2v2d3sxd3n879f4sg4gr0000gn/T/spring-cloud-dataflow-4093992067314402881/events-1509226103269/events.log/stdout_0.log</screen>
</listitem>
<listitem>
<simpara>Using <literal>gfsh</literal>, create and update some cache entries</simpara>
<screen>gfsh&gt;put --region /Test --key 1  --value "value 1"
gfsh&gt;put --region /Test --key 2  --value "value 2"
gfsh&gt;put --region /Test --key 3  --value "value 3"
gfsh&gt;put --region /Test --key 1  --value "new value 1"</screen>
</listitem>
<listitem>
<simpara>Observe the log output
You should see messages like:</simpara>
<screen>2017-10-28 17:28:52.893  INFO 18986 --- [emfire.events-1] log sink                               : value 1"
2017-10-28 17:28:52.893  INFO 18986 --- [emfire.events-1] log sink                               : value 2"
2017-10-28 17:28:52.893  INFO 18986 --- [emfire.events-1] log sink                               : value 3"
2017-10-28 17:28:52.893  INFO 18986 --- [emfire.events-1] log sink                               : new value 1"</screen>
<simpara>By default, the message payload contains the updated value. Depending on your application, you may need additional information. The data comes from <link xlink:href="https://geode.apache.org/releases/latest/javadoc/org/apache/geode/cache/EntryEvent.html">EntryEvent</link>. You
can access any fields using the source&#8217;s <literal>cache-event-expression</literal> property. This takes a SpEL expression bound to the EntryEvent. Try something like <literal>--cache-event-expression='{key:'+key+',new_value:'+newValue+'}'</literal> (HINT: You will need to destroy the stream and recreate it to
add this property, an exercise left to the reader). Now you should see log messages like:</simpara>
<screen>2017-10-28 17:28:52.893  INFO 18986 --- [emfire.events-1] log-sink                                 : {key:1,new_value:value 1}
2017-10-28 17:41:24.466  INFO 18986 --- [emfire.events-1] log-sink                                 : {key:2,new_value:value 2}</screen>
</listitem>
<listitem>
<simpara>You&#8217;re done!</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="gemfire-log-cf">
<title>Using the Cloud Foundry Server</title>
<section xml:id="_additional_prerequisites_8">
<title>Additional Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>A Cloud Foundry instance</simpara>
</listitem>
<listitem>
<simpara>The Spring Cloud Data Flow Cloud Foundry Server</simpara>
</listitem>
</itemizedlist>
<simpara>The Cloud Foundry Data Flow Server is Spring Boot application available for <link xlink:href="http://cloud.spring.io/spring-cloud-dataflow/">download</link> or you can <link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow-server-cloudfoundry">build</link> it yourself.
If you build it yourself, the executable jar will be in <literal>spring-cloud-dataflow-server-cloudfoundry/target</literal></simpara>
<note>
<simpara>Although you can run the Data Flow Cloud Foundry Server locally and configure it to deploy to any Cloud Foundry instance, we will
deploy the server to Cloud Foundry as recommended.</simpara>
</note>
<orderedlist numeration="arabic">
<listitem>
<simpara>Verify that CF instance is reachable (Your endpoint urls will be different from what is shown here).</simpara>
<screen>$ cf api
API endpoint: https://api.system.io (API version: ...)

$ cf apps
Getting apps in org [your-org] / space [your-space] as user...
OK

No apps found</screen>
</listitem>
<listitem>
<simpara>Follow the instructions to deploy the <link xlink:href="https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current/reference/htmlsingle">Spring Cloud Data Flow Cloud Foundry server</link>. Don&#8217;t worry about creating a Redis service. We won&#8217;t need it. If you are familiar with Cloud Foundry
application manifests, we recommend creating a manifest for the the Data Flow server as shown <link xlink:href="https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current/reference/htmlsingle/#sample-manifest-template">here</link>.</simpara>
<warning>
<simpara>As of this writing, there is a typo on the <literal>SPRING_APPLICATION_JSON</literal> entry in the sample manifest. <literal>SPRING_APPLICATION_JSON</literal> must be followed by <literal>:</literal> and The JSON string must be
wrapped in single quotes. Alternatively, you can replace that line with <literal>MAVEN_REMOTE_REPOSITORIES_REPO1_URL: <link xlink:href="https://repo.spring.io/libs-snapshot">repo.spring.io/libs-snapshot</link></literal>.  If your Cloud Foundry installation is behind a firewall, you may need to install the stream apps used in this sample in your internal Maven repository and <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/1.3.0.M2/reference/htmlsingle/#getting-started-maven-configuration">configure</link> the server to access that repository.</simpara>
</warning>
</listitem>
<listitem>
<simpara>Once you have successfully executed <literal>cf push</literal>, verify the dataflow server is running</simpara>
<screen>$ cf apps
Getting apps in org [your-org] / space [your-space] as user...
OK

name                 requested state   instances   memory   disk   urls
dataflow-server      started           1/1         1G       1G     dataflow-server.app.io</screen>
</listitem>
<listitem>
<simpara>Notice that the <literal>dataflow-server</literal> application is started and ready for interaction via the url endpoint</simpara>
</listitem>
<listitem>
<simpara>Connect the <literal>shell</literal> with <literal>server</literal> running on Cloud Foundry, e.g., <literal><link xlink:href="http://dataflow-server.app.io">dataflow-server.app.io</link></literal></simpara>
<screen>$ cd &lt;PATH/TO/SPRING-CLOUD-DATAFLOW-SHELL-JAR&gt;
$ java -jar spring-cloud-dataflow-shell-&lt;VERSION&gt;.jar

  ____                              ____ _                __
 / ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
 \___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
  ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
 |____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
  ____ |_|    _          __|___/                 __________
 |  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
 | | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
 | |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
 |____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/


Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
server-unknown:&gt;</screen>
<screen>server-unknown:&gt;dataflow config server http://dataflow-server.app.io
Successfully targeted http://dataflow-server.app.io
dataflow:&gt;</screen>
<itemizedlist>
<listitem>
<simpara>Running instance of a <literal>rabbit</literal> service in Cloud Foundry</simpara>
</listitem>
<listitem>
<simpara>Running instance of the <link xlink:href="https://docs.pivotal.io/p-cloud-cache/1-0/developer.html">Pivotal Cloud Cache for PCF</link> (PCC) service <literal>cloudcache</literal> in Cloud Foundry.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow/blob/master/spring-cloud-dataflow-docs/src/main/asciidoc/streams.adoc#register-a-stream-app">Register</link> the out-of-the-box applications for the Rabbit binder</simpara>
<note>
<simpara>These samples assume that the Data Flow Server can access a remote Maven repository, <literal><link xlink:href="https://repo.spring.io/libs-release">repo.spring.io/libs-release</link></literal> by default. If your Data Flow server is running behind a firewall, or you are using a maven proxy preventing
access to public repositories, you will need to install the sample apps in your internal Maven repository and <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started-maven-configuration">configure</link>
the server accordingly.  The sample applications are typically registered using Data Flow&#8217;s bulk import facility. For example, the Shell command <literal>dataflow:&gt;app import --uri <link xlink:href="http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven">bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</link></literal> (The actual URI is release and binder specific so refer to the sample instructions for the actual URL).
The bulk import URI references a plain text file containing entries for all of the publicly available Spring Cloud Stream and Task applications published to <literal><link xlink:href="https://repo.spring.io">repo.spring.io</link></literal>. For example,
<literal>source.http=maven://org.springframework.cloud.stream.app:http-source-rabbit:1.2.0.RELEASE</literal> registers the <literal>http</literal> source app at the corresponding Maven address, relative to the remote repository(ies) configured for the
Data Flow server. The format is <literal>maven://&lt;groupId&gt;:&lt;artifactId&gt;:&lt;version&gt;</literal>  You will need to <link xlink:href="https://repo.spring.io/libs-release/org/springframework/cloud/stream/app/spring-cloud-stream-app-descriptor/Bacon.RELEASE/spring-cloud-stream-app-descriptor-Bacon.RELEASE.rabbit-apps-maven-repo-url.properties">download</link> the required apps or <link xlink:href="https://github.com/spring-cloud-stream-app-starters">build</link> them and then install them in your Maven repository, using whatever group, artifact, and version you choose. If you do
this, register individual apps using <literal>dataflow:&gt;app register&#8230;&#8203;</literal> using the <literal>maven://</literal> resource URI format corresponding to your installed app.</simpara>
</note>
<screen>dataflow:&gt;app import --uri http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</screen>
</listitem>
<listitem>
<simpara>Get the PCC connection information</simpara>
<screen>$ cf service-key cloudcache my-service-key
Getting key my-service-key for service instance cloudcache as &lt;user&gt;...

{
 "locators": [
  "10.0.16.9[55221]",
  "10.0.16.11[55221]",
  "10.0.16.10[55221]"
 ],
 "urls": {
  "gfsh": "http://...",
  "pulse": "http://.../pulse"
 },
 "users": [
  {
   "password": &lt;password&gt;,
   "username": "cluster_operator"
  },
  {
   "password": &lt;password&gt;,
   "username": "developer"
  }
 ]
}</screen>
</listitem>
<listitem>
<simpara>Using <literal>gfsh</literal>, connect to the PCC instance as <literal>cluster_operator</literal> using the service key values and create the Test region.</simpara>
<screen>gfsh&gt;connect --use-http --url=&lt;gfsh-url&gt; --user=cluster_operator --password=&lt;cluster_operator_password&gt;
gfsh&gt;create region --name Test --type=REPLICATE</screen>
</listitem>
<listitem>
<simpara>Create the stream, connecting to the PCC instance as developer. This example creates an gemfire source to which will publish events on a region</simpara>
<screen>dataflow stream create --name events --definition " gemfire --username=developer --password=&lt;developer-password&gt; --connect-type=locator --host-addresses=10.0.16.9:55221 --regionName=Test | log" --deploy</screen>
</listitem>
<listitem>
<simpara>Verify the stream is successfully deployed</simpara>
<screen>dataflow:&gt;stream list</screen>
</listitem>
<listitem>
<simpara>Monitor stdout for the log sink</simpara>
<screen>cf logs &lt;log-sink-app-name&gt;</screen>
</listitem>
<listitem>
<simpara>Using <literal>gfsh</literal>, create and update some cache entries</simpara>
<screen>gfsh&gt;connect --use-http --url=&lt;gfsh-url&gt; --user=cluster_operator --password=&lt;cluster_operator_password&gt;
gfsh&gt;put --region /Test --key 1  --value "value 1"
gfsh&gt;put --region /Test --key 2  --value "value 2"
gfsh&gt;put --region /Test --key 3  --value "value 3"
gfsh&gt;put --region /Test --key 1  --value "new value 1"</screen>
</listitem>
<listitem>
<simpara>Observe the log output</simpara>
<simpara>You should see messages like:</simpara>
<screen>2017-10-28 17:28:52.893  INFO 18986 --- [emfire.events-1] log sink                               : value 1"
2017-10-28 17:28:52.893  INFO 18986 --- [emfire.events-1] log sink                               : value 2"
2017-10-28 17:28:52.893  INFO 18986 --- [emfire.events-1] log sink                               : value 3"
2017-10-28 17:28:52.893  INFO 18986 --- [emfire.events-1] log sink                               : new value 1"</screen>
<simpara>By default, the message payload contains the updated value. Depending on your application, you may need additional information. The data comes from <link xlink:href="https://geode.apache.org/releases/latest/javadoc/org/apache/geode/cache/EntryEvent.html">EntryEvent</link>. You
can access any fields using the source&#8217;s <literal>cache-event-expression</literal> property. This takes a SpEL expression bound to the EntryEvent. Try something like <literal>--cache-event-expression='{key:'+key+',new_value:'+newValue+'}'</literal> (HINT: You will need to destroy the stream and recreate it to
add this property, an exercise left to the reader). Now you should see log messages like:</simpara>
<screen>2017-10-28 17:28:52.893  INFO 18986 --- [emfire.events-1] log-sink                                 : {key:1,new_value:value 1}
2017-10-28 17:41:24.466  INFO 18986 --- [emfire.events-1] log-sink                                 : {key:2,new_value:value 2}</screen>
</listitem>
<listitem>
<simpara>You&#8217;re done!</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="_summary_4">
<title>Summary</title>
<simpara>In this sample, you have learned:</simpara>
<itemizedlist>
<listitem>
<simpara>How to use Spring Cloud Data Flow&#8217;s <literal>Local</literal> and <literal>Cloud Foundry</literal> servers</simpara>
</listitem>
<listitem>
<simpara>How to use Spring Cloud Data Flow&#8217;s <literal>shell</literal></simpara>
</listitem>
<listitem>
<simpara>How to create streaming data pipeline to connect and publish events from <literal>gemfire</literal></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="spring-cloud-data-flow-samples-gemfire-cq-log-overview">
<title>Gemfire CQ to Log Demo</title>
<simpara>In this demonstration, you will learn how to build a data pipeline using <link xlink:href="http://cloud.spring.io/spring-cloud-dataflow/">Spring Cloud Data Flow</link> to consume data from a <literal>gemfire-cq</literal> (Continuous Query) endpoint and write to a log using the <literal>log</literal> sink.
The <literal>gemfire-cq</literal> source creates a Continuous Query to monitor events for a region that match the query&#8217;s result set and publish a message whenever such an event is emitted. In this example, we simulate monitoring orders to trigger a process whenever
the quantity ordered is above a defined limit.</simpara>
<simpara>We will take you through the steps to configure and run Spring Cloud Data Flow server in either a <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started/">local</link> or <link xlink:href="https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current/reference/htmlsingle/#getting-started">Cloud Foundry</link> environment.</simpara>
<note>
<simpara>For legacy reasons the <literal>gemfire</literal> Spring Cloud Stream Apps are named after <literal>Pivotal GemFire</literal>. The code base for the commercial product has since been open sourced as <literal>Apache Geode</literal>. These samples should work with compatible versions of Pivotal GemFire or Apache Geode. Herein we will refer to the installed IMDG simply as <literal>Geode</literal>.</simpara>
</note>
<section xml:id="_prerequisites_5">
<title>Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>A Running Data Flow Shell</simpara>
</listitem>
</itemizedlist>
<simpara>The Spring Cloud Data Flow Shell is available for <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started-deploying-spring-cloud-dataflow">download</link> or you can <link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow">build</link> it yourself.</simpara>
<note>
<simpara>the Spring Cloud Data Flow Shell and Local server implementation are in the same repository and are both built by running <literal>./mvnw install</literal> from the project root directory. If you have already run the build, use the jar in <literal>spring-cloud-dataflow-shell/target</literal></simpara>
</note>
<simpara>To run the Shell open a new terminal session:</simpara>
<screen>$ cd &lt;PATH/TO/SPRING-CLOUD-DATAFLOW-SHELL-JAR&gt;
$ java -jar spring-cloud-dataflow-shell-&lt;VERSION&gt;.jar
  ____                              ____ _                __
 / ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
 \___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
  ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
 |____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
  ____ |_|    _          __|___/                 __________
 |  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
 | | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
 | |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
 |____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/


Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
dataflow:&gt;</screen>
<note>
<simpara>The Spring Cloud Data Flow Shell is a Spring Boot application that connects to the Data Flow Server’s REST API and supports a DSL that simplifies the process of defining a stream or task and managing its lifecycle. Most of these samples
use the shell. If you prefer, you can use the Data Flow UI <link xlink:href="http://localhost:9393/dashboard">localhost:9393/dashboard</link>, (or wherever it the server is hosted) to perform the same operations.</simpara>
</note>
<itemizedlist>
<listitem>
<simpara>A Geode installation with a locator and cache server running
Unresolved directive in streaming/gemfire/gemfire-cq-log/overview.adoc - include::geode-setup.adoc[]</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="gemfire-cq-log-local">
<title>Using the Local Server</title>
<section xml:id="_additional_prerequisites_9">
<title>Additional Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>A Running Data Flow Server</simpara>
</listitem>
</itemizedlist>
<simpara>The Local Data Flow Server is Spring Boot application available for <link xlink:href="http://cloud.spring.io/spring-cloud-dataflow/">download</link> or you can <link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow">build</link> it yourself.
If you build it yourself, the executable jar will be in <literal>spring-cloud-dataflow-server-local/target</literal></simpara>
<simpara>To run the Local Data Flow server Open a new terminal session:</simpara>
<screen>$cd  &lt;PATH/TO/SPRING-CLOUD-DATAFLOW-LOCAL-JAR&gt;
$java -jar spring-cloud-dataflow-server-local-&lt;VERSION&gt;.jar</screen>
<itemizedlist>
<listitem>
<simpara>A running instance of <link xlink:href="https://www.rabbitmq.com">Rabbit MQ</link></simpara>
</listitem>
</itemizedlist>
<simpara><?asciidoc-hr?></simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Use gfsh to start a locator and server</simpara>
<screen>gfsh&gt;start locator --name=locator1
gfsh&gt;start server --name=server1</screen>
</listitem>
<listitem>
<simpara>Create a region called <literal>Orders</literal></simpara>
<screen>gfsh&gt;create region --name Orders --type=REPLICATE</screen>
<simpara>===== Use the Shell to create the sample stream.</simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow/blob/master/spring-cloud-dataflow-docs/src/main/asciidoc/streams.adoc#register-a-stream-app">Register</link> the out-of-the-box applications for the Rabbit binder</simpara>
<note>
<simpara>These samples assume that the Data Flow Server can access a remote Maven repository, <literal><link xlink:href="https://repo.spring.io/libs-release">repo.spring.io/libs-release</link></literal> by default. If your Data Flow server is running behind a firewall, or you are using a maven proxy preventing
access to public repositories, you will need to install the sample apps in your internal Maven repository and <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started-maven-configuration">configure</link>
the server accordingly.  The sample applications are typically registered using Data Flow&#8217;s bulk import facility. For example, the Shell command <literal>dataflow:&gt;app import --uri <link xlink:href="http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven">bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</link></literal> (The actual URI is release and binder specific so refer to the sample instructions for the actual URL).
The bulk import URI references a plain text file containing entries for all of the publicly available Spring Cloud Stream and Task applications published to <literal><link xlink:href="https://repo.spring.io">repo.spring.io</link></literal>. For example,
<literal>source.http=maven://org.springframework.cloud.stream.app:http-source-rabbit:1.2.0.RELEASE</literal> registers the <literal>http</literal> source app at the corresponding Maven address, relative to the remote repository(ies) configured for the
Data Flow server. The format is <literal>maven://&lt;groupId&gt;:&lt;artifactId&gt;:&lt;version&gt;</literal>  You will need to <link xlink:href="https://repo.spring.io/libs-release/org/springframework/cloud/stream/app/spring-cloud-stream-app-descriptor/Bacon.RELEASE/spring-cloud-stream-app-descriptor-Bacon.RELEASE.rabbit-apps-maven-repo-url.properties">download</link> the required apps or <link xlink:href="https://github.com/spring-cloud-stream-app-starters">build</link> them and then install them in your Maven repository, using whatever group, artifact, and version you choose. If you do
this, register individual apps using <literal>dataflow:&gt;app register&#8230;&#8203;</literal> using the <literal>maven://</literal> resource URI format corresponding to your installed app.</simpara>
</note>
<screen>dataflow:&gt;app import --uri http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</screen>
</listitem>
<listitem>
<simpara>Create the stream</simpara>
<simpara>This example creates an gemfire-cq source to which will publish events matching a query criteria on a region. In this case we will monitor the <literal>Orders</literal> region. For simplicity, we will avoid creating a data structure for the order.
Each cache entry contains an integer value representing the quantity of the ordered item. This stream will fire a message whenever the value&gt;999. By default, the source emits only the value. Here we will override that using the
<literal>cq-event-expression</literal> property.  This accepts a SpEL expression bound to a <link xlink:href="https://geode.apache.org/releases/latest/javadoc/org/apache/geode/cache/query/CqEvent.html">CQEvent</link>. To reference the entire CQEvent instace, we use <literal>#this</literal>.
In order to display the contents in the log, we will invoke <literal>toString()</literal> on the instance.</simpara>
<screen>dataflow:&gt;stream create --name orders --definition " gemfire-cq --query='SELECT * from /Orders o where o &gt; 999' --cq-event-expression=#this.toString() | log" --deploy
Created and deployed new stream 'events'</screen>
<note>
<simpara>If the Geode locator isn&#8217;t running on default port on <literal>localhost</literal>, add the options <literal>--connect-type=locator --host-addresses=&lt;host&gt;:&lt;port&gt;</literal>. If there are multiple
locators, you can provide a comma separated list of locator addresses. This is not necessary for the sample but is typical for production environments to enable fail-over.</simpara>
</note>
</listitem>
<listitem>
<simpara>Verify the stream is successfully deployed</simpara>
<screen>dataflow:&gt;stream list</screen>
</listitem>
<listitem>
<simpara>Monitor stdout for the log sink. When you deploy the stream, you will see log messages in the Data Flow server console like this</simpara>
<screen>2017-10-30 09:39:36.283  INFO 8167 --- [nio-9393-exec-5] o.s.c.d.spi.local.LocalAppDeployer       : Deploying app with deploymentId orders.log instance 0.
   Logs will be in /var/folders/hd/5yqz2v2d3sxd3n879f4sg4gr0000gn/T/spring-cloud-dataflow-5375107584795488581/orders-1509370775940/orders.log</screen>
<simpara>Copy the location of the <literal>log</literal> sink logs. This is a directory that ends in <literal>orders.log</literal>. The log files will be in <literal>stdout_0.log</literal> under this directory. You can monitor the output of the log sink using <literal>tail</literal>, or something similar:</simpara>
<screen>$tail -f /var/folders/hd/5yqz2v2d3sxd3n879f4sg4gr0000gn/T/spring-cloud-dataflow-5375107584795488581/orders-1509370775940/orders.log/stdout_0.log</screen>
</listitem>
<listitem>
<simpara>Using <literal>gfsh</literal>, create and update some cache entries</simpara>
<screen>gfsh&gt;put --region Orders  --value-class java.lang.Integer --key 01234 --value 1000
gfsh&gt;put --region Orders  --value-class java.lang.Integer --key 11234 --value 1005
gfsh&gt;put --region Orders  --value-class java.lang.Integer --key 21234 --value 100
gfsh&gt;put --region Orders  --value-class java.lang.Integer --key 31234 --value 999
gfsh&gt;put --region Orders  --value-class java.lang.Integer --key 21234 --value 1000</screen>
</listitem>
<listitem>
<simpara>Observe the log output
You should see messages like:</simpara>
<screen>2017-10-30 09:53:02.231  INFO 8563 --- [ire-cq.orders-1] log-sink                                 : CqEvent [CqName=GfCq1; base operation=CREATE; cq operation=CREATE; key=01234; value=1000]
2017-10-30 09:53:19.732  INFO 8563 --- [ire-cq.orders-1] log-sink                                 : CqEvent [CqName=GfCq1; base operation=CREATE; cq operation=CREATE; key=11234; value=1005]
2017-10-30 09:53:53.242  INFO 8563 --- [ire-cq.orders-1] log-sink                                 : CqEvent [CqName=GfCq1; base operation=UPDATE; cq operation=CREATE; key=21234; value=1000]</screen>
</listitem>
<listitem>
<simpara>Another interesting demonstration combines <literal>gemfire-cq</literal> with the <link xlink:href=":../http-gemfire/README.adoc">http-gemfire</link> example.</simpara>
</listitem>
</orderedlist>
<screen>dataflow:&gt; stream create --name stocks --definition "http --port=9090 | gemfire-json-server --regionName=Stocks --keyExpression=payload.getField('symbol')" --deploy
dataflow:&gt; stream create --name stock_watch --definition "gemfire-cq --query='Select * from /Stocks where symbol=''VMW''' | log" --deploy</screen>
<orderedlist numeration="arabic">
<listitem>
<simpara>You&#8217;re done!</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="gemfire-cq-log-cf">
<title>Using the Cloud Foundry Server</title>
<section xml:id="_additional_prerequisites_10">
<title>Additional Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>A Cloud Foundry instance</simpara>
</listitem>
<listitem>
<simpara>The Spring Cloud Data Flow Cloud Foundry Server</simpara>
</listitem>
</itemizedlist>
<simpara>The Cloud Foundry Data Flow Server is Spring Boot application available for <link xlink:href="http://cloud.spring.io/spring-cloud-dataflow/">download</link> or you can <link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow-server-cloudfoundry">build</link> it yourself.
If you build it yourself, the executable jar will be in <literal>spring-cloud-dataflow-server-cloudfoundry/target</literal></simpara>
<note>
<simpara>Although you can run the Data Flow Cloud Foundry Server locally and configure it to deploy to any Cloud Foundry instance, we will
deploy the server to Cloud Foundry as recommended.</simpara>
</note>
<orderedlist numeration="arabic">
<listitem>
<simpara>Verify that CF instance is reachable (Your endpoint urls will be different from what is shown here).</simpara>
<screen>$ cf api
API endpoint: https://api.system.io (API version: ...)

$ cf apps
Getting apps in org [your-org] / space [your-space] as user...
OK

No apps found</screen>
</listitem>
<listitem>
<simpara>Follow the instructions to deploy the <link xlink:href="https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current/reference/htmlsingle">Spring Cloud Data Flow Cloud Foundry server</link>. Don&#8217;t worry about creating a Redis service. We won&#8217;t need it. If you are familiar with Cloud Foundry
application manifests, we recommend creating a manifest for the the Data Flow server as shown <link xlink:href="https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current/reference/htmlsingle/#sample-manifest-template">here</link>.</simpara>
<warning>
<simpara>As of this writing, there is a typo on the <literal>SPRING_APPLICATION_JSON</literal> entry in the sample manifest. <literal>SPRING_APPLICATION_JSON</literal> must be followed by <literal>:</literal> and The JSON string must be
wrapped in single quotes. Alternatively, you can replace that line with <literal>MAVEN_REMOTE_REPOSITORIES_REPO1_URL: <link xlink:href="https://repo.spring.io/libs-snapshot">repo.spring.io/libs-snapshot</link></literal>.  If your Cloud Foundry installation is behind a firewall, you may need to install the stream apps used in this sample in your internal Maven repository and <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/1.3.0.M2/reference/htmlsingle/#getting-started-maven-configuration">configure</link> the server to access that repository.</simpara>
</warning>
</listitem>
<listitem>
<simpara>Once you have successfully executed <literal>cf push</literal>, verify the dataflow server is running</simpara>
<screen>$ cf apps
Getting apps in org [your-org] / space [your-space] as user...
OK

name                 requested state   instances   memory   disk   urls
dataflow-server      started           1/1         1G       1G     dataflow-server.app.io</screen>
</listitem>
<listitem>
<simpara>Notice that the <literal>dataflow-server</literal> application is started and ready for interaction via the url endpoint</simpara>
</listitem>
<listitem>
<simpara>Connect the <literal>shell</literal> with <literal>server</literal> running on Cloud Foundry, e.g., <literal><link xlink:href="http://dataflow-server.app.io">dataflow-server.app.io</link></literal></simpara>
<screen>$ cd &lt;PATH/TO/SPRING-CLOUD-DATAFLOW-SHELL-JAR&gt;
$ java -jar spring-cloud-dataflow-shell-&lt;VERSION&gt;.jar

  ____                              ____ _                __
 / ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
 \___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
  ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
 |____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
  ____ |_|    _          __|___/                 __________
 |  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
 | | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
 | |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
 |____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/


Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
server-unknown:&gt;</screen>
<screen>server-unknown:&gt;dataflow config server http://dataflow-server.app.io
Successfully targeted http://dataflow-server.app.io
dataflow:&gt;</screen>
<itemizedlist>
<listitem>
<simpara>Running instance of a <literal>rabbit</literal> service in Cloud Foundry</simpara>
</listitem>
<listitem>
<simpara>Running instance of the <link xlink:href="https://docs.pivotal.io/p-cloud-cache/1-0/developer.html">Pivotal Cloud Cache for PCF</link> (PCC) service <literal>cloudcache</literal> in Cloud Foundry.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow/blob/master/spring-cloud-dataflow-docs/src/main/asciidoc/streams.adoc#register-a-stream-app">Register</link> the out-of-the-box applications for the Rabbit binder</simpara>
<note>
<simpara>These samples assume that the Data Flow Server can access a remote Maven repository, <literal><link xlink:href="https://repo.spring.io/libs-release">repo.spring.io/libs-release</link></literal> by default. If your Data Flow server is running behind a firewall, or you are using a maven proxy preventing
access to public repositories, you will need to install the sample apps in your internal Maven repository and <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started-maven-configuration">configure</link>
the server accordingly.  The sample applications are typically registered using Data Flow&#8217;s bulk import facility. For example, the Shell command <literal>dataflow:&gt;app import --uri <link xlink:href="http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven">bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</link></literal> (The actual URI is release and binder specific so refer to the sample instructions for the actual URL).
The bulk import URI references a plain text file containing entries for all of the publicly available Spring Cloud Stream and Task applications published to <literal><link xlink:href="https://repo.spring.io">repo.spring.io</link></literal>. For example,
<literal>source.http=maven://org.springframework.cloud.stream.app:http-source-rabbit:1.2.0.RELEASE</literal> registers the <literal>http</literal> source app at the corresponding Maven address, relative to the remote repository(ies) configured for the
Data Flow server. The format is <literal>maven://&lt;groupId&gt;:&lt;artifactId&gt;:&lt;version&gt;</literal>  You will need to <link xlink:href="https://repo.spring.io/libs-release/org/springframework/cloud/stream/app/spring-cloud-stream-app-descriptor/Bacon.RELEASE/spring-cloud-stream-app-descriptor-Bacon.RELEASE.rabbit-apps-maven-repo-url.properties">download</link> the required apps or <link xlink:href="https://github.com/spring-cloud-stream-app-starters">build</link> them and then install them in your Maven repository, using whatever group, artifact, and version you choose. If you do
this, register individual apps using <literal>dataflow:&gt;app register&#8230;&#8203;</literal> using the <literal>maven://</literal> resource URI format corresponding to your installed app.</simpara>
</note>
<screen>dataflow:&gt;app import --uri http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</screen>
</listitem>
<listitem>
<simpara>Get the PCC connection information</simpara>
<screen>$ cf service-key cloudcache my-service-key
Getting key my-service-key for service instance cloudcache as &lt;user&gt;...

{
 "locators": [
  "10.0.16.9[55221]",
  "10.0.16.11[55221]",
  "10.0.16.10[55221]"
 ],
 "urls": {
  "gfsh": "http://...",
  "pulse": "http://.../pulse"
 },
 "users": [
  {
   "password": &lt;password&gt;,
   "username": "cluster_operator"
  },
  {
   "password": &lt;password&gt;,
   "username": "developer"
  }
 ]
}</screen>
</listitem>
<listitem>
<simpara>Using <literal>gfsh</literal>, connect to the PCC instance as <literal>cluster_operator</literal> using the service key values and create the Test region.</simpara>
<screen>gfsh&gt;connect --use-http --url=&lt;gfsh-url&gt; --user=cluster_operator --password=&lt;cluster_operator_password&gt;
gfsh&gt;create region --name Orders --type=REPLICATE</screen>
</listitem>
<listitem>
<simpara>Create the stream using the Data Flow Shell</simpara>
<simpara>This example creates an gemfire-cq source to which will publish events matching a query criteria on a region. In this case we will monitor the <literal>Orders</literal> region. For simplicity, we will avoid creating a data structure for the order.
Each cache entry contains an integer value representing the quantity of the ordered item. This stream will fire a message whenever the value&gt;999. By default, the source emits only the value. Here we will override that using the
<literal>cq-event-expression</literal> property.  This accepts a SpEL expression bound to a <link xlink:href="https://geode.apache.org/releases/latest/javadoc/org/apache/geode/cache/query/CqEvent.html">CQEvent</link>. To reference the entire CQEvent instance, we use <literal>#this</literal>.
In order to display the contents in the log, we will invoke <literal>toString()</literal> on the instance.</simpara>
<screen>dataflow:&gt;stream create --name orders --definition " gemfire-cq  --username=developer --password=&lt;developer-password&gt; --connect-type=locator --host-addresses=10.0.16.9:55221 --query='SELECT * from /Orders o where o &gt; 999' --cq-event-expression=#this.toString()  | log" --deploy
Created and deployed new stream 'events'</screen>
</listitem>
<listitem>
<simpara>Verify the stream is successfully deployed</simpara>
<screen>dataflow:&gt;stream list</screen>
</listitem>
<listitem>
<simpara>Monitor stdout for the log sink</simpara>
<screen>cf logs &lt;log-sink-app-name&gt;</screen>
</listitem>
<listitem>
<simpara>Using <literal>gfsh</literal>, create and update some cache entries</simpara>
<screen>gfsh&gt;connect --use-http --url=&lt;gfsh-url&gt; --user=cluster_operator --password=&lt;cluster_operator_password&gt;
gfsh&gt;put --region Orders  --value-class java.lang.Integer --key 01234 --value 1000
gfsh&gt;put --region Orders  --value-class java.lang.Integer --key 11234 --value 1005
gfsh&gt;put --region Orders  --value-class java.lang.Integer --key 21234 --value 100
gfsh&gt;put --region Orders  --value-class java.lang.Integer --key 31234 --value 999
gfsh&gt;put --region Orders  --value-class java.lang.Integer --key 21234 --value 1000</screen>
</listitem>
<listitem>
<simpara>Observe the log output
You should see messages like:</simpara>
<screen>2017-10-30 09:53:02.231  INFO 8563 --- [ire-cq.orders-1] log-sink                                 : CqEvent [CqName=GfCq1; base operation=CREATE; cq operation=CREATE; key=01234; value=1000]
2017-10-30 09:53:19.732  INFO 8563 --- [ire-cq.orders-1] log-sink                                 : CqEvent [CqName=GfCq1; base operation=CREATE; cq operation=CREATE; key=11234; value=1005]
2017-10-30 09:53:53.242  INFO 8563 --- [ire-cq.orders-1] log-sink                                 : CqEvent [CqName=GfCq1; base operation=UPDATE; cq operation=CREATE; key=21234; value=1000]</screen>
</listitem>
<listitem>
<simpara>Another interesting demonstration combines <literal>gemfire-cq</literal> with the <link xlink:href="../http-gemfire/README.adoc">http-gemfire</link> example.</simpara>
</listitem>
</orderedlist>
<screen>dataflow:&gt; stream create --name stocks --definition "http --port=9090 | gemfire-json-server --regionName=Stocks --keyExpression=payload.getField('symbol')" --deploy
dataflow:&gt; stream create --name stock_watch --definition "gemfire-cq --query='Select * from /Stocks where symbol=''VMW''' | log" --deploy</screen>
<orderedlist numeration="arabic">
<listitem>
<simpara>You&#8217;re done!</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="_summary_5">
<title>Summary</title>
<simpara>In this sample, you have learned:</simpara>
<itemizedlist>
<listitem>
<simpara>How to use Spring Cloud Data Flow&#8217;s <literal>Local</literal> and <literal>Cloud Foundry</literal> servers</simpara>
</listitem>
<listitem>
<simpara>How to use Spring Cloud Data Flow&#8217;s <literal>shell</literal></simpara>
</listitem>
<listitem>
<simpara>How to create streaming data pipeline to connect and publish CQ events from <literal>gemfire</literal></simpara>
</listitem>
</itemizedlist>
</section>
</section>
</section>
<section xml:id="spring-cloud-data-flow-samples-custom-apps-overview">
<title>Custom Stream Application Samples</title>
<section xml:id="spring-cloud-data-flow-samples-custom-application-overview">
<title>Custom Spring Cloud Stream Processor</title>
<section xml:id="_prerequisites_6">
<title>Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>A Running Data Flow Shell</simpara>
</listitem>
</itemizedlist>
<simpara>The Spring Cloud Data Flow Shell is available for <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started-deploying-spring-cloud-dataflow">download</link> or you can <link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow">build</link> it yourself.</simpara>
<note>
<simpara>the Spring Cloud Data Flow Shell and Local server implementation are in the same repository and are both built by running <literal>./mvnw install</literal> from the project root directory. If you have already run the build, use the jar in <literal>spring-cloud-dataflow-shell/target</literal></simpara>
</note>
<simpara>To run the Shell open a new terminal session:</simpara>
<screen>$ cd &lt;PATH/TO/SPRING-CLOUD-DATAFLOW-SHELL-JAR&gt;
$ java -jar spring-cloud-dataflow-shell-&lt;VERSION&gt;.jar
  ____                              ____ _                __
 / ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
 \___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
  ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
 |____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
  ____ |_|    _          __|___/                 __________
 |  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
 | | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
 | |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
 |____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/


Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
dataflow:&gt;</screen>
<note>
<simpara>The Spring Cloud Data Flow Shell is a Spring Boot application that connects to the Data Flow Server’s REST API and supports a DSL that simplifies the process of defining a stream or task and managing its lifecycle. Most of these samples
use the shell. If you prefer, you can use the Data Flow UI <link xlink:href="http://localhost:9393/dashboard">localhost:9393/dashboard</link>, (or wherever it the server is hosted) to perform the same operations.</simpara>
</note>
<itemizedlist>
<listitem>
<simpara>A running local Data Flow Server</simpara>
</listitem>
</itemizedlist>
<simpara>The Local Data Flow Server is Spring Boot application available for <link xlink:href="http://cloud.spring.io/spring-cloud-dataflow/">download</link> or you can <link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow">build</link> it yourself.
If you build it yourself, the executable jar will be in <literal>spring-cloud-dataflow-server-local/target</literal></simpara>
<simpara>To run the Local Data Flow server Open a new terminal session:</simpara>
<screen>$cd  &lt;PATH/TO/SPRING-CLOUD-DATAFLOW-LOCAL-JAR&gt;
$java -jar spring-cloud-dataflow-server-local-&lt;VERSION&gt;.jar</screen>
<itemizedlist>
<listitem>
<simpara>A Java IDE</simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://maven.apache.org/">Maven</link> Installed</simpara>
</listitem>
<listitem>
<simpara>A running instance of <link xlink:href="https://www.rabbitmq.com/">Rabbit MQ</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="_create_the_custom_stream_app">
<title>Create the custom stream app</title>
<simpara>We will create a custom <link xlink:href="https://cloud.spring.io/spring-cloud-stream/">Spring Cloud Stream</link> application and run it on Spring Cloud Data Flow.
We&#8217;ll go through the steps to make a simple processor that converts temperature from Fahrenheit to Celsius.
We will be running the demo locally, but all the steps will work in a Cloud Foundry environment as well.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Create a new spring cloud stream project</simpara>
<itemizedlist>
<listitem>
<simpara>Create a <link xlink:href="http://start.spring.io/">Spring initializer</link> project</simpara>
</listitem>
<listitem>
<simpara>Set the group to <literal>demo.celsius.converter</literal> and the artifact name as <literal>celsius-converter-processor</literal></simpara>
</listitem>
<listitem>
<simpara>Choose a message transport binding as a dependency for the custom app
There are options for choosing <literal>Rabbit MQ</literal> or <literal>Kafka</literal> as the message transport.
For this demo, we will use <literal>rabbit</literal>. Type <emphasis>rabbit</emphasis> in the search bar under <emphasis>Search for dependencies</emphasis> and select <literal>Stream Rabbit</literal>.</simpara>
</listitem>
<listitem>
<simpara>Hit the generate project button and open the new project in an IDE of your choice</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Develop the app</simpara>
<simpara>We can now create our custom app. Our Spring Cloud Stream application is a Spring Boot application that runs as an executable jar. The application will include two Java classes:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>CelsiusConverterProcessorAplication.java</literal> - the main Spring Boot application class, generated by Spring initializr</simpara>
</listitem>
<listitem>
<simpara><literal>CelsiusConverterProcessorConfiguration.java</literal> - the Spring Cloud Stream code that we will write</simpara>
<simpara>We are creating a transformer that takes a Fahrenheit input and converts it to Celsius.
Following the same naming convention as the application file, create a new Java class in the same package called <literal>CelsiusConverterProcessorConfiguration.java</literal>.</simpara>
<formalpara>
<title>CelsiusConverterProcessorConfiguration.java</title>
<para>
<screen>@EnableBinding(Processor.class)
public class CelsiusConverterProcessorConfiguration {

    @Transformer(inputChannel = Processor.INPUT, outputChannel = Processor.OUTPUT)
    public int convertToCelsius(String payload) {
        int fahrenheitTemperature = Integer.parseInt(payload);
        return (farenheitTemperature-32)*5/9;
    }
}</screen>
</para>
</formalpara>
<simpara>Here we introduced two important Spring annotations.
First we annotated the class with <literal>@EnableBinding(Processor.class)</literal>.
Second we created a method and annotated it with <literal>@Transformer(inputChannel = Processor.INPUT, outputChannel = Processor.OUTPUT)</literal>.
By adding these two annotations we have configured this stream app as a <literal>Processor</literal> (as opposed to a <literal>Source</literal> or a <literal>Sink</literal>).
This means that the application receives input from an upstream application via the <literal>Processor.INPUT</literal> channel and sends its output to a downstream application via the <literal>Processor.OUTPUT</literal> channel.</simpara>
<simpara>The <literal>convertToCelsius</literal> method takes a <literal>String</literal> as input for Fahrenheit and then returns the converted Celsius as an integer.
This method is very simple, but that is also the beauty of this programming style.
We can add as much logic as we want to this method to enrich this processor.
As long as we annotate it properly and return valid output, it works as a Spring Cloud Stream Processor. Also note that it is straightforward to unit test this code.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Build the Spring Boot application with Maven</simpara>
<screen>$cd &lt;PROJECT_DIR&gt;
$./mvnw clean package</screen>
</listitem>
<listitem>
<simpara>Run the Application standalone</simpara>
<screen>java -jar target/celsius-converter-processor-0.0.1-SNAPSHOT.jar</screen>
<simpara>If all goes well, we should have a running standalone Spring Boot Application.
Once we verify that the app is started and running without any errors, we can stop it.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="_deploy_the_app_to_spring_cloud_data_flow">
<title>Deploy the app to Spring Cloud Data Flow</title>
<orderedlist numeration="arabic">
<listitem>
<simpara><link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow/blob/master/spring-cloud-dataflow-docs/src/main/asciidoc/streams.adoc#register-a-stream-app">Register</link> the out-of-the-box applications for the Rabbit binder</simpara>
<note>
<simpara>These samples assume that the Data Flow Server can access a remote Maven repository, <literal><link xlink:href="https://repo.spring.io/libs-release">repo.spring.io/libs-release</link></literal> by default. If your Data Flow server is running behind a firewall, or you are using a maven proxy preventing
access to public repositories, you will need to install the sample apps in your internal Maven repository and <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started-maven-configuration">configure</link>
the server accordingly.  The sample applications are typically registered using Data Flow&#8217;s bulk import facility. For example, the Shell command <literal>dataflow:&gt;app import --uri <link xlink:href="http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven">bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</link></literal> (The actual URI is release and binder specific so refer to the sample instructions for the actual URL).
The bulk import URI references a plain text file containing entries for all of the publicly available Spring Cloud Stream and Task applications published to <literal><link xlink:href="https://repo.spring.io">repo.spring.io</link></literal>. For example,
<literal>source.http=maven://org.springframework.cloud.stream.app:http-source-rabbit:1.2.0.RELEASE</literal> registers the <literal>http</literal> source app at the corresponding Maven address, relative to the remote repository(ies) configured for the
Data Flow server. The format is <literal>maven://&lt;groupId&gt;:&lt;artifactId&gt;:&lt;version&gt;</literal>  You will need to <link xlink:href="https://repo.spring.io/libs-release/org/springframework/cloud/stream/app/spring-cloud-stream-app-descriptor/Bacon.RELEASE/spring-cloud-stream-app-descriptor-Bacon.RELEASE.rabbit-apps-maven-repo-url.properties">download</link> the required apps or <link xlink:href="https://github.com/spring-cloud-stream-app-starters">build</link> them and then install them in your Maven repository, using whatever group, artifact, and version you choose. If you do
this, register individual apps using <literal>dataflow:&gt;app register&#8230;&#8203;</literal> using the <literal>maven://</literal> resource URI format corresponding to your installed app.</simpara>
</note>
<screen>dataflow:&gt;app import --uri http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</screen>
</listitem>
<listitem>
<simpara>Register the custom processor</simpara>
<screen>app register --type processor --name convertToCelsius --uri &lt;File URL of the jar file on the local filesystem where you built the project above&gt; --force</screen>
</listitem>
<listitem>
<simpara>Create the stream</simpara>
<simpara>We will create a stream that uses the out of the box <literal>http</literal> source and <literal>log</literal> sink and our custom transformer.</simpara>
<screen>dataflow:&gt;stream create --name convertToCelsiusStream --definition "http  --port=9090 | convertToCelsius | log" --deploy

Created and deployed new stream 'convertToCelsiusStream'</screen>
</listitem>
<listitem>
<simpara>Verify the stream is successfully deployed</simpara>
<screen>dataflow:&gt;stream list</screen>
</listitem>
<listitem>
<simpara>Verify that the apps have successfully deployed</simpara>
<screen>dataflow:&gt;runtime apps</screen>
<screen>2016-09-27 10:03:11.988  INFO 95234 --- [nio-9393-exec-9] o.s.c.d.spi.local.LocalAppDeployer       : deploying app convertToCelsiusStream.log instance 0
   Logs will be in /var/folders/2q/krqwcbhj2d58csmthyq_n1nw0000gp/T/spring-cloud-dataflow-3236898888473815319/convertToCelsiusStream-1474984991968/convertToCelsiusStream.log
2016-09-27 10:03:12.397  INFO 95234 --- [nio-9393-exec-9] o.s.c.d.spi.local.LocalAppDeployer       : deploying app convertToCelsiusStream.convertToCelsius instance 0
   Logs will be in /var/folders/2q/krqwcbhj2d58csmthyq_n1nw0000gp/T/spring-cloud-dataflow-3236898888473815319/convertToCelsiusStream-1474984992392/convertToCelsiusStream.convertToCelsius
2016-09-27 10:03:14.445  INFO 95234 --- [nio-9393-exec-9] o.s.c.d.spi.local.LocalAppDeployer       : deploying app convertToCelsiusStream.http instance 0
   Logs will be in /var/folders/2q/krqwcbhj2d58csmthyq_n1nw0000gp/T/spring-cloud-dataflow-3236898888473815319/convertToCelsiusStream-1474984994440/convertToCelsiusStream.http</screen>
</listitem>
<listitem>
<simpara>Post sample data to the <literal>http</literal> endpoint: <literal><link xlink:href="http://localhost:9090">localhost:9090</link></literal> (<literal>9090</literal> is the <literal>port</literal> we specified for the <literal>http</literal> source in this case)</simpara>
<screen>dataflow:&gt;http post --target http://localhost:9090 --data 76
&gt; POST (text/plain;Charset=UTF-8) http://localhost:9090 76
&gt; 202 ACCEPTED</screen>
</listitem>
<listitem>
<simpara>Open the log file for the <literal>convertToCelsiusStream.log</literal> app to see the output of our stream</simpara>
<screen>tail -f /var/folders/2q/krqwcbhj2d58csmthyq_n1nw0000gp/T/spring-cloud-dataflow-7563139704229890655/convertToCelsiusStream-1474990317406/convertToCelsiusStream.log/stdout_0.log</screen>
<simpara>You should see the temperature you posted converted to Celsius!</simpara>
</listitem>
</orderedlist>
<screen>2016-09-27 10:05:34.933  INFO 95616 --- [CelsiusStream-1] log.sink                                 : 24</screen>
</section>
<section xml:id="_summary_6">
<title>Summary</title>
<simpara>In this sample, you have learned:</simpara>
<itemizedlist>
<listitem>
<simpara>How to write a custom <literal>Processor</literal> stream application</simpara>
</listitem>
<listitem>
<simpara>How to use Spring Cloud Data Flow&#8217;s <literal>Local</literal> server</simpara>
</listitem>
<listitem>
<simpara>How to use Spring Cloud Data Flow&#8217;s <literal>shell</literal> application</simpara>
</listitem>
</itemizedlist>
</section>
</section>
</section>
</chapter>
<chapter xml:id="_task_batch">
<title>Task / Batch</title>
<section xml:id="spring-cloud-data-flow-samples-task-overview">
<title>Task Samples</title>
<section xml:id="_batch_job_on_cloud_foundry">
<title>Batch Job on Cloud Foundry</title>
<simpara>In this demonstration, you will learn how to orchestrate short-lived data processing application (<emphasis>eg: Spring Batch Jobs</emphasis>) using <link xlink:href="http://cloud.spring.io/spring-cloud-task/">Spring Cloud Task</link> and <link xlink:href="http://cloud.spring.io/spring-cloud-dataflow/">Spring Cloud Data Flow</link> on Cloud Foundry.</simpara>
<section xml:id="_prerequisites_7">
<title>Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>Local <link xlink:href="https://pivotal.io/pcf-dev">PCFDev</link> instance</simpara>
</listitem>
<listitem>
<simpara>Local install of <link xlink:href="https://github.com/cloudfoundry/cli">cf CLI</link> command line tool</simpara>
</listitem>
<listitem>
<simpara>Running instance of mysql in PCFDev</simpara>
</listitem>
<listitem>
<simpara>A Running Data Flow Shell</simpara>
</listitem>
</itemizedlist>
<simpara>The Spring Cloud Data Flow Shell is available for <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started-deploying-spring-cloud-dataflow">download</link> or you can <link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow">build</link> it yourself.</simpara>
<note>
<simpara>the Spring Cloud Data Flow Shell and Local server implementation are in the same repository and are both built by running <literal>./mvnw install</literal> from the project root directory. If you have already run the build, use the jar in <literal>spring-cloud-dataflow-shell/target</literal></simpara>
</note>
<simpara>To run the Shell open a new terminal session:</simpara>
<screen>$ cd &lt;PATH/TO/SPRING-CLOUD-DATAFLOW-SHELL-JAR&gt;
$ java -jar spring-cloud-dataflow-shell-&lt;VERSION&gt;.jar
  ____                              ____ _                __
 / ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
 \___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
  ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
 |____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
  ____ |_|    _          __|___/                 __________
 |  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
 | | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
 | |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
 |____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/


Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
dataflow:&gt;</screen>
<note>
<simpara>The Spring Cloud Data Flow Shell is a Spring Boot application that connects to the Data Flow Server’s REST API and supports a DSL that simplifies the process of defining a stream or task and managing its lifecycle. Most of these samples
use the shell. If you prefer, you can use the Data Flow UI <link xlink:href="http://localhost:9393/dashboard">localhost:9393/dashboard</link>, (or wherever it the server is hosted) to perform the same operations.</simpara>
</note>
<itemizedlist>
<listitem>
<simpara>The Spring Cloud Data Flow Cloud Foundry Server running in PCFDev</simpara>
</listitem>
</itemizedlist>
<simpara>The Cloud Foundry Data Flow Server is Spring Boot application available for <link xlink:href="http://cloud.spring.io/spring-cloud-dataflow/">download</link> or you can <link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow-server-cloudfoundry">build</link> it yourself.
If you build it yourself, the executable jar will be in <literal>spring-cloud-dataflow-server-cloudfoundry/target</literal></simpara>
<note>
<simpara>Although you can run the Data Flow Cloud Foundry Server locally and configure it to deploy to any Cloud Foundry instance, we will
deploy the server to Cloud Foundry as recommended.</simpara>
</note>
<orderedlist numeration="arabic">
<listitem>
<simpara>Verify that CF instance is reachable (Your endpoint urls will be different from what is shown here).</simpara>
<screen>$ cf api
API endpoint: https://api.system.io (API version: ...)

$ cf apps
Getting apps in org [your-org] / space [your-space] as user...
OK

No apps found</screen>
</listitem>
<listitem>
<simpara>Follow the instructions to deploy the <link xlink:href="https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current/reference/htmlsingle">Spring Cloud Data Flow Cloud Foundry server</link>. Don&#8217;t worry about creating a Redis service. We won&#8217;t need it. If you are familiar with Cloud Foundry
application manifests, we recommend creating a manifest for the the Data Flow server as shown <link xlink:href="https://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current/reference/htmlsingle/#sample-manifest-template">here</link>.</simpara>
<warning>
<simpara>As of this writing, there is a typo on the <literal>SPRING_APPLICATION_JSON</literal> entry in the sample manifest. <literal>SPRING_APPLICATION_JSON</literal> must be followed by <literal>:</literal> and The JSON string must be
wrapped in single quotes. Alternatively, you can replace that line with <literal>MAVEN_REMOTE_REPOSITORIES_REPO1_URL: <link xlink:href="https://repo.spring.io/libs-snapshot">repo.spring.io/libs-snapshot</link></literal>.  If your Cloud Foundry installation is behind a firewall, you may need to install the stream apps used in this sample in your internal Maven repository and <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/1.3.0.M2/reference/htmlsingle/#getting-started-maven-configuration">configure</link> the server to access that repository.</simpara>
</warning>
</listitem>
<listitem>
<simpara>Once you have successfully executed <literal>cf push</literal>, verify the dataflow server is running</simpara>
<screen>$ cf apps
Getting apps in org [your-org] / space [your-space] as user...
OK

name                 requested state   instances   memory   disk   urls
dataflow-server      started           1/1         1G       1G     dataflow-server.app.io</screen>
</listitem>
<listitem>
<simpara>Notice that the <literal>dataflow-server</literal> application is started and ready for interaction via the url endpoint</simpara>
</listitem>
<listitem>
<simpara>Connect the <literal>shell</literal> with <literal>server</literal> running on Cloud Foundry, e.g., <literal><link xlink:href="http://dataflow-server.app.io">dataflow-server.app.io</link></literal></simpara>
<screen>$ cd &lt;PATH/TO/SPRING-CLOUD-DATAFLOW-SHELL-JAR&gt;
$ java -jar spring-cloud-dataflow-shell-&lt;VERSION&gt;.jar

  ____                              ____ _                __
 / ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
 \___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
  ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
 |____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
  ____ |_|    _          __|___/                 __________
 |  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
 | | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
 | |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
 |____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/


Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
server-unknown:&gt;</screen>
<screen>server-unknown:&gt;dataflow config server http://dataflow-server.app.io
Successfully targeted http://dataflow-server.app.io
dataflow:&gt;</screen>
<note>
<simpara>PCF 1.7.12 or greater is required to run Tasks on Spring Cloud Data Flow. As of this writing, PCFDev and PWS supports builds upon this version.</simpara>
</note>
</listitem>
<listitem>
<simpara>Task support needs to be enabled on pcf-dev. Being logged as <literal>admin</literal>, issue the following command:</simpara>
<screen>cf enable-feature-flag task_creation
Setting status of task_creation as admin...

OK

Feature task_creation Enabled.</screen>
<note>
<simpara>For this sample, all you need is the <literal>mysql</literal> service and in PCFDev, the <literal>mysql</literal> service comes with a different plan. From CF CLI, create the service by: <literal>cf create-service p-mysql 512mb mysql</literal> and bind this service to <literal>dataflow-server</literal> by: <literal>cf bind-service dataflow-server mysql</literal>.</simpara>
</note>
<note>
<simpara>All the apps deployed to PCFDev start with low memory by default. It is recommended to change it to at least 768MB for <literal>dataflow-server</literal>. Ditto for every app spawned <emphasis role="strong">by</emphasis> Spring Cloud Data Flow. Change the memory by: <literal>cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_MEMORY 512</literal>. Likewise, we would have to skip SSL validation by: <literal>cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SKIP_SSL_VALIDATION true</literal>.</simpara>
</note>
</listitem>
<listitem>
<simpara>Tasks in Spring Cloud Data Flow require an RDBMS to host "task repository" (see <link xlink:href="http://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#spring-cloud-dataflow-task-repository">here</link> for more details), so let&#8217;s instruct the Spring Cloud Data Flow server to bind the <literal>mysql</literal> service to each deployed task:</simpara>
<screen>$ cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_SERVICES mysql
$ cf restage dataflow-server</screen>
<note>
<simpara>We only need <literal>mysql</literal> service for this sample.</simpara>
</note>
</listitem>
<listitem>
<simpara>As a recap, here is what you should see as configuration for the Spring Cloud Data Flow server:</simpara>
<screen>cf env dataflow-server

....
User-Provided:
SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_DOMAIN: local.pcfdev.io
SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_MEMORY: 512
SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_ORG: pcfdev-org
SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_PASSWORD: pass
SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SKIP_SSL_VALIDATION: false
SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SPACE: pcfdev-space
SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_SERVICES: mysql
SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_URL: https://api.local.pcfdev.io
SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_USERNAME: user

No running env variables have been set

No staging env variables have been set</screen>
</listitem>
<listitem>
<simpara>Notice that <literal>dataflow-server</literal> application is started and ready for interaction via <literal><link xlink:href="http://dataflow-server.local.pcfdev.io">dataflow-server.local.pcfdev.io</link></literal> endpoint</simpara>
</listitem>
<listitem>
<simpara>Build and register the batch-job <link xlink:href="https://github.com/spring-cloud/spring-cloud-task/tree/master/spring-cloud-task-samples/batch-job">example</link> from Spring Cloud Task samples. For convenience, the final <link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow-samples/raw/master/src/main/asciidoc/tasks/simple-batch-job/batch-job-1.0.0.BUILD-SNAPSHOT.jar">uber-jar artifact</link> is provided with this sample.</simpara>
<screen>dataflow:&gt;app register --type task --name simple_batch_job --uri https://github.com/spring-cloud/spring-cloud-dataflow-samples/raw/master/tasks/simple-batch-job/batch-job-1.3.0.BUILD-SNAPSHOT.jar</screen>
</listitem>
<listitem>
<simpara>Create the task with <literal>simple-batch-job</literal> application</simpara>
<screen>dataflow:&gt;task create foo --definition "simple_batch_job"</screen>
<note>
<simpara>Unlike Streams, the Task definitions don&#8217;t require explicit deployment. They can be launched on-demand, scheduled, or triggered by streams.</simpara>
</note>
</listitem>
<listitem>
<simpara>Verify there&#8217;s <emphasis role="strong">still</emphasis> no Task applications running on PCFDev - they are listed only after the initial launch/staging attempt on PCF</simpara>
<screen>$ cf apps
Getting apps in org pcfdev-org / space pcfdev-space as user...
OK

name              requested state   instances   memory   disk   urls
dataflow-server   started           1/1         768M     512M   dataflow-server.local.pcfdev.io</screen>
</listitem>
<listitem>
<simpara>Let&#8217;s launch <literal>foo</literal></simpara>
<screen>dataflow:&gt;task launch foo</screen>
</listitem>
<listitem>
<simpara>Verify the execution of <literal>foo</literal> by tailing the logs</simpara>
<screen>$ cf logs foo
Retrieving logs for app foo in org pcfdev-org / space pcfdev-space as user...

2016-08-14T18:48:54.22-0700 [APP/TASK/foo/0]OUT Creating container
2016-08-14T18:48:55.47-0700 [APP/TASK/foo/0]OUT

2016-08-14T18:49:06.59-0700 [APP/TASK/foo/0]OUT 2016-08-15 01:49:06.598  INFO 14 --- [           main] o.s.b.c.l.support.SimpleJobLauncher      : Job: [SimpleJob: [name=job1]] launched with the following parameters: [{}]

...
...

2016-08-14T18:49:06.78-0700 [APP/TASK/foo/0]OUT 2016-08-15 01:49:06.785  INFO 14 --- [           main] o.s.b.c.l.support.SimpleJobLauncher      : Job: [SimpleJob: [name=job1]] completed with the following parameters: [{}] and the following status: [COMPLETED]

...
...

2016-08-14T18:49:07.36-0700 [APP/TASK/foo/0]OUT 2016-08-15 01:49:07.363  INFO 14 --- [           main] o.s.b.c.l.support.SimpleJobLauncher      : Job: [SimpleJob: [name=job2]] launched with the following parameters: [{}]

...
...

2016-08-14T18:49:07.53-0700 [APP/TASK/foo/0]OUT 2016-08-15 01:49:07.536  INFO 14 --- [           main] o.s.b.c.l.support.SimpleJobLauncher      : Job: [SimpleJob: [name=job2]] completed with the following parameters: [{}] and the following status: [COMPLETED]

...
...

2016-08-14T18:49:07.71-0700 [APP/TASK/foo/0]OUT Exit status 0
2016-08-14T18:49:07.78-0700 [APP/TASK/foo/0]OUT Destroying container
2016-08-14T18:49:08.47-0700 [APP/TASK/foo/0]OUT Successfully destroyed container</screen>
<note>
<simpara>Verify <literal>job1</literal> and <literal>job2</literal> operations embeddded in <literal>simple-batch-job</literal> application are launched independently and they returned with the status <literal>COMPLETED</literal>.</simpara>
</note>
<note>
<simpara>Unlike LRPs in Cloud Foundry, tasks are short-lived, so the logs aren&#8217;t always available. They are generated only when the Task application runs; at the end of Task operation, the container that ran the Task application is destroyed to free-up resources.</simpara>
</note>
</listitem>
<listitem>
<simpara>List Tasks in Cloud Foundry</simpara>
<screen>$ cf apps
Getting apps in org pcfdev-org / space pcfdev-space as user...
OK

name              requested state   instances   memory   disk   urls
dataflow-server   started           1/1         768M     512M   dataflow-server.local.pcfdev.io
foo               stopped           0/1         1G       1G</screen>
</listitem>
<listitem>
<simpara>Verify Task execution details</simpara>
<screen>dataflow:&gt;task execution list
╔══════════════════════════╤══╤════════════════════════════╤════════════════════════════╤═════════╗
║        Task Name         │ID│         Start Time         │          End Time          │Exit Code║
╠══════════════════════════╪══╪════════════════════════════╪════════════════════════════╪═════════╣
║foo                       │1 │Sun Aug 14 18:49:05 PDT 2016│Sun Aug 14 18:49:07 PDT 2016│0        ║
╚══════════════════════════╧══╧════════════════════════════╧════════════════════════════╧═════════╝</screen>
</listitem>
<listitem>
<simpara>Verify Job execution details</simpara>
<screen>dataflow:&gt;job execution list
╔═══╤═══════╤═════════╤════════════════════════════╤═════════════════════╤══════════════════╗
║ID │Task ID│Job Name │         Start Time         │Step Execution Count │Definition Status ║
╠═══╪═══════╪═════════╪════════════════════════════╪═════════════════════╪══════════════════╣
║2  │1      │job2     │Sun Aug 14 18:49:07 PDT 2016│1                    │Destroyed         ║
║1  │1      │job1     │Sun Aug 14 18:49:06 PDT 2016│1                    │Destroyed         ║
╚═══╧═══════╧═════════╧════════════════════════════╧═════════════════════╧══════════════════╝</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="_summary_7">
<title>Summary</title>
<simpara>In this sample, you have learned:</simpara>
<itemizedlist>
<listitem>
<simpara>How to register and orchestrate Spring Batch jobs in Spring Cloud Data Flow</simpara>
</listitem>
<listitem>
<simpara>How to use the <literal>cf</literal> CLI in the context of Task applications orchestrated by Spring Cloud Data Flow</simpara>
</listitem>
<listitem>
<simpara>How to verify task executions and task repository</simpara>
</listitem>
</itemizedlist>
</section>
</section>
</section>
</chapter>
<chapter xml:id="_analytics">
<title>Analytics</title>
<section xml:id="spring-cloud-data-flow-samples-twitter-analytics-overview">
<title>Twitter Analytics</title>
<simpara>In this demonstration, you will learn how to build a data pipeline using <link xlink:href="http://cloud.spring.io/spring-cloud-dataflow/">Spring Cloud Data Flow</link> to consume data from <emphasis>TwitterStream</emphasis> and compute simple analytics over data-in-transit using <emphasis>Field-Value-Counter</emphasis>.</simpara>
<simpara>We will take you through the steps to configure Spring Cloud Data Flow&#8217;s <literal>Local</literal> server.</simpara>
<section xml:id="_prerequisites_8">
<title>Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>A Running Data Flow Shell</simpara>
</listitem>
</itemizedlist>
<simpara>The Spring Cloud Data Flow Shell is available for <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started-deploying-spring-cloud-dataflow">download</link> or you can <link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow">build</link> it yourself.</simpara>
<note>
<simpara>the Spring Cloud Data Flow Shell and Local server implementation are in the same repository and are both built by running <literal>./mvnw install</literal> from the project root directory. If you have already run the build, use the jar in <literal>spring-cloud-dataflow-shell/target</literal></simpara>
</note>
<simpara>To run the Shell open a new terminal session:</simpara>
<screen>$ cd &lt;PATH/TO/SPRING-CLOUD-DATAFLOW-SHELL-JAR&gt;
$ java -jar spring-cloud-dataflow-shell-&lt;VERSION&gt;.jar
  ____                              ____ _                __
 / ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
 \___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
  ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
 |____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
  ____ |_|    _          __|___/                 __________
 |  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
 | | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
 | |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
 |____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/


Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
dataflow:&gt;</screen>
<note>
<simpara>The Spring Cloud Data Flow Shell is a Spring Boot application that connects to the Data Flow Server’s REST API and supports a DSL that simplifies the process of defining a stream or task and managing its lifecycle. Most of these samples
use the shell. If you prefer, you can use the Data Flow UI <link xlink:href="http://localhost:9393/dashboard">localhost:9393/dashboard</link>, (or wherever it the server is hosted) to perform the same operations.</simpara>
</note>
<itemizedlist>
<listitem>
<simpara>A running local Data Flow Server</simpara>
</listitem>
</itemizedlist>
<simpara>The Local Data Flow Server is Spring Boot application available for <link xlink:href="http://cloud.spring.io/spring-cloud-dataflow/">download</link> or you can <link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow">build</link> it yourself.
If you build it yourself, the executable jar will be in <literal>spring-cloud-dataflow-server-local/target</literal></simpara>
<simpara>To run the Local Data Flow server Open a new terminal session:</simpara>
<screen>$cd  &lt;PATH/TO/SPRING-CLOUD-DATAFLOW-LOCAL-JAR&gt;
$java -jar spring-cloud-dataflow-server-local-&lt;VERSION&gt;.jar</screen>
<itemizedlist>
<listitem>
<simpara>Running instance of <link xlink:href="http://redis.io/">Redis</link></simpara>
</listitem>
<listitem>
<simpara>Running instance of <link xlink:href="http://kafka.apache.org/downloads.html">Kafka</link></simpara>
</listitem>
<listitem>
<simpara>Twitter credentials from <link xlink:href="https://apps.twitter.com/">Twitter Developers</link> site</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara><link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow/blob/master/spring-cloud-dataflow-docs/src/main/asciidoc/streams.adoc#register-a-stream-app">Register</link> the out-of-the-box applications for the Kafka binder</simpara>
<note>
<simpara>These samples assume that the Data Flow Server can access a remote Maven repository, <literal><link xlink:href="https://repo.spring.io/libs-release">repo.spring.io/libs-release</link></literal> by default. If your Data Flow server is running behind a firewall, or you are using a maven proxy preventing
access to public repositories, you will need to install the sample apps in your internal Maven repository and <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started-maven-configuration">configure</link>
the server accordingly.  The sample applications are typically registered using Data Flow&#8217;s bulk import facility. For example, the Shell command <literal>dataflow:&gt;app import --uri <link xlink:href="http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven">bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</link></literal> (The actual URI is release and binder specific so refer to the sample instructions for the actual URL).
The bulk import URI references a plain text file containing entries for all of the publicly available Spring Cloud Stream and Task applications published to <literal><link xlink:href="https://repo.spring.io">repo.spring.io</link></literal>. For example,
<literal>source.http=maven://org.springframework.cloud.stream.app:http-source-rabbit:1.2.0.RELEASE</literal> registers the <literal>http</literal> source app at the corresponding Maven address, relative to the remote repository(ies) configured for the
Data Flow server. The format is <literal>maven://&lt;groupId&gt;:&lt;artifactId&gt;:&lt;version&gt;</literal>  You will need to <link xlink:href="https://repo.spring.io/libs-release/org/springframework/cloud/stream/app/spring-cloud-stream-app-descriptor/Bacon.RELEASE/spring-cloud-stream-app-descriptor-Bacon.RELEASE.rabbit-apps-maven-repo-url.properties">download</link> the required apps or <link xlink:href="https://github.com/spring-cloud-stream-app-starters">build</link> them and then install them in your Maven repository, using whatever group, artifact, and version you choose. If you do
this, register individual apps using <literal>dataflow:&gt;app register&#8230;&#8203;</literal> using the <literal>maven://</literal> resource URI format corresponding to your installed app.</simpara>
</note>
<screen>dataflow:&gt;app import --uri http://bit.ly/Bacon-RELEASE-stream-applications-kafka-10-maven</screen>
</listitem>
<listitem>
<simpara>Create and deploy the following streams</simpara>
<screen>(1) dataflow:&gt;stream create tweets --definition "twitterstream --consumerKey=&lt;CONSUMER_KEY&gt; --consumerSecret=&lt;CONSUMER_SECRET&gt; --accessToken=&lt;ACCESS_TOKEN&gt; --accessTokenSecret=&lt;ACCESS_TOKEN_SECRET&gt; | log"
Created new stream 'tweets'

(2) dataflow:&gt;stream create tweetlang  --definition ":tweets.twitterstream &gt; field-value-counter --fieldName=lang --name=language" --deploy
Created and deployed new stream 'tweetlang'

(3) dataflow:&gt;stream create tagcount --definition ":tweets.twitterstream &gt; field-value-counter --fieldName=entities.hashtags.text --name=hashtags" --deploy
Created and deployed new stream 'tagcount'

(4) dataflow:&gt;stream deploy tweets
Deployed stream 'tweets'</screen>
<note>
<simpara>To get a consumerKey and consumerSecret you need to register a twitter application. If you don’t already have one set up, you can create an app at the <link xlink:href="https://apps.twitter.com/">Twitter Developers</link> site to get these credentials. The tokens <literal>&lt;CONSUMER_KEY&gt;</literal>, <literal>&lt;CONSUMER_SECRET&gt;</literal>, <literal>&lt;ACCESS_TOKEN&gt;</literal>, and <literal>&lt;ACCESS_TOKEN_SECRET&gt;</literal> are required to be replaced with your account credentials.</simpara>
</note>
</listitem>
<listitem>
<simpara>Verify the streams are successfully deployed. Where: (1) is the primary pipeline; (2) and (3) are tapping the primary pipeline with the DSL syntax <literal>&lt;stream-name&gt;.&lt;label/app name&gt;</literal> [e.x. <literal>:tweets.twitterstream</literal>]; and (4) is the final deployment of primary pipeline</simpara>
<screen>dataflow:&gt;stream list</screen>
</listitem>
<listitem>
<simpara>Notice that <literal>tweetlang.field-value-counter</literal>, <literal>tagcount.field-value-counter</literal>, <literal>tweets.log</literal> and <literal>tweets.twitterstream</literal> <link xlink:href="https://github.com/spring-cloud-stream-app-starters/">Spring Cloud Stream</link> applications are running as Spring Boot applications within the <literal>local-server</literal>.</simpara>
<screen>2016-02-16 11:43:26.174  INFO 10189 --- [nio-9393-exec-2] o.s.c.d.d.l.OutOfProcessModuleDeployer   : deploying module org.springframework.cloud.stream.module:field-value-counter-sink:jar:exec:1.0.0.BUILD-SNAPSHOT instance 0
   Logs will be in /var/folders/c3/ctx7_rns6x30tq7rb76wzqwr0000gp/T/spring-cloud-data-flow-6990537012958280418/tweetlang-1455651806160/tweetlang.field-value-counter
2016-02-16 11:43:26.206  INFO 10189 --- [nio-9393-exec-3] o.s.c.d.d.l.OutOfProcessModuleDeployer   : deploying module org.springframework.cloud.stream.module:field-value-counter-sink:jar:exec:1.0.0.BUILD-SNAPSHOT instance 0
   Logs will be in /var/folders/c3/ctx7_rns6x30tq7rb76wzqwr0000gp/T/spring-cloud-data-flow-6990537012958280418/tagcount-1455651806202/tagcount.field-value-counter
2016-02-16 11:43:26.806  INFO 10189 --- [nio-9393-exec-4] o.s.c.d.d.l.OutOfProcessModuleDeployer   : deploying module org.springframework.cloud.stream.module:log-sink:jar:exec:1.0.0.BUILD-SNAPSHOT instance 0
   Logs will be in /var/folders/c3/ctx7_rns6x30tq7rb76wzqwr0000gp/T/spring-cloud-data-flow-6990537012958280418/tweets-1455651806800/tweets.log
2016-02-16 11:43:26.813  INFO 10189 --- [nio-9393-exec-4] o.s.c.d.d.l.OutOfProcessModuleDeployer   : deploying module org.springframework.cloud.stream.module:twitterstream-source:jar:exec:1.0.0.BUILD-SNAPSHOT instance 0
   Logs will be in /var/folders/c3/ctx7_rns6x30tq7rb76wzqwr0000gp/T/spring-cloud-data-flow-6990537012958280418/tweets-1455651806800/tweets.twitterstream</screen>
</listitem>
<listitem>
<simpara>Verify that two <literal>field-value-counter</literal> with the names <literal>hashtags</literal> and <literal>language</literal> is listing successfully</simpara>
<screen>dataflow:&gt;field-value-counter list
╔════════════════════════╗
║Field Value Counter name║
╠════════════════════════╣
║hashtags                ║
║language                ║
╚════════════════════════╝</screen>
</listitem>
<listitem>
<simpara>Verify you can query individual <literal>field-value-counter</literal> results successfully</simpara>
<screen>dataflow:&gt;field-value-counter display hashtags
Displaying values for field value counter 'hashtags'
╔══════════════════════════════════════╤═════╗
║                Value                 │Count║
╠══════════════════════════════════════╪═════╣
║KCA                                   │   40║
║PENNYSTOCKS                           │   17║
║TEAMBILLIONAIRE                       │   17║
║UCL                                   │   11║
║...                                   │   ..║
║...                                   │   ..║
║...                                   │   ..║
╚══════════════════════════════════════╧═════╝

dataflow:&gt;field-value-counter display language
Displaying values for field value counter 'language'
╔═════╤═════╗
║Value│Count║
╠═════╪═════╣
║en   │1,171║
║es   │  337║
║ar   │  296║
║und  │  251║
║pt   │  175║
║ja   │  137║
║..   │  ...║
║..   │  ...║
║..   │  ...║
╚═════╧═════╝</screen>
</listitem>
<listitem>
<simpara>Go to <literal>Dashboard</literal> accessible at <literal><link xlink:href="http://localhost:9393/dashboard">localhost:9393/dashboard</link></literal> and launch the <literal>Analytics</literal> tab. From the default <literal>Dashboard</literal> menu, select the following combinations to visualize real-time updates on <literal>field-value-counter</literal>.</simpara>
<itemizedlist>
<listitem>
<simpara>For real-time updates on <literal>language</literal> tags, select:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Metric Type as <literal>Field-Value-Counters</literal></simpara>
</listitem>
<listitem>
<simpara>Stream as <literal>language</literal></simpara>
</listitem>
<listitem>
<simpara>Visualization as <literal>Bubble-Chart</literal> or <literal>Pie-Chart</literal></simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>For real-time updates on <literal>hashtags</literal> tags, select:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Metric Type as <literal>Field-Value-Counters</literal></simpara>
</listitem>
<listitem>
<simpara>Stream as <literal>hashtags</literal></simpara>
</listitem>
<listitem>
<simpara>Visualization as <literal>Bubble-Chart</literal> or <literal>Pie-Chart</literal></simpara>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/twitter_analytics.png" width="50%" scalefit="1"/>
</imageobject>
<textobject><phrase>Twitter Analytics Visualization</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="_summary_8">
<title>Summary</title>
<simpara>In this sample, you have learned:</simpara>
<itemizedlist>
<listitem>
<simpara>How to use Spring Cloud Data Flow&#8217;s <literal>Local</literal> server</simpara>
</listitem>
<listitem>
<simpara>How to use Spring Cloud Data Flow&#8217;s <literal>shell</literal> application</simpara>
</listitem>
<listitem>
<simpara>How to create streaming data pipeline to compute simple analytics using <literal>Twitter Stream</literal> and <literal>Field Value Counter</literal> applications</simpara>
</listitem>
</itemizedlist>
</section>
</section>
</chapter>
<chapter xml:id="_functions">
<title>Functions</title>
<section xml:id="_functions_in_spring_cloud_data_flow">
<title>Functions in Spring Cloud Data Flow</title>
<simpara>In this sample, you will learn how to use <link xlink:href="https://github.com/spring-cloud/spring-cloud-function">Spring Cloud Function</link> based streaming applications in Spring Cloud Data Flow. To learn more about Spring Cloud Function, check out the <link xlink:href="http://cloud.spring.io/spring-cloud-function/">project page</link>.</simpara>
<section xml:id="_prerequisites_9">
<title>Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>A Running Data Flow Shell</simpara>
</listitem>
</itemizedlist>
<simpara>The Spring Cloud Data Flow Shell is available for <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started-deploying-spring-cloud-dataflow">download</link> or you can <link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow">build</link> it yourself.</simpara>
<note>
<simpara>the Spring Cloud Data Flow Shell and Local server implementation are in the same repository and are both built by running <literal>./mvnw install</literal> from the project root directory. If you have already run the build, use the jar in <literal>spring-cloud-dataflow-shell/target</literal></simpara>
</note>
<simpara>To run the Shell open a new terminal session:</simpara>
<screen>$ cd &lt;PATH/TO/SPRING-CLOUD-DATAFLOW-SHELL-JAR&gt;
$ java -jar spring-cloud-dataflow-shell-&lt;VERSION&gt;.jar
  ____                              ____ _                __
 / ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
 \___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
  ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
 |____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
  ____ |_|    _          __|___/                 __________
 |  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
 | | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
 | |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
 |____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/


Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
dataflow:&gt;</screen>
<note>
<simpara>The Spring Cloud Data Flow Shell is a Spring Boot application that connects to the Data Flow Server’s REST API and supports a DSL that simplifies the process of defining a stream or task and managing its lifecycle. Most of these samples
use the shell. If you prefer, you can use the Data Flow UI <link xlink:href="http://localhost:9393/dashboard">localhost:9393/dashboard</link>, (or wherever it the server is hosted) to perform the same operations.</simpara>
</note>
<itemizedlist>
<listitem>
<simpara>A running local Data Flow Server</simpara>
</listitem>
</itemizedlist>
<simpara>The Local Data Flow Server is Spring Boot application available for <link xlink:href="http://cloud.spring.io/spring-cloud-dataflow/">download</link> or you can <link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow">build</link> it yourself.
If you build it yourself, the executable jar will be in <literal>spring-cloud-dataflow-server-local/target</literal></simpara>
<simpara>To run the Local Data Flow server Open a new terminal session:</simpara>
<screen>$cd  &lt;PATH/TO/SPRING-CLOUD-DATAFLOW-LOCAL-JAR&gt;
$java -jar spring-cloud-dataflow-server-local-&lt;VERSION&gt;.jar</screen>
<itemizedlist>
<listitem>
<simpara>A local build of <link xlink:href="https://github.com/spring-cloud/spring-cloud-function">Spring Cloud Function</link></simpara>
</listitem>
<listitem>
<simpara>A running instance of <link xlink:href="https://www.rabbitmq.com/">Rabbit MQ</link></simpara>
</listitem>
<listitem>
<simpara>General understanding of the out-of-the-box <link xlink:href="https://github.com/spring-cloud-stream-app-starters/function/blob/master/spring-cloud-starter-stream-app-function/README.adoc">function-runner</link> application</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara><link xlink:href="https://github.com/spring-cloud/spring-cloud-dataflow/blob/master/spring-cloud-dataflow-docs/src/main/asciidoc/streams.adoc#register-a-stream-app">Register</link> the out-of-the-box applications for the Rabbit binder</simpara>
<note>
<simpara>These samples assume that the Data Flow Server can access a remote Maven repository, <literal><link xlink:href="https://repo.spring.io/libs-release">repo.spring.io/libs-release</link></literal> by default. If your Data Flow server is running behind a firewall, or you are using a maven proxy preventing
access to public repositories, you will need to install the sample apps in your internal Maven repository and <link xlink:href="https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#getting-started-maven-configuration">configure</link>
the server accordingly.  The sample applications are typically registered using Data Flow&#8217;s bulk import facility. For example, the Shell command <literal>dataflow:&gt;app import --uri <link xlink:href="http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven">bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</link></literal> (The actual URI is release and binder specific so refer to the sample instructions for the actual URL).
The bulk import URI references a plain text file containing entries for all of the publicly available Spring Cloud Stream and Task applications published to <literal><link xlink:href="https://repo.spring.io">repo.spring.io</link></literal>. For example,
<literal>source.http=maven://org.springframework.cloud.stream.app:http-source-rabbit:1.2.0.RELEASE</literal> registers the <literal>http</literal> source app at the corresponding Maven address, relative to the remote repository(ies) configured for the
Data Flow server. The format is <literal>maven://&lt;groupId&gt;:&lt;artifactId&gt;:&lt;version&gt;</literal>  You will need to <link xlink:href="https://repo.spring.io/libs-release/org/springframework/cloud/stream/app/spring-cloud-stream-app-descriptor/Bacon.RELEASE/spring-cloud-stream-app-descriptor-Bacon.RELEASE.rabbit-apps-maven-repo-url.properties">download</link> the required apps or <link xlink:href="https://github.com/spring-cloud-stream-app-starters">build</link> them and then install them in your Maven repository, using whatever group, artifact, and version you choose. If you do
this, register individual apps using <literal>dataflow:&gt;app register&#8230;&#8203;</literal> using the <literal>maven://</literal> resource URI format corresponding to your installed app.</simpara>
</note>
<screen>dataflow:&gt;app import --uri http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</screen>
</listitem>
<listitem>
<simpara>Register the out-of-the-box <link xlink:href="https://github.com/spring-cloud-stream-app-starters/function/blob/master/spring-cloud-starter-stream-app-function/README.adoc">function-runner</link> application (<emphasis>we will use the <literal>1.0.0.BUILD-SNAPSHOT</literal> built by the Spring CI system</emphasis>)</simpara>
<screen>dataflow:&gt;app register --name function-runner --type processor --uri http://repo.spring.io/libs-snapshot/org/springframework/cloud/stream/app/function-app-rabbit/1.0.0.BUILD-SNAPSHOT/function-app-rabbit-1.0.0.BUILD-SNAPSHOT.jar --metadata-uri http://repo.spring.io/libs-snapshot/org/springframework/cloud/stream/app/function-app-rabbit/1.0.0.BUILD-SNAPSHOT/function-app-rabbit-1.0.0.BUILD-SNAPSHOT-metadata.jar</screen>
</listitem>
<listitem>
<simpara>Create and deploy the following stream</simpara>
<screen>dataflow:&gt;stream create foo --definition "http --server.port=9001 | function-runner --function.className=com.example.functions.CharCounter --function.location=file:///&lt;PATH/TO/SPRING-CLOUD-FUNCTION&gt;/spring-cloud-function-samples/function-sample/target/spring-cloud-function-sample-1.0.0.BUILD-SNAPSHOT.jar | log" --deploy</screen>
<note>
<simpara>Replace the <literal>&lt;PATH/TO/SPRING-CLOUD-FUNCTION&gt;</literal> with the correct path.</simpara>
</note>
<note>
<simpara>The source core of <literal>CharCounter</literal> function is in Spring cloud Function&#8217;s <link xlink:href="https://github.com/spring-cloud/spring-cloud-function/blob/master/spring-cloud-function-samples/function-sample/src/main/java/com/example/functions/CharCounter.java">samples repo</link>.</simpara>
</note>
</listitem>
<listitem>
<simpara>Verify the stream is successfully deployed.</simpara>
<programlisting language="bash" linenumbering="unnumbered">dataflow:&gt;stream list
╔══════╤══════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════╤════════════╗
║Stream│                                                                                        Stream Definition                                                                                         │   Status   ║
║ Name │                                                                                                                                                                                                  │            ║
╠══════╪══════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════╪════════════╣
║foo   │http --server.port=9001 | function-runner --function.className=com.example.functions.CharCounter                                                                                                  │All apps    ║
║      │--function.location=file:///&lt;PATH/TO/SPRING-CLOUD-FUNCTION&gt;/spring-cloud-function-samples/function-sample/target/spring-cloud-function-sample-1.0.0.BUILD-&lt;SNAPSHOT class="jar"&gt;&lt;/SNAPSHOT&gt;		    │have been   ║
║      │| log                                                                                                                                                                                             │successfully║
║      │                                                                                                                                                                                                  │deployed    ║
╚══════╧══════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════╧════════════╝</programlisting>
</listitem>
<listitem>
<simpara>Notice that <literal>foo-http</literal>, <literal>foo-function-runner</literal>, and <literal>foo-log</literal> <link xlink:href="https://github.com/spring-cloud-stream-app-starters/">Spring Cloud Stream</link> applications are running as Spring Boot applications and the log locations will be printed in the Local-server console.</simpara>
<programlisting language="bash" linenumbering="unnumbered">....
....
2017-10-17 11:43:03.714  INFO 18409 --- [nio-9393-exec-7] o.s.c.d.s.s.AppDeployerStreamDeployer    : Deploying application named [log] as part of stream named [foo] with resource URI [maven://org.springframework.cloud.stream.app:log-sink-rabbit:jar:1.2.0.RELEASE]
2017-10-17 11:43:04.379  INFO 18409 --- [nio-9393-exec-7] o.s.c.d.spi.local.LocalAppDeployer       : Deploying app with deploymentId foo.log instance 0.
   Logs will be in /var/folders/c3/ctx7_rns6x30tq7rb76wzqwr0000gs/T/spring-cloud-dataflow-6549025456609489200/foo-1508265783715/foo.log
2017-10-17 11:43:04.380  INFO 18409 --- [nio-9393-exec-7] o.s.c.d.s.s.AppDeployerStreamDeployer    : Deploying application named [function-runner] as part of stream named [foo] with resource URI [file:/var/folders/c3/ctx7_rns6x30tq7rb76wzqwr0000gs/T/deployer-resource-cache8941581850579153886/http-c73a62adae0abd7ec0dee91d891575709f02f8c9]
2017-10-17 11:43:04.384  INFO 18409 --- [nio-9393-exec-7] o.s.c.d.spi.local.LocalAppDeployer       : Deploying app with deploymentId foo.function-runner instance 0.
   Logs will be in /var/folders/c3/ctx7_rns6x30tq7rb76wzqwr0000gs/T/spring-cloud-dataflow-6549025456609489200/foo-1508265784380/foo.function-runner
2017-10-17 11:43:04.385  INFO 18409 --- [nio-9393-exec-7] o.s.c.d.s.s.AppDeployerStreamDeployer    : Deploying application named [http] as part of stream named [foo] with resource URI [maven://org.springframework.cloud.stream.app:http-source-rabbit:jar:1.2.0.RELEASE]
2017-10-17 11:43:04.391  INFO 18409 --- [nio-9393-exec-7] o.s.c.d.spi.local.LocalAppDeployer       : Deploying app with deploymentId foo.http instance 0.
   Logs will be in /var/folders/c3/ctx7_rns6x30tq7rb76wzqwr0000gs/T/spring-cloud-dataflow-6549025456609489200/foo-1508265784385/foo.http
....
....</programlisting>
</listitem>
<listitem>
<simpara>Post sample data pointing to the <literal>http</literal> endpoint: <literal><link xlink:href="http://localhost:9001">localhost:9001</link></literal> (<literal>9001</literal> is the <literal>port</literal> we specified for the <literal>http</literal> source in this case)</simpara>
<screen>dataflow:&gt;http post --target http://localhost:9001 --data "hello world"
&gt; POST (text/plain) http://localhost:9001 hello world
&gt; 202 ACCEPTED


dataflow:&gt;http post --target http://localhost:9001 --data "hmm, yeah, it works now!"
&gt; POST (text/plain) http://localhost:9001 hmm, yeah, it works now!
&gt; 202 ACCEPTED</screen>
</listitem>
<listitem>
<simpara>Tail the log-sink&#8217;s standard-out logs to see the character counts</simpara>
<programlisting language="bash" linenumbering="unnumbered">$ tail -f /var/folders/c3/ctx7_rns6x30tq7rb76wzqwr0000gs/T/spring-cloud-dataflow-6549025456609489200/foo-1508265783715/foo.log/stdout_0.log

....
....
....
....
2017-10-17 11:45:39.363  INFO 19193 --- [on-runner.foo-1] log-sink                                 : 11
2017-10-17 11:46:40.997  INFO 19193 --- [on-runner.foo-1] log-sink                                 : 24
....
....</programlisting>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="_summary_9">
<title>Summary</title>
<simpara>In this sample, you have learned:</simpara>
<itemizedlist>
<listitem>
<simpara>How to use Spring Cloud Data Flow&#8217;s <literal>Local</literal> server</simpara>
</listitem>
<listitem>
<simpara>How to use Spring Cloud Data Flow&#8217;s <literal>shell</literal> application</simpara>
</listitem>
<listitem>
<simpara>How to use the out-of-the-box <literal>function-runner</literal> application in Spring Cloud Data Flow</simpara>
</listitem>
</itemizedlist>
</section>
</section>
</chapter>
</book>